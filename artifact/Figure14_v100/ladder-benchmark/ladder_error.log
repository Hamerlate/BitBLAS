['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 4, 1, 128], 'thread': [1, 4, 1, 32], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 1, 128], 'thread': [4, 1, 32], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 2, 1, 128], 'thread': [1, 2, 1, 64], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 1, 128], 'thread': [2, 1, 64], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 512, 4194304)

['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 1, 1, 128], 'thread': [1, 1, 1, 128], 'rstep': [], 'block_order': int64(block_idx)}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 1, 128], 'thread': [1, 1, 128], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 256, 512)

['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 8, 1, 128], 'thread': [1, 8, 1, 16], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 1, 128], 'thread': [8, 1, 16], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 64, 1, 128], 'thread': [1, 64, 1, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [64, 1, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 32, 1, 128], 'thread': [1, 32, 1, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [32, 1, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 16, 1, 128], 'thread': [1, 16, 1, 8], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 1, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 32, 2, 128], 'thread': [1, 32, 2, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [32, 2, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 16, 2, 128], 'thread': [1, 16, 2, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 2, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 64, 2, 128], 'thread': [1, 64, 2, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [64, 2, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 16, 4, 128], 'thread': [1, 16, 4, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 4, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 32, 4, 128], 'thread': [1, 32, 4, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [32, 4, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 8, 2, 128], 'thread': [1, 8, 2, 8], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 2, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 16, 8, 128], 'thread': [1, 16, 8, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 8, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 8, 4, 128], 'thread': [1, 8, 4, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 4, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 8, 8, 128], 'thread': [1, 8, 8, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 8, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 4, 2, 128], 'thread': [1, 4, 2, 16], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 2, 128], 'thread': [4, 2, 16], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 8, 16, 128], 'thread': [1, 8, 16, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 16, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 4, 4, 128], 'thread': [1, 4, 4, 8], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 4, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 4, 8, 128], 'thread': [1, 4, 8, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 8, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 4, 16, 128], 'thread': [1, 4, 16, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 16, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 2, 2, 128], 'thread': [1, 2, 2, 32], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 2, 128], 'thread': [2, 2, 32], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 1, 2, 128], 'thread': [1, 1, 2, 64], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 2, 128], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 512, 1024)

['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 4, 32, 128], 'thread': [1, 4, 32, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 32, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 1, 4, 128], 'thread': [1, 1, 4, 32], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 4, 128], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 1024, 2048)

['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 2, 4, 128], 'thread': [1, 2, 4, 16], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 4, 128], 'thread': [2, 4, 16], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 1, 8, 128], 'thread': [1, 1, 8, 16], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 8, 128], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 2048, 4096)

['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 2, 8, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 8, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 2, 32, 128], 'thread': [1, 2, 32, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 32, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 1, 64, 128], 'thread': [1, 1, 64, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 64, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 16384, 32768)

['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 2, 16, 128], 'thread': [1, 2, 16, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 16, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 1, 32, 128], 'thread': [1, 1, 32, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 32, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 8192, 16384)

['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 1, 16, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 4096, 8192)

['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 2, 64, 128], 'thread': [1, 2, 64, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 64, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_reshape_transpose_4>: {'block': [1, 1, 128, 128], 'thread': [1, 1, 128, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 128, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 32768, 65536)

['ladder_perfect_quant_linear_cast_6', 'reshape_reshape_transpose_7', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_6>: {'block': [1, 4, 16, 16], 'thread': [1, 4, 8, 4], 'rstep': [8, 32], 'block_order': int32(int64(int32(int64(block_idx)))), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_reshape_transpose_7>: {'block': [1, 8, 1, 128], 'thread': [1, 8, 1, 16], 'rstep': [], 'block_order': int64(int32(int64(block_idx))), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [64, 1, 128], 'thread': [8, 1, 16], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  169: TVMFuncCall
  168: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  167: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  166: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  165: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  164: tvm::transform::Pass::operator()(tvm::IRModule) const
  163: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  162: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  161: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  160: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  159: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_8IRModuleES5_NS_9transform11PassContextEEE17AssignTypedLambdaIZNS_3tir9transform13MakePackedAPIEvEUlS5_S7_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SF_SJ_
  158: tvm::tir::transform::MakePackedAPI()::{lambda(tvm::IRModule, tvm::transform::PassContext)#1}::operator()(tvm::IRModule, tvm::transform::PassContext) const [clone .isra.0]
  157: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&)
  156: tvm::tir::UndefinedVars(tvm::tir::Stmt const&, tvm::runtime::Array<tvm::tir::Var, void> const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  154: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  153: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  152: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  151: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  150: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  149: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  148: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  147: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  146: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  145: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  144: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  143: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  142: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  141: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  140: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  139: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  138: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  137: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  136: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  135: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  134: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  132: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  131: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  129: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  128: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  126: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  125: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  123: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  122: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  120: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  119: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  118: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  117: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  116: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  115: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  114: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  113: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  112: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  111: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  110: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  109: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  108: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  107: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  106: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  105: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  104: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  103: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  102: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  100: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  99: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  98: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  97: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  96: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  95: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  94: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  93: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  92: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  91: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  90: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  89: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  88: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  87: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  86: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  85: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  84: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  83: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  82: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  81: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  80: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  79: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  78: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  77: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  76: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  75: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::SeqStmtNode const*)
  74: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  73: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  71: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  70: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  68: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  67: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  65: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  64: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  62: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  61: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  59: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  58: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  42: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  41: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  40: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  39: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  38: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  37: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  36: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  31: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  30: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  29: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  28: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  27: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  26: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  25: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  23: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::SeqStmtNode const*)
  22: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  19: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  18: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  15: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  12: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AllocateNode const*)
  11: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  10: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  8: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AllocateNode const*)
  7: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  4: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  1: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::ForNode const*)
  0: tvm::tir::VarUseDefAnalysis::HandleDef(tvm::tir::VarNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/split_host_device.cc", line 191
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (!use_count_.count(v)) is false: variable vthread.s has been used before definition!

['ladder_perfect_quant_linear_cast_6', 'reshape_reshape_transpose_7', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_6>: {'block': [1, 8, 16, 16], 'thread': [1, 8, 4, 4], 'rstep': [4, 32], 'block_order': int32(int64(int32(int64(block_idx)))), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_reshape_transpose_7>: {'block': [1, 8, 2, 128], 'thread': [1, 8, 2, 8], 'rstep': [], 'block_order': int64(int32(int64(block_idx))), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [64, 2, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  169: TVMFuncCall
  168: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  167: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  166: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  165: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  164: tvm::transform::Pass::operator()(tvm::IRModule) const
  163: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  162: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  161: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  160: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  159: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_8IRModuleES5_NS_9transform11PassContextEEE17AssignTypedLambdaIZNS_3tir9transform13MakePackedAPIEvEUlS5_S7_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SF_SJ_
  158: tvm::tir::transform::MakePackedAPI()::{lambda(tvm::IRModule, tvm::transform::PassContext)#1}::operator()(tvm::IRModule, tvm::transform::PassContext) const [clone .isra.0]
  157: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&)
  156: tvm::tir::UndefinedVars(tvm::tir::Stmt const&, tvm::runtime::Array<tvm::tir::Var, void> const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  154: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  153: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  152: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  151: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  150: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  149: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  148: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  147: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  146: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  145: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  144: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  143: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  142: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  141: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  140: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  139: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  138: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  137: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  136: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  135: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  134: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  132: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  131: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  129: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  128: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  126: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  125: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  123: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  122: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  120: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  119: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  118: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  117: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  116: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  115: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  114: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  113: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  112: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  111: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  110: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  109: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  108: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  107: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  106: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  105: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  104: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  103: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  102: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  100: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  99: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  98: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  97: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  96: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  95: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  94: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  93: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  92: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  91: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  90: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  89: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  88: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  87: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  86: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  85: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  84: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  83: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  82: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  81: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  80: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  79: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  78: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  77: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  76: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  75: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::SeqStmtNode const*)
  74: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  73: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  71: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  70: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  68: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  67: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  65: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  64: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  62: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  61: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  59: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  58: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  42: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  41: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  40: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  39: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  38: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  37: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  36: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  31: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  30: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  29: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  28: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  27: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  26: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  25: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  23: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::SeqStmtNode const*)
  22: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  19: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  18: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  15: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  12: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AllocateNode const*)
  11: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  10: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  8: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AllocateNode const*)
  7: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  4: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  1: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::ForNode const*)
  0: tvm::tir::VarUseDefAnalysis::HandleDef(tvm::tir::VarNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/split_host_device.cc", line 191
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (!use_count_.count(v)) is false: variable vthread.s has been used before definition!

['ladder_perfect_quant_linear_cast_6', 'reshape_reshape_transpose_7', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_6>: {'block': [1, 16, 16, 16], 'thread': [1, 16, 4, 2], 'rstep': [4, 32], 'block_order': int32(int64(int32(int64(block_idx)))), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_reshape_transpose_7>: {'block': [1, 8, 4, 128], 'thread': [1, 8, 4, 4], 'rstep': [], 'block_order': int64(int32(int64(block_idx))), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [64, 4, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  169: TVMFuncCall
  168: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  167: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  166: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  165: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  164: tvm::transform::Pass::operator()(tvm::IRModule) const
  163: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  162: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  161: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  160: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  159: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_8IRModuleES5_NS_9transform11PassContextEEE17AssignTypedLambdaIZNS_3tir9transform13MakePackedAPIEvEUlS5_S7_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SF_SJ_
  158: tvm::tir::transform::MakePackedAPI()::{lambda(tvm::IRModule, tvm::transform::PassContext)#1}::operator()(tvm::IRModule, tvm::transform::PassContext) const [clone .isra.0]
  157: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&)
  156: tvm::tir::UndefinedVars(tvm::tir::Stmt const&, tvm::runtime::Array<tvm::tir::Var, void> const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  154: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  153: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  152: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  151: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  150: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  149: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  148: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  147: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  146: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  145: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  144: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  143: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  142: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  141: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  140: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  139: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  138: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  137: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  136: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  135: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  134: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  132: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  131: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  129: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  128: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  126: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  125: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  123: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  122: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  120: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  119: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  118: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  117: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  116: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  115: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  114: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  113: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  112: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  111: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  110: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  109: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  108: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  107: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  106: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  105: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  104: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  103: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  102: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  100: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  99: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  98: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  97: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  96: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  95: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  94: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  93: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  92: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  91: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  90: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  89: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  88: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  87: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  86: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  85: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  84: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  83: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  82: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  81: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  80: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  79: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  78: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  77: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  76: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  75: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::SeqStmtNode const*)
  74: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  73: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  71: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  70: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  68: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  67: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  65: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  64: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  62: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  61: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  59: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  58: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  42: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  41: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  40: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  39: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  38: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  37: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  36: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  31: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  30: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  29: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  28: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  27: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  26: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  25: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  23: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::SeqStmtNode const*)
  22: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  19: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  18: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  15: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  12: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AllocateNode const*)
  11: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  10: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  8: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AllocateNode const*)
  7: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  4: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  1: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::ForNode const*)
  0: tvm::tir::VarUseDefAnalysis::HandleDef(tvm::tir::VarNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/split_host_device.cc", line 191
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (!use_count_.count(v)) is false: variable vthread.s has been used before definition!

['ladder_perfect_quant_linear_cast_6', 'reshape_reshape_transpose_7', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_6>: {'block': [1, 32, 16, 16], 'thread': [1, 32, 2, 2], 'rstep': [2, 32], 'block_order': int32(int64(int32(int64(block_idx)))), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8, 'B_decode': 16}}, <Node, reshape_reshape_transpose_7>: {'block': [1, 8, 8, 128], 'thread': [1, 8, 8, 2], 'rstep': [], 'block_order': int64(int32(int64(block_idx))), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [64, 8, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  169: TVMFuncCall
  168: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  167: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  166: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  165: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  164: tvm::transform::Pass::operator()(tvm::IRModule) const
  163: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  162: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  161: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  160: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  159: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_8IRModuleES5_NS_9transform11PassContextEEE17AssignTypedLambdaIZNS_3tir9transform13MakePackedAPIEvEUlS5_S7_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SF_SJ_
  158: tvm::tir::transform::MakePackedAPI()::{lambda(tvm::IRModule, tvm::transform::PassContext)#1}::operator()(tvm::IRModule, tvm::transform::PassContext) const [clone .isra.0]
  157: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&)
  156: tvm::tir::UndefinedVars(tvm::tir::Stmt const&, tvm::runtime::Array<tvm::tir::Var, void> const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  154: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  153: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  152: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  151: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  150: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  149: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  148: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  147: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  146: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  145: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  144: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  143: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  142: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  141: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  140: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  139: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  138: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  137: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  136: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  135: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  134: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  132: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  131: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  129: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  128: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  126: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  125: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  123: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  122: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  120: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  119: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  118: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  117: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  116: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  115: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  114: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  113: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::LetStmtNode const*)
  112: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  111: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  110: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  109: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  108: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  107: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  106: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  105: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  104: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  103: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  102: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  100: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  99: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  98: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  97: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  96: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  95: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  94: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  93: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  92: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  91: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  90: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  89: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  88: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  87: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  86: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  85: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  84: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  83: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  82: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  81: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  80: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  79: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  78: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  77: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  76: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  75: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::SeqStmtNode const*)
  74: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  73: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  71: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  70: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  68: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  67: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  65: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  64: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  62: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  61: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  59: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  58: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  42: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  41: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  40: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  39: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  38: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  37: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  36: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  31: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  30: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  29: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  28: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  27: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  26: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AssertStmtNode const*)
  25: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  23: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::SeqStmtNode const*)
  22: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  19: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  18: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  15: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  12: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AllocateNode const*)
  11: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  10: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  8: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AllocateNode const*)
  7: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  4: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  1: tvm::tir::VarUseDefAnalysis::VisitStmt_(tvm::tir::ForNode const*)
  0: tvm::tir::VarUseDefAnalysis::HandleDef(tvm::tir::VarNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/split_host_device.cc", line 191
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (!use_count_.count(v)) is false: variable vthread.s has been used before definition!

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 128], 'thread': [1, 8, 16], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 64, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 16384, 524288)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 64], 'thread': [1, 16, 8], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 128, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 16384, 1048576)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 128], 'thread': [1, 16, 8], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 128, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 32768, 1048576)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 64], 'thread': [1, 16, 8], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 64, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 8192, 524288)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 128], 'thread': [1, 8, 16], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 32, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 8192, 262144)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 32], 'thread': [1, 32, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 128, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 8192, 1048576)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 64], 'thread': [2, 8, 8], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 64, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 16384, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 64], 'thread': [1, 8, 16], 'rstep': [128], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 32, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 4096, 262144)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 32], 'thread': [1, 16, 8], 'rstep': [128], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 64, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 4096, 524288)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 256], 'thread': [1, 8, 16], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 64, 256], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 32768, 524288)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 64], 'thread': [1, 32, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 256, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 32768, 2097152)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 64], 'thread': [2, 8, 8], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 32, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 8192, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 32], 'thread': [2, 16, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 64, 32], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 8192, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 128], 'thread': [2, 8, 8], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 64, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 32768, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 128, 64], 'thread': [2, 16, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 128, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 32768, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 32], 'thread': [1, 16, 8], 'rstep': [128], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 32, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 2048, 262144)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 32], 'thread': [2, 8, 8], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 32, 32], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 4096, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 16, 128], 'thread': [1, 4, 32], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 16, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 4096, 131072)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 16], 'thread': [1, 32, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 128, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 4096, 1048576)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 16, 64], 'thread': [1, 8, 16], 'rstep': [128], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 16, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 2048, 131072)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 16], 'thread': [1, 32, 4], 'rstep': [128], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 64, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 2048, 524288)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 16, 64], 'thread': [2, 4, 16], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 16, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 4096, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 16], 'thread': [2, 16, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 64, 16], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 4096, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 256], 'thread': [1, 4, 32], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 32, 256], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 16384, 262144)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 32], 'thread': [1, 32, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 256, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 16384, 2097152)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 32, 32], 'thread': [4, 8, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 4, 32, 32], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 128], 'thread': [2, 4, 16], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 32, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 16384, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 128, 32], 'thread': [2, 16, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 128, 32], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 16384, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 16, 32], 'thread': [2, 8, 8], 'rstep': [128], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 16, 32], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 2048, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 16], 'thread': [2, 16, 4], 'rstep': [128], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 32, 16], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 2048, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 16, 32], 'thread': [1, 8, 16], 'rstep': [128], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 16, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 1024, 131072)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 16], 'thread': [1, 16, 8], 'rstep': [128], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 32, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 1024, 262144)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 16, 32], 'thread': [4, 4, 8], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 4, 16, 32], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 32, 16], 'thread': [4, 8, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 4, 32, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 32, 64], 'thread': [4, 4, 8], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 4, 32, 64], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 64, 32], 'thread': [4, 8, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 4, 64, 32], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 16, 16], 'thread': [4, 8, 4], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 4, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 16, 16], 'thread': [2, 8, 8], 'rstep': [128], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 2, 16, 16], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 1024, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 16, 16], 'thread': [1, 16, 8], 'rstep': [128], 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 16, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 512, 131072)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 8, 128], 'thread': [1, 4, 32], 'rstep': [64], 'step': [1, 1, 2], 'vectorize': {'p0': 4, 'p1': 8}}, <Node, reshape_divide_10>: {'block': [1, 1, 8, 128], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 2048, 65536)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 1, 64], 'thread': [1, 1, 64], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 1, 32], 'thread': [1, 1, 32], 'rstep': [128], 'reduce_thread': [4], 'vectorize': {'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 1, 16], 'thread': [1, 1, 16], 'rstep': [128], 'reduce_thread': [8], 'vectorize': {'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 1, 8], 'thread': [1, 1, 8], 'rstep': [128], 'reduce_thread': [16], 'vectorize': {'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 1, 4], 'thread': [1, 1, 4], 'rstep': [128], 'reduce_thread': [32], 'vectorize': {'input1': 4}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 2, 32], 'thread': [1, 2, 32], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 2, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 1, 2], 'thread': [1, 1, 2], 'rstep': [128], 'reduce_thread': [64], 'vectorize': {'input1': 2}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 4, 1], 'thread': [1, 1, 4, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 2, 1], 'thread': [1, 2, 2, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 1, 1], 'thread': [1, 4, 1, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 2, 1], 'thread': [1, 1, 2, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 1, 1], 'thread': [1, 2, 1, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 1, 1], 'thread': [1, 1, 1, 1], 'rstep': [4096], 'reduce_thread': [128], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 8, 1], 'thread': [1, 1, 8, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 4, 1], 'thread': [1, 2, 4, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 2, 1], 'thread': [1, 4, 2, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 8, 1, 1], 'thread': [1, 8, 1, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 64, 1], 'thread': [1, 1, 64, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 32, 1], 'thread': [1, 2, 32, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 16, 1], 'thread': [1, 4, 16, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 8, 8, 1], 'thread': [1, 8, 8, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 16, 4, 1], 'thread': [1, 16, 4, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 32, 2, 1], 'thread': [1, 32, 2, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 64, 1, 1], 'thread': [1, 64, 1, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 32, 1], 'thread': [1, 1, 32, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 16, 1], 'thread': [1, 2, 16, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 8, 1], 'thread': [1, 4, 8, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 8, 4, 1], 'thread': [1, 8, 4, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 16, 2, 1], 'thread': [1, 16, 2, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 32, 1, 1], 'thread': [1, 32, 1, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 16, 1], 'thread': [1, 1, 16, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 8, 1], 'thread': [1, 2, 8, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 4, 1], 'thread': [1, 4, 4, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 8, 2, 1], 'thread': [1, 8, 2, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 16, 1, 1], 'thread': [1, 16, 1, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12', 'sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 2, 1, 4096], 'thread': [1, 2, 1, 64], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, subtract_exp_12>: {'block': [1, 2, 1, 4096], 'thread': [1, 2, 1, 64], 'rstep': [], 'step': [1, 1, 1, 2]}, <Node, sum_13>: {'block': [1, 2, 1, 4096], 'thread': [1, 2, 1, 64], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 1, 4096], 'thread': [2, 1, 64], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, divide_cast_cast_reshape_14>, Tensor(shape=[1, 64, 4096, 4096], op.name=p0), 16384, 67108864)

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 4, 1], 'thread': [1, 1, 4, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 2, 1], 'thread': [1, 1, 2, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 1, 1], 'thread': [1, 1, 1, 1], 'rstep': [4096], 'reduce_thread': [128], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 8, 1], 'thread': [1, 1, 8, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 64, 1], 'thread': [1, 1, 64, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 32, 1], 'thread': [1, 1, 32, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 16, 1], 'thread': [1, 1, 16, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 32, 1], 'thread': [1, 2, 32, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 16, 1], 'thread': [1, 4, 16, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 16, 1], 'thread': [1, 2, 16, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 8, 8, 1], 'thread': [1, 8, 8, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 8, 1], 'thread': [1, 4, 8, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 8, 1], 'thread': [1, 2, 8, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 4, 1], 'thread': [1, 2, 4, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 16, 4, 1], 'thread': [1, 16, 4, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 8, 4, 1], 'thread': [1, 8, 4, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 4, 1], 'thread': [1, 4, 4, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 2, 1], 'thread': [1, 2, 2, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 2, 1], 'thread': [1, 4, 2, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 32, 2, 1], 'thread': [1, 32, 2, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 16, 2, 1], 'thread': [1, 16, 2, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 8, 2, 1], 'thread': [1, 8, 2, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 1, 1], 'thread': [1, 4, 1, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 1, 1], 'thread': [1, 2, 1, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 8, 1, 1], 'thread': [1, 8, 1, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 64, 1, 1], 'thread': [1, 64, 1, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['ladder_perfect_quant_linear_cast_15', 'reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_15>: {'block': [1, 8, 16, 16], 'thread': [1, 8, 4, 4], 'rstep': [4, 32], 'block_order': int32(int64(block_idx)), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [64, 128, 2], 'thread': [8, 8, 2], 'rstep': [], 'step': [1, 2, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>, Tensor(shape=[256, 64, 16, 16], op.name=p0), 4096, 8388608)

['ladder_perfect_quant_linear_cast_15', 'reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_15>: {'block': [1, 4, 16, 16], 'thread': [1, 4, 8, 4], 'rstep': [8, 32], 'block_order': int32(int64(block_idx)), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [64, 128, 1], 'thread': [8, 16, 1], 'rstep': [], 'step': [1, 2, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>, Tensor(shape=[256, 64, 16, 16], op.name=p0), 2048, 8388608)

['ladder_perfect_quant_linear_cast_15', 'reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_15>: {'block': [1, 16, 16, 16], 'thread': [1, 16, 4, 2], 'rstep': [4, 32], 'block_order': int32(int64(block_idx)), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [64, 128, 4], 'thread': [8, 4, 4], 'rstep': [], 'step': [1, 2, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>, Tensor(shape=[256, 64, 16, 16], op.name=p0), 8192, 8388608)

['ladder_perfect_quant_linear_cast_15', 'reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_15>: {'block': [1, 32, 16, 16], 'thread': [1, 32, 2, 2], 'rstep': [2, 32], 'block_order': int32(int64(block_idx)), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8, 'B_decode': 16}}, <Node, reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [64, 128, 8], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 2, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>, Tensor(shape=[256, 64, 16, 16], op.name=p0), 16384, 8388608)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 1, 128], 'thread': [1, 1, 128], 'rstep': [64], 'block_order': ((floormod(block_idx, 64)*4096) + floordiv(block_idx, 64)), 'vectorize': {'p1': 8}}, <Node, reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 1, 4, 32], 'thread': [1, 1, 4, 32], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_cast_cast_reshape_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 256, 67108864)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 1, 64], 'thread': [1, 1, 64], 'rstep': [128], 'block_order': (((floordiv(floormod(block_idx, 128), 2)*8192) + (floordiv(block_idx, 128)*2)) + floormod(block_idx, 2)), 'vectorize': {'p0': 2, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 1, 2, 32], 'thread': [1, 1, 2, 32], 'rstep': []}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 1, 32], 'thread': [1, 1, 32], 'rstep': [256], 'block_order': (((floordiv(floormod(block_idx, 256), 4)*16384) + (floordiv(block_idx, 256)*4)) + floormod(block_idx, 4)), 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 1, 1, 32], 'thread': [1, 1, 1, 32], 'rstep': []}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 1, 16], 'thread': [1, 1, 16], 'rstep': [512], 'block_order': (((floordiv(floormod(block_idx, 512), 8)*32768) + (floordiv(block_idx, 512)*8)) + floormod(block_idx, 8)), 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 1, 1, 16], 'thread': [1, 1, 1, 16], 'rstep': []}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 1, 8], 'thread': [1, 1, 8], 'rstep': [1024], 'block_order': (((floordiv(floormod(block_idx, 1024), 16)*65536) + (floordiv(block_idx, 1024)*16)) + floormod(block_idx, 16)), 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 1, 1, 8], 'thread': [1, 1, 1, 8], 'rstep': []}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 1, 4], 'thread': [1, 1, 4], 'rstep': [2048], 'block_order': (((floordiv(floormod(block_idx, 2048), 32)*131072) + (floordiv(block_idx, 2048)*32)) + floormod(block_idx, 32)), 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 1, 1, 4], 'thread': [1, 1, 1, 4], 'rstep': []}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 1, 2], 'thread': [1, 1, 2], 'rstep': [4096], 'block_order': (((floordiv(floormod(block_idx, 4096), 64)*262144) + (floordiv(block_idx, 4096)*64)) + floormod(block_idx, 64)), 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 1, 1, 2], 'thread': [1, 1, 1, 2], 'rstep': []}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 1, 112], 'thread': [1, 1, 16], 'rstep': [64], 'block_order': ((floordiv(floormod(block_idx, 128), 2)*8192) + (floordiv(block_idx, 128)*2)), 'vectorize': {'p0': 4, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 1, 4, 16], 'thread': [1, 1, 2, 8], 'rstep': []}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 1, 128], 'thread': [2, 1, 64], 'rstep': [64], 'block_order': ((floormod(block_idx, 32)*4096) + floordiv(block_idx, 32)), 'step': [1, 1, 2], 'vectorize': {'p1': 8}}, <Node, reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 1, 8, 32], 'thread': [1, 1, 8, 16], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_cast_cast_reshape_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 512, 67108864)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 64, 128], 'thread': [1, 8, 16], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 128, 128], 'thread': [1, 16, 8], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 128, 64], 'thread': [1, 16, 8], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 256, 128], 'thread': [1, 16, 8], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 32, 128], 'thread': [1, 8, 16], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 64, 64], 'thread': [1, 16, 8], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 32, 64], 'thread': [1, 8, 16], 'rstep': [128], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 256, 64], 'thread': [1, 16, 8], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 128, 32], 'thread': [1, 16, 8], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 16, 128], 'thread': [1, 4, 32], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 64, 32], 'thread': [1, 16, 8], 'rstep': [128], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 16, 64], 'thread': [1, 8, 16], 'rstep': [128], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 32, 32], 'thread': [1, 16, 8], 'rstep': [128], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 16, 32], 'thread': [1, 8, 16], 'rstep': [256], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 256, 32], 'thread': [1, 32, 4], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 128, 16], 'thread': [1, 32, 4], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 8, 128], 'thread': [1, 4, 32], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 4, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 64, 16], 'thread': [1, 16, 8], 'rstep': [128], 'step': [1, 4, 1], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 8, 64], 'thread': [1, 4, 32], 'rstep': [128], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 32, 16], 'thread': [1, 16, 8], 'rstep': [256], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 8, 32], 'thread': [1, 8, 16], 'rstep': [256], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 16, 16], 'thread': [1, 16, 8], 'rstep': [256], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 8, 16], 'thread': [1, 8, 16], 'rstep': [512], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 128, 8], 'thread': [1, 32, 4], 'rstep': [64], 'step': [1, 4, 1], 'vectorize': {'input0': 8, 'input1': 4}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 256, 16], 'thread': [1, 32, 4], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 4, 128], 'thread': [1, 2, 64], 'rstep': [64], 'vectorize': {'input0': 2, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 64, 8], 'thread': [1, 32, 4], 'rstep': [128], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 4, 64], 'thread': [1, 4, 32], 'rstep': [128], 'vectorize': {'input0': 4, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 32, 8], 'thread': [1, 16, 8], 'rstep': [256], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 4, 32], 'thread': [1, 4, 32], 'rstep': [256], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 16, 8], 'thread': [1, 16, 8], 'rstep': [512], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 4, 16], 'thread': [1, 4, 16], 'rstep': [512], 'reduce_thread': [2], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 8, 8], 'thread': [1, 8, 8], 'rstep': [512], 'reduce_thread': [2], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 4, 8], 'thread': [1, 4, 8], 'rstep': [1024], 'reduce_thread': [4], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 128, 4], 'thread': [1, 32, 4], 'rstep': [64], 'step': [1, 4, 1], 'vectorize': {'input0': 8, 'input1': 2}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 256, 8], 'thread': [1, 64, 2], 'rstep': [64], 'step': [1, 1, 4], 'vectorize': {'input0': 8, 'input1': 4}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 2, 128], 'thread': [1, 2, 64], 'rstep': [64], 'vectorize': {'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 64, 4], 'thread': [1, 32, 4], 'rstep': [128], 'vectorize': {'input0': 8, 'input1': 4}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 2, 64], 'thread': [1, 2, 64], 'rstep': [128], 'vectorize': {'input0': 2, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_cast_cast_reshape_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_cast_cast_reshape_18>: {'block': [1, 32, 4], 'thread': [1, 32, 4], 'rstep': [256], 'vectorize': {'input0': 8, 'input1': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['ladder_perfect_quant_linear_cast_19', 'reshape_add_20']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_19>: {'block': [1, 64, 16, 16], 'thread': [1, 64, 2, 1], 'rstep': [1, 32], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 4, 'B_decode': 16}}, <Node, reshape_add_20>: {'block': [1, 2, 8192], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_20>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 32768, 262144)

['ladder_perfect_quant_linear_cast_19', 'reshape_add_20']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_19>: {'block': [1, 1, 8, 16], 'thread': [1, 1, 8, 16], 'rstep': [32, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_20>: {'block': [1, 1, 128], 'thread': [1, 1, 128], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_20>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 256, 512)

['ladder_perfect_quant_linear_cast_19', 'reshape_add_20']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_19>: {'block': [1, 1, 4, 16], 'thread': [1, 1, 4, 16], 'rstep': [32, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_20>: {'block': [1, 1, 64], 'thread': [1, 1, 64], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_20>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 128, 512)

['ladder_perfect_quant_linear_cast_19', 'reshape_add_20']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_19>: {'block': [1, 1, 2, 16], 'thread': [1, 1, 2, 16], 'rstep': [32, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_20>: {'block': [1, 1, 32], 'thread': [1, 1, 32], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_20>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 64, 512)

['ladder_perfect_quant_linear_cast_19', 'reshape_add_20']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_19>: {'block': [1, 1, 1, 8], 'thread': [1, 1, 1, 8], 'rstep': [64, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_20>: {'block': [1, 1, 8], 'thread': [1, 1, 8], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_20>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 32, 512)

['ladder_perfect_quant_linear_cast_19', 'reshape_add_20']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_19>: {'block': [1, 1, 1, 4], 'thread': [1, 1, 1, 4], 'rstep': [128, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_20>: {'block': [1, 1, 4], 'thread': [1, 1, 4], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_20>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 32, 512)

['ladder_perfect_quant_linear_cast_19', 'reshape_add_20']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_19>: {'block': [1, 1, 1, 2], 'thread': [1, 1, 1, 2], 'rstep': [256, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_20>: {'block': [1, 1, 2], 'thread': [1, 1, 2], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_20>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 32, 512)

['multiply_cast_multiply_23', 'reshape_cast_cast_reshape_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 1, 256], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}, <Node, reshape_cast_cast_reshape_24>: {'block': [1, 1, 8, 32], 'thread': [1, 1, 8, 16], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_cast_cast_reshape_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 512, 16384)

['multiply_cast_multiply_23', 'reshape_cast_cast_reshape_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 1, 128], 'thread': [1, 1, 128], 'rstep': []}, <Node, reshape_cast_cast_reshape_24>: {'block': [1, 1, 4, 32], 'thread': [1, 1, 4, 32], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_cast_cast_reshape_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 256, 16384)

['multiply_cast_multiply_23', 'reshape_cast_cast_reshape_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 1, 64], 'thread': [1, 1, 64], 'rstep': []}, <Node, reshape_cast_cast_reshape_24>: {'block': [1, 1, 2, 32], 'thread': [1, 1, 2, 32], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_cast_cast_reshape_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 128, 16384)

['multiply_cast_multiply_23', 'reshape_cast_cast_reshape_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 1, 1024], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}, <Node, reshape_cast_cast_reshape_24>: {'block': [1, 2, 16, 32], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 4]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_cast_cast_reshape_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 2048, 16384)

['multiply_cast_multiply_23', 'reshape_cast_cast_reshape_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 1, 4096], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}, <Node, reshape_cast_cast_reshape_24>: {'block': [1, 8, 16, 32], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 4]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_cast_cast_reshape_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 8192, 16384)

['multiply_cast_multiply_23', 'reshape_cast_cast_reshape_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 1, 2048], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}, <Node, reshape_cast_cast_reshape_24>: {'block': [1, 4, 16, 32], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 4]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_cast_cast_reshape_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 4096, 16384)

['multiply_cast_multiply_23', 'reshape_cast_cast_reshape_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 1, 32], 'thread': [1, 1, 32], 'rstep': []}, <Node, reshape_cast_cast_reshape_24>: {'block': [1, 1, 1, 32], 'thread': [1, 1, 1, 32], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_cast_cast_reshape_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 64, 16384)

['ladder_perfect_quant_linear_cast_25', 'reshape_26', 'ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_25>: {'block': [1, 1, 1, 8], 'thread': [1, 1, 1, 8], 'rstep': [64, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_26>: {'block': [1, 1, 8], 'thread': [1, 1, 8], 'rstep': [], 'block_order': ((floordiv(block_idx, 3584)*3584) + floormod(block_idx, 3584))}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [1, 1, 1, 8], 'thread': [1, 1, 1, 8], 'rstep': [64, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [1, 1, 1, 8], 'thread': [1, 1, 1, 8], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 32, 512)

['ladder_perfect_quant_linear_cast_25', 'reshape_26', 'ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_25>: {'block': [1, 1, 1, 4], 'thread': [1, 1, 1, 4], 'rstep': [128, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_26>: {'block': [1, 1, 4], 'thread': [1, 1, 4], 'rstep': [], 'block_order': ((floordiv(block_idx, 7168)*7168) + floormod(block_idx, 7168))}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [1, 1, 1, 4], 'thread': [1, 1, 1, 4], 'rstep': [128, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [1, 1, 1, 4], 'thread': [1, 1, 1, 4], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 32, 512)

['ladder_perfect_quant_linear_cast_25', 'reshape_26', 'ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_25>: {'block': [1, 1, 1, 2], 'thread': [1, 1, 1, 2], 'rstep': [256, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_26>: {'block': [1, 1, 2], 'thread': [1, 1, 2], 'rstep': [], 'block_order': ((floordiv(block_idx, 14336)*14336) + floormod(block_idx, 14336))}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [1, 1, 1, 2], 'thread': [1, 1, 1, 2], 'rstep': [128, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [1, 1, 1, 2], 'thread': [1, 1, 1, 2], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 32, 512)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [32, 1, 15, 16], 'thread': [32, 1, 1, 4], 'rstep': [1, 32], 'block_order': (floordiv(block_idx, 2)*2), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8, 'B_decode': 4}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [32, 1, 8, 16], 'thread': [2, 1, 8, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [16, 1, 15, 16], 'thread': [16, 1, 1, 8], 'rstep': [2, 32], 'block_order': (floordiv(block_idx, 2)*2), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8, 'B_decode': 8}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 1, 8, 16], 'thread': [2, 1, 8, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [64, 1, 15, 16], 'thread': [64, 1, 1, 2], 'rstep': [1, 32], 'block_order': (floordiv(block_idx, 2)*2), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 4}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [64, 1, 8, 16], 'thread': [2, 1, 8, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [8, 1, 15, 16], 'thread': [8, 1, 1, 16], 'rstep': [4, 32], 'block_order': (floordiv(block_idx, 2)*2), 'vectorize': {'p0': 8, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 1, 8, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [4, 1, 15, 16], 'thread': [4, 1, 1, 16], 'rstep': [8, 32], 'block_order': (floordiv(block_idx, 2)*2), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 1, 8, 16], 'thread': [4, 1, 2, 8], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 1920, 3670016)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [32, 1, 15, 8], 'thread': [32, 1, 1, 4], 'rstep': [1, 32], 'block_order': ((floordiv(block_idx, 4)*4) + floormod(block_idx, 2)), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8, 'B_decode': 2}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [32, 1, 8, 8], 'thread': [2, 1, 8, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 7680, 29360128)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [16, 1, 15, 8], 'thread': [16, 1, 1, 8], 'rstep': [2, 32], 'block_order': ((floordiv(block_idx, 4)*4) + floormod(block_idx, 2)), 'vectorize': {'p0': 8, 'B_decode': 4}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 1, 8, 8], 'thread': [2, 1, 8, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [256, 1, 1, 16], 'thread': [64, 1, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 8}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [256, 1, 1, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Compiler timeout.
['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [128, 1, 1, 16], 'thread': [32, 1, 1, 4], 'rstep': [4, 32], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [128, 1, 1, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 4096, 117440512)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [8, 1, 15, 8], 'thread': [8, 1, 1, 8], 'rstep': [4, 32], 'block_order': ((floordiv(block_idx, 4)*4) + floormod(block_idx, 2)), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 1, 8, 8], 'thread': [8, 1, 1, 8], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [64, 1, 1, 16], 'thread': [32, 1, 1, 4], 'rstep': [8, 32], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [64, 1, 1, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 2048, 58720256)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [64, 1, 15, 8], 'thread': [64, 1, 1, 2], 'rstep': [1, 32], 'block_order': ((floordiv(block_idx, 4)*4) + floormod(block_idx, 2)), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 2}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [64, 1, 8, 8], 'thread': [2, 1, 8, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 15360, 58720256)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [4, 1, 15, 8], 'thread': [4, 1, 1, 8], 'rstep': [8, 32], 'block_order': ((floordiv(block_idx, 4)*4) + floormod(block_idx, 2)), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 1, 8, 8], 'thread': [4, 1, 1, 8], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 960, 3670016)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [2, 1, 15, 16], 'thread': [2, 1, 1, 16], 'rstep': [16, 32], 'block_order': (floordiv(block_idx, 2)*2), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [2, 1, 8, 16], 'thread': [2, 1, 2, 8], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 960, 1835008)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [32, 1, 1, 16], 'thread': [16, 1, 1, 8], 'rstep': [16, 32], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [32, 1, 1, 16], 'thread': [16, 1, 1, 8], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 1024, 29360128)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [2, 1, 15, 8], 'thread': [2, 1, 1, 8], 'rstep': [16, 32], 'block_order': ((floordiv(block_idx, 4)*4) + floormod(block_idx, 2)), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [2, 1, 8, 8], 'thread': [2, 1, 1, 8], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 480, 1835008)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [32, 1, 15, 4], 'thread': [32, 1, 1, 4], 'rstep': [1, 32], 'block_order': ((floordiv(block_idx, 8)*8) + floormod(block_idx, 4)), 'vectorize': {'p0': 8}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [32, 1, 8, 4], 'thread': [4, 1, 8, 4], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 3840, 29360128)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [16, 1, 1, 16], 'thread': [16, 1, 1, 8], 'rstep': [16, 32], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 1, 1, 16], 'thread': [16, 1, 1, 8], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 512, 14680064)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [16, 1, 15, 4], 'thread': [16, 1, 1, 4], 'rstep': [2, 32], 'block_order': ((floordiv(block_idx, 8)*8) + floormod(block_idx, 4)), 'vectorize': {'p0': 16, 'B_decode': 4}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 1, 8, 4], 'thread': [2, 1, 8, 4], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [256, 1, 1, 8], 'thread': [64, 1, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 4}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [256, 1, 1, 8], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Compiler timeout.
['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [8, 1, 15, 4], 'thread': [8, 1, 1, 4], 'rstep': [4, 32], 'block_order': ((floordiv(block_idx, 8)*8) + floormod(block_idx, 4)), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 1, 8, 4], 'thread': [8, 1, 1, 4], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [128, 1, 1, 8], 'thread': [64, 1, 1, 2], 'rstep': [4, 32], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 8}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [128, 1, 1, 8], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 2048, 117440512)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [64, 1, 1, 8], 'thread': [32, 1, 1, 4], 'rstep': [8, 32], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [64, 1, 1, 8], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 1024, 58720256)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [4, 1, 15, 4], 'thread': [4, 1, 1, 4], 'rstep': [8, 32], 'block_order': ((floordiv(block_idx, 8)*8) + floormod(block_idx, 4)), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 1, 8, 4], 'thread': [4, 1, 1, 4], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 480, 3670016)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [1, 1, 15, 16], 'thread': [1, 1, 1, 16], 'rstep': [16, 32], 'block_order': (floordiv(block_idx, 2)*2), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [1, 1, 8, 16], 'thread': [1, 1, 2, 8], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 480, 57344)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [32, 1, 1, 8], 'thread': [32, 1, 1, 4], 'rstep': [16, 32], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [32, 1, 1, 8], 'thread': [16, 1, 1, 8], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 512, 29360128)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [64, 1, 15, 4], 'thread': [64, 1, 1, 2], 'rstep': [1, 32], 'block_order': ((floordiv(block_idx, 8)*8) + floormod(block_idx, 4)), 'step': [1, 1, 1, 2], 'vectorize': {'p0': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [64, 1, 8, 4], 'thread': [4, 1, 8, 4], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 7680, 58720256)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [2, 1, 15, 4], 'thread': [2, 1, 1, 4], 'rstep': [16, 32], 'block_order': ((floordiv(block_idx, 8)*8) + floormod(block_idx, 4)), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [2, 1, 8, 4], 'thread': [2, 1, 1, 4], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 256, 1835008)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [1, 1, 15, 8], 'thread': [1, 1, 1, 8], 'rstep': [32, 32], 'block_order': ((floordiv(block_idx, 4)*4) + floormod(block_idx, 2)), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [1, 1, 8, 8], 'thread': [1, 1, 1, 8], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 256, 57344)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [8, 1, 1, 16], 'thread': [8, 1, 1, 16], 'rstep': [32, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 1, 1, 16], 'thread': [8, 1, 1, 16], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 256, 7340032)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [16, 1, 1, 8], 'thread': [16, 1, 1, 8], 'rstep': [32, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 1, 1, 8], 'thread': [16, 1, 1, 8], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 256, 14680064)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [1, 1, 15, 4], 'thread': [1, 1, 1, 4], 'rstep': [32, 32], 'block_order': ((floordiv(block_idx, 8)*8) + floormod(block_idx, 4)), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [1, 1, 8, 4], 'thread': [1, 1, 1, 4], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 128, 57344)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [32, 1, 15, 2], 'thread': [32, 1, 1, 2], 'rstep': [1, 32], 'block_order': ((floordiv(block_idx, 16)*16) + floormod(block_idx, 8)), 'vectorize': {'p0': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [32, 1, 8, 2], 'thread': [4, 1, 8, 2], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 1920, 29360128)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [16, 1, 15, 2], 'thread': [16, 1, 1, 2], 'rstep': [2, 32], 'block_order': ((floordiv(block_idx, 16)*16) + floormod(block_idx, 8)), 'vectorize': {'p0': 16, 'B_decode': 4}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 1, 8, 2], 'thread': [2, 1, 8, 2], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [8, 1, 1, 8], 'thread': [8, 1, 1, 8], 'rstep': [32, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 1, 1, 8], 'thread': [8, 1, 1, 8], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 128, 7340032)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [8, 1, 15, 2], 'thread': [8, 1, 1, 2], 'rstep': [4, 32], 'block_order': ((floordiv(block_idx, 16)*16) + floormod(block_idx, 8)), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 1, 8, 2], 'thread': [1, 1, 8, 2], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [4, 1, 15, 2], 'thread': [4, 1, 1, 2], 'rstep': [8, 32], 'block_order': ((floordiv(block_idx, 16)*16) + floormod(block_idx, 8)), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 1, 8, 2], 'thread': [4, 1, 1, 2], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 256, 3670016)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [2, 1, 15, 2], 'thread': [2, 1, 1, 2], 'rstep': [16, 32], 'block_order': ((floordiv(block_idx, 16)*16) + floormod(block_idx, 8)), 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [2, 1, 8, 2], 'thread': [2, 1, 1, 2], 'rstep': [], 'step': [1, 1, 4, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 128, 1835008)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [4, 1, 1, 16], 'thread': [4, 1, 1, 16], 'rstep': [32, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 1, 1, 16], 'thread': [4, 1, 1, 16], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 128, 3670016)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27>: {'block': [64, 1, 15, 2], 'thread': [64, 1, 1, 2], 'rstep': [1, 32], 'block_order': ((floordiv(block_idx, 16)*16) + floormod(block_idx, 8)), 'vectorize': {'p0': 16}}, <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [64, 1, 8, 2], 'thread': [8, 1, 8, 2], 'rstep': [], 'step': [4, 1, 1, 1]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>, Tensor(shape=[256, 1792, 16, 16], op.name=p1), 3840, 58720256)

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 14, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 16, 16, 16], 'thread': [8, 8, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  70: TVMFuncCall
  69: _ZN3tvm7runtime13PackedFuncObj
  68: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  67: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  66: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  65: tvm::transform::Pass::operator()(tvm::IRModule) const
  64: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  63: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  62: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  61: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  60: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  59: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  58: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  57: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 8, 16, 16], 'thread': [16, 4, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  70: TVMFuncCall
  69: _ZN3tvm7runtime13PackedFuncObj
  68: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  67: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  66: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  65: tvm::transform::Pass::operator()(tvm::IRModule) const
  64: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  63: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  62: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  61: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  60: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  59: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  58: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  57: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 28, 16, 8], 'thread': [8, 4, 1, 4], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 32, 16, 8], 'thread': [8, 8, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 7, 16, 16], 'thread': [16, 1, 1, 8], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 16, 16, 8], 'thread': [16, 4, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 14, 16, 8], 'thread': [16, 2, 1, 4], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 56, 16, 4], 'thread': [8, 8, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  66: TVMFuncCall
  65: _ZN3tvm7runtime13PackedFuncObj
  64: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  63: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  62: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  61: tvm::transform::Pass::operator()(tvm::IRModule) const
  60: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  59: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  58: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  57: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  56: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  55: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  51: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  49: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  48: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  46: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  45: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 64, 16, 4], 'thread': [8, 8, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  70: TVMFuncCall
  69: _ZN3tvm7runtime13PackedFuncObj
  68: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  67: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  66: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  65: tvm::transform::Pass::operator()(tvm::IRModule) const
  64: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  63: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  62: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  61: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  60: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  59: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  58: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  57: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 32, 16, 4], 'thread': [16, 4, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  70: TVMFuncCall
  69: _ZN3tvm7runtime13PackedFuncObj
  68: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  67: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  66: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  65: tvm::transform::Pass::operator()(tvm::IRModule) const
  64: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  63: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  62: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  61: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  60: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  59: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  58: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  57: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 28, 16, 4], 'thread': [16, 4, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 8, 16, 16], 'thread': [8, 4, 1, 4], 'rstep': [2, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 16, 8, 16], 'thread': [8, 16, 1, 1], 'rstep': [2, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  66: TVMFuncCall
  65: _ZN3tvm7runtime13PackedFuncObj
  64: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  63: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  62: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  61: tvm::transform::Pass::operator()(tvm::IRModule) const
  60: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  59: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  58: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  57: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  56: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  55: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  51: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  49: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  48: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  46: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  45: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 14, 8, 16], 'thread': [8, 2, 1, 8], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 16, 16, 8], 'thread': [8, 4, 1, 4], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  70: TVMFuncCall
  69: _ZN3tvm7runtime13PackedFuncObj
  68: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  67: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  66: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  65: tvm::transform::Pass::operator()(tvm::IRModule) const
  64: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  63: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  62: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  61: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  60: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  59: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  58: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  57: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 7, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 28, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [1, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 128, 16, 2], 'thread': [8, 8, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  66: TVMFuncCall
  65: _ZN3tvm7runtime13PackedFuncObj
  64: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  63: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  62: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  61: tvm::transform::Pass::operator()(tvm::IRModule) const
  60: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  59: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  58: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  57: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  56: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  55: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  51: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  49: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  48: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  46: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  45: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 256, 16, 1], 'thread': [8, 16, 1, 1], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 16, 16, 16], 'thread': [8, 8, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  70: TVMFuncCall
  69: _ZN3tvm7runtime13PackedFuncObj
  68: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  67: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  66: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  65: tvm::transform::Pass::operator()(tvm::IRModule) const
  64: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  63: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  62: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  61: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  60: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  59: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  58: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  57: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 14, 16, 8], 'thread': [8, 2, 2, 4], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 32, 16, 16], 'thread': [4, 16, 1, 2], 'rstep': [1, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  70: TVMFuncCall
  69: _ZN3tvm7runtime13PackedFuncObj
  68: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  67: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  66: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  65: tvm::transform::Pass::operator()(tvm::IRModule) const
  64: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  63: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  62: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  61: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  60: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  59: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  58: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  57: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 56, 16, 8], 'thread': [4, 8, 1, 4], 'rstep': [1, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  72: TVMFuncCall
  71: _ZN3tvm7runtime13PackedFuncObj
  70: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  69: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  68: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  67: tvm::transform::Pass::operator()(tvm::IRModule) const
  66: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  65: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  64: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  63: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  62: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  61: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  59: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  58: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  57: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  56: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  55: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  54: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  53: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  52: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  51: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  49: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  48: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  46: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  45: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  42: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  41: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  40: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  39: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  38: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  37: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  36: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  35: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  34: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  33: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  32: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  31: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  30: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  21: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  20: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  19: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  18: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  17: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  16: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  15: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  14: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 14, 16, 16], 'thread': [16, 2, 1, 4], 'rstep': [2, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 112, 16, 2], 'thread': [8, 8, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  66: TVMFuncCall
  65: _ZN3tvm7runtime13PackedFuncObj
  64: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  63: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  62: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  61: tvm::transform::Pass::operator()(tvm::IRModule) const
  60: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  59: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  58: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  57: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  56: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  55: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  51: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  49: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  48: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  46: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  45: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 224, 16, 1], 'thread': [8, 16, 1, 1], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  66: TVMFuncCall
  65: _ZN3tvm7runtime13PackedFuncObj
  64: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  63: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  62: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  61: tvm::transform::Pass::operator()(tvm::IRModule) const
  60: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  59: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  58: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  57: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  56: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  55: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  51: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  49: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  48: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  46: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  45: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 32, 16, 4], 'thread': [8, 4, 1, 4], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  66: TVMFuncCall
  65: _ZN3tvm7runtime13PackedFuncObj
  64: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  63: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  62: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  61: tvm::transform::Pass::operator()(tvm::IRModule) const
  60: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  59: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  58: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  57: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  56: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  55: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  51: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  49: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  48: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  46: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  45: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 32, 8, 8], 'thread': [8, 16, 1, 1], 'rstep': [2, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  70: TVMFuncCall
  69: _ZN3tvm7runtime13PackedFuncObj
  68: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  67: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  66: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  65: tvm::transform::Pass::operator()(tvm::IRModule) const
  64: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  63: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  62: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  61: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  60: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  59: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  58: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  57: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 28, 8, 8], 'thread': [8, 4, 1, 4], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 64, 16, 8], 'thread': [4, 16, 1, 2], 'rstep': [1, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 16, 16, 16], 'thread': [4, 8, 1, 4], 'rstep': [2, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 112, 16, 4], 'thread': [4, 16, 1, 2], 'rstep': [1, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  70: TVMFuncCall
  69: _ZN3tvm7runtime13PackedFuncObj
  68: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  67: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  66: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  65: tvm::transform::Pass::operator()(tvm::IRModule) const
  64: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  63: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  62: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  61: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  60: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  59: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  58: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  57: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  56: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  55: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  52: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  50: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  49: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  47: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  46: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  44: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 64, 16, 2], 'thread': [16, 4, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  66: TVMFuncCall
  65: _ZN3tvm7runtime13PackedFuncObj
  64: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  63: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  62: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  61: tvm::transform::Pass::operator()(tvm::IRModule) const
  60: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  59: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  58: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  57: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  56: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  55: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  51: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  49: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  48: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  46: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  45: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 224, 16, 2], 'thread': [4, 16, 1, 2], 'rstep': [1, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  66: TVMFuncCall
  65: _ZN3tvm7runtime13PackedFuncObj
  64: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  63: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  62: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  61: tvm::transform::Pass::operator()(tvm::IRModule) const
  60: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  59: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  58: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  57: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  56: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  55: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  51: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  49: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  48: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  46: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  45: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 56, 16, 2], 'thread': [16, 4, 1, 2], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [8, 28, 16, 4], 'thread': [8, 4, 2, 2], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 448, 16, 1], 'thread': [4, 32, 1, 1], 'rstep': [1, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 227, in build
    input_mod = lower(inputs, args, name=name, binds=binds)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 134, in lower
    return ffi.lower_schedule(inp, args, name, binds, simple_mode)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  66: TVMFuncCall
  65: _ZN3tvm7runtime13PackedFuncObj
  64: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  63: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)
  62: tvm::LowerWithPassList(tvm::IRModule, tvm::runtime::Array<tvm::transform::Pass, void>)
  61: tvm::transform::Pass::operator()(tvm::IRModule) const
  60: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  59: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  58: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  57: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  56: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_3tir8PrimFuncES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_9transform10UnrollLoopEvEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SG_SK_
  55: tvm::tir::UnrollLoop(tvm::tir::Stmt, tvm::tir::UnrollLoopConfig)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  53: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  52: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  51: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  49: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  48: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  46: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  45: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  43: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AllocateNode const*)
  42: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  40: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::AttrStmtNode const*)
  39: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::AttrStmtNode const*)
  38: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  36: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  35: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  34: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  32: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  31: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  28: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  27: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  26: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  25: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  24: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  22: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  20: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  19: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  18: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  16: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  15: tvm::tir::StmtMutator::VisitStmt_(tvm::tir::ForNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  12: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  11: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  10: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  8: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9Ob
  6: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)
  5: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  4: tvm::tir::StmtMutator::VisitSeqStmt_(tvm::tir::SeqStmtNode const*, bool, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)::{lambda(tvm::tir::SeqStmtNode const*)#1}::operator()(tvm::tir::SeqStmtNode const*) const [clone .isra.0]
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::function<tvm::tir::Stmt (tvm::tir::Stmt const&)>)
  2: std::_Function_handler<tvm::tir::Stmt (tvm::tir::Stmt const&), tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::SeqStmtNode const*)::{lambda(tvm::tir::Stmt const&)#1}>::_M_invoke(std::_Any_data const&, tvm::tir::Stmt const&)
  1: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime
  0: tvm::tir::LoopUnroller::VisitStmt_(tvm::tir::ForNode const*)
  File "/root/Ladder/3rdparty/tvm/src/tir/transforms/unroll_loop.cc", line 110
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: value >= 0 (-1 vs. 0) : Cannot unroll non-constant loop

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [16, 32, 16, 8], 'thread': [16, 8, 1, 1], 'rstep': [2, 32], 'step': [1, 1, 1, 4], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_27', 'sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_27__sigmoid_multiply_reshape_multiply_reshape_cast_cast_reshape_28>: {'block': [4, 14, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [2, 32], 'step': [1, 1, 4, 1], 'vectorize': {'input0': 16, 'mediate0': 16}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/te_base.py", line 64, in build
    mod = tvm.build(self.sche, self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  46: TVMFuncCall
  45: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  44: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  43: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  42: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  41: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  40: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  37: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  36: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  34: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  32: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  21: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  20: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  19: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  17: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  14: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  13: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  12: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  10: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  9: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  8: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  7: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  6: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::CallNode const*, std::ostream&)
  4: tvm::codegen::PrintBinaryIntrinsic(tvm::tir::CallNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::BroadcastNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1226
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (p) is false: 

['ladder_perfect_quant_linear_cast_29', 'reshape_add_30']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_29>: {'block': [1, 64, 16, 16], 'thread': [1, 64, 2, 1], 'rstep': [1, 32], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 4, 'B_decode': 16}}, <Node, reshape_add_30>: {'block': [1, 2, 8192], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_30>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 32768, 262144)

['ladder_perfect_quant_linear_cast_29', 'reshape_add_30']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_29>: {'block': [1, 1, 8, 16], 'thread': [1, 1, 8, 16], 'rstep': [32, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_30>: {'block': [1, 1, 128], 'thread': [1, 1, 128], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_30>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 256, 512)

['ladder_perfect_quant_linear_cast_29', 'reshape_add_30']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_29>: {'block': [1, 1, 4, 16], 'thread': [1, 1, 4, 16], 'rstep': [32, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_30>: {'block': [1, 1, 64], 'thread': [1, 1, 64], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_30>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 128, 512)

['ladder_perfect_quant_linear_cast_29', 'reshape_add_30']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_29>: {'block': [1, 1, 2, 16], 'thread': [1, 1, 2, 16], 'rstep': [32, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_30>: {'block': [1, 1, 32], 'thread': [1, 1, 32], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_30>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 64, 512)

['ladder_perfect_quant_linear_cast_29', 'reshape_add_30']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_29>: {'block': [1, 1, 1, 8], 'thread': [1, 1, 1, 8], 'rstep': [64, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_30>: {'block': [1, 1, 8], 'thread': [1, 1, 8], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_30>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 32, 512)

['ladder_perfect_quant_linear_cast_29', 'reshape_add_30']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_29>: {'block': [1, 1, 1, 4], 'thread': [1, 1, 1, 4], 'rstep': [128, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_30>: {'block': [1, 1, 4], 'thread': [1, 1, 4], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_30>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 32, 512)

['ladder_perfect_quant_linear_cast_29', 'reshape_add_30']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_cast_29>: {'block': [1, 1, 1, 2], 'thread': [1, 1, 1, 2], 'rstep': [224, 32], 'vectorize': {'p0': 16, 'B_decode': 16}}, <Node, reshape_add_30>: {'block': [1, 1, 2], 'thread': [1, 1, 2], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_add_30>, Tensor(shape=[256, 512, 16, 16], op.name=p0), 32, 512)

