['ladder_perfect_quant_linear_3', 'layout_transform_reshape_reshape_reshape_transpose_4']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_3>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'block_order': (((floordiv(floormod(block_idx, 2048), 8)*512) + (floordiv(block_idx, 2048)*8)) + floormod(block_idx, 8)), 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 16, 16], 'thread': [1, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 4, 128], 'thread': [1, 1, 4, 32], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 4, 128], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 1024, 2048)

['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 8, 128], 'thread': [1, 1, 8, 16], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 8, 128], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 2048, 4096)

['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 4, 128], 'thread': [1, 2, 4, 16], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 4, 128], 'thread': [2, 4, 16], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 64, 128], 'thread': [1, 1, 64, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 64, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 16384, 32768)

['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 32, 128], 'thread': [1, 2, 32, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 32, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 4, 16, 128], 'thread': [1, 4, 16, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 16, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 8, 8, 128], 'thread': [1, 8, 8, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 8, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 16, 4, 128], 'thread': [1, 16, 4, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 4, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 32, 128], 'thread': [1, 1, 32, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 32, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 8192, 16384)

['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 16, 128], 'thread': [1, 2, 16, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 16, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 4, 8, 128], 'thread': [1, 4, 8, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 8, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 8, 4, 128], 'thread': [1, 8, 4, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 4, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 16, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 4096, 8192)

['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 8, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 8, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 4, 4, 128], 'thread': [1, 4, 4, 8], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 4, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 2, 128], 'thread': [1, 2, 2, 32], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 2, 128], 'thread': [2, 2, 32], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 2, 128], 'thread': [1, 1, 2, 64], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 2, 128], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 512, 1024)

['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 4, 2, 128], 'thread': [1, 4, 2, 16], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 2, 128], 'thread': [4, 2, 16], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 32, 2, 128], 'thread': [1, 32, 2, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [32, 2, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 16, 2, 128], 'thread': [1, 16, 2, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 2, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 8, 2, 128], 'thread': [1, 8, 2, 8], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 2, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 128, 128], 'thread': [1, 1, 128, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 128, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 32768, 65536)

['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 64, 128], 'thread': [1, 2, 64, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 64, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 4, 32, 128], 'thread': [1, 4, 32, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 32, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 8, 16, 128], 'thread': [1, 8, 16, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 16, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 16, 8, 128], 'thread': [1, 16, 8, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 8, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 32, 4, 128], 'thread': [1, 32, 4, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [32, 4, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 4, 1, 128], 'thread': [1, 4, 1, 32], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 1, 128], 'thread': [4, 1, 32], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 1, 128], 'thread': [1, 2, 1, 64], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 1, 128], 'thread': [2, 1, 64], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 512, 4194304)

['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 1, 128], 'thread': [1, 1, 1, 128], 'rstep': [], 'block_order': int64(block_idx)}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 1, 128], 'thread': [1, 1, 128], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, Tensor(shape=[1, 64, 4096, 128], op.name=p0), 256, 512)

['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 64, 2, 128], 'thread': [1, 64, 2, 1], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [64, 2, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 8, 1, 128], 'thread': [1, 8, 1, 16], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 1, 128], 'thread': [8, 1, 16], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 64, 1, 128], 'thread': [1, 64, 1, 2], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [64, 1, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 32, 1, 128], 'thread': [1, 32, 1, 4], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [32, 1, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
{'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 16, 1, 128], 'thread': [1, 16, 1, 8], 'rstep': [], 'block_order': int64(block_idx), 'step': [1, 1, 1, 2]}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 1, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Compiler timeout.
['multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5', 'ladder_perfect_quant_linear_6', 'layout_transform_reshape_reshape_reshape_transpose_7', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8', 'nn_batch_matmul_9']
None
object of type 'NoneType' has no len()
['ladder_perfect_quant_linear_6', 'layout_transform_reshape_reshape_reshape_transpose_7']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_6>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'block_order': (((floordiv(floormod(block_idx, 2048), 8)*64) + (floordiv(block_idx, 2048)*8)) + floormod(block_idx, 8)), 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 16, 16], 'thread': [1, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 128], 'warp': [1, 32, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_divide_10>: {'block': [1, 1, 64, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 64], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_divide_10>: {'block': [1, 1, 128, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 64], 'warp': [2, 32, 32], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_divide_10>: {'block': [1, 2, 64, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 18432, 67485696)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_divide_10>: {'block': [1, 1, 64, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 256], 'warp': [1, 32, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}, <Node, reshape_divide_10>: {'block': [1, 1, 32, 256], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 32], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 1, 256, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 128], 'warp': [1, 64, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_divide_10>: {'block': [1, 1, 128, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 128], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_divide_10>: {'block': [1, 1, 32, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 1, 128, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_divide_10>: {'block': [1, 1, 32, 64], 'thread': [1, 1, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 1, 64, 32], 'thread': [1, 1, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 256], 'warp': [1, 32, 128], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}, <Node, reshape_divide_10>: {'block': [1, 1, 64, 256], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 64], 'warp': [1, 128, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_divide_10>: {'block': [1, 1, 256, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 64], 'warp': [2, 16, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_divide_10>: {'block': [1, 2, 32, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 9216, 67485696)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 32], 'warp': [2, 32, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 2, 64, 32], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 10240, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 128], 'warp': [2, 32, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_divide_10>: {'block': [1, 2, 64, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 34816, 68583424)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 128, 64], 'warp': [2, 64, 32], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_divide_10>: {'block': [1, 2, 128, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 36864, 67485696)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 128], 'warp': [2, 16, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_divide_10>: {'block': [1, 2, 32, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 17408, 68583424)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 128, 32], 'warp': [2, 64, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 2, 128, 32], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 20480, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 1, 32, 32], 'thread': [1, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 32], 'warp': [2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 2, 32, 32], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 5120, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 32, 32], 'warp': [4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 4, 32, 32], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 32, 64], 'warp': [4, 16, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_divide_10>: {'block': [1, 4, 32, 64], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 64, 32], 'warp': [4, 32, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 4, 64, 32], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 16, 256], 'warp': [1, 8, 128], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}, <Node, reshape_divide_10>: {'block': [1, 1, 16, 256], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 8448, 134688)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 16], 'warp': [1, 128, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}, <Node, reshape_divide_10>: {'block': [1, 1, 256, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 12288, 2108416)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 16, 128], 'warp': [1, 8, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_divide_10>: {'block': [1, 1, 16, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 4352, 133952)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 16], 'warp': [1, 64, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}, <Node, reshape_divide_10>: {'block': [1, 1, 128, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 6144, 1054208)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 64, 64], 'warp': [4, 32, 32], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_divide_10>: {'block': [1, 4, 64, 64], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 16, 128], 'warp': [2, 8, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_divide_10>: {'block': [1, 2, 16, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 8704, 68583424)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 128, 16], 'warp': [2, 64, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}, <Node, reshape_divide_10>: {'block': [1, 2, 128, 16], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 12288, 67469312)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 512], 'warp': [1, 32, 128], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 520>}}, <Node, reshape_divide_10>: {'block': [1, 1, 32, 512], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 512, 32], 'warp': [1, 128, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 1, 512, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 16, 64], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_divide_10>: {'block': [1, 1, 16, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 2304, 131808)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 16], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}, <Node, reshape_divide_10>: {'block': [1, 1, 64, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 3072, 527104)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 16, 64], 'warp': [2, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_divide_10>: {'block': [1, 2, 16, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 4608, 67485696)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 16], 'warp': [2, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}, <Node, reshape_divide_10>: {'block': [1, 2, 64, 16], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 6144, 67469312)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 256], 'warp': [2, 16, 128], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}, <Node, reshape_divide_10>: {'block': [1, 2, 32, 256], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 33792, 68960256)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 256, 32], 'warp': [2, 128, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 2, 256, 32], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 40960, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 16, 32], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_divide_10>: {'block': [1, 2, 16, 32], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_divide_10>, Tensor(shape=[64, 4096, 4096], op.name=p0), 2560, 67108864)

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 256], 'warp': [1, 32, 128], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 512], 'warp': [1, 32, 256], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 520>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 256], 'warp': [1, 32, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 512], 'warp': [1, 32, 128], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 520>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 16, 256], 'warp': [1, 8, 128], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 8, 256], 'warp': [1, 8, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 16, 512], 'warp': [1, 8, 256], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 520>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 128], 'warp': [1, 64, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 256], 'warp': [1, 64, 128], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 128], 'warp': [1, 32, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 512], 'warp': [1, 64, 256], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 520>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 128], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 16, 128], 'warp': [1, 8, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 8, 512], 'warp': [1, 8, 128], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 520>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 8, 128], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 256, 64], 'warp': [1, 128, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 256, 128], 'warp': [1, 128, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 64], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 256, 256], 'warp': [1, 128, 128], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 16, 64], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 8, 64], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 512, 64], 'warp': [1, 256, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 256, 32], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 512, 128], 'warp': [1, 256, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 16, 32], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 8, 32], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 512, 32], 'warp': [1, 128, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 256, 16], 'warp': [1, 128, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 16], 'warp': [1, 64, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 16], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 16], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 16, 16], 'warp': [1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 256, 8], 'warp': [1, 64, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 512, 16], 'warp': [1, 256, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9', 'reshape_divide_10']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 8], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 128], 'warp': [1, 64, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 256], 'warp': [1, 32, 128], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 64], 'warp': [1, 128, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 128], 'warp': [1, 32, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 64], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 256], 'warp': [1, 64, 128], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 128], 'warp': [1, 128, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 256], 'warp': [1, 32, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 32], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 128], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 512], 'warp': [1, 32, 256], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 520>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 512, 64], 'warp': [1, 256, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 256], 'warp': [1, 128, 128], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 264>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 512], 'warp': [1, 64, 256], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 520>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 512, 128], 'warp': [1, 256, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_9']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 512], 'warp': [1, 32, 128], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 520>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 4, 1], 'thread': [1, 1, 4, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 2, 1], 'thread': [1, 2, 2, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 1, 1], 'thread': [1, 4, 1, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 2, 1], 'thread': [1, 1, 2, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 1, 1], 'thread': [1, 2, 1, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 1, 1], 'thread': [1, 1, 1, 1], 'rstep': [4096], 'reduce_thread': [128], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 8, 1], 'thread': [1, 1, 8, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 4, 1], 'thread': [1, 2, 4, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 2, 1], 'thread': [1, 4, 2, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 8, 1, 1], 'thread': [1, 8, 1, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 64, 1], 'thread': [1, 1, 64, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 32, 1], 'thread': [1, 2, 32, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 16, 1], 'thread': [1, 4, 16, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 8, 8, 1], 'thread': [1, 8, 8, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 16, 4, 1], 'thread': [1, 16, 4, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 32, 2, 1], 'thread': [1, 32, 2, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 64, 1, 1], 'thread': [1, 64, 1, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 32, 1], 'thread': [1, 1, 32, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 16, 1], 'thread': [1, 2, 16, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 8, 1], 'thread': [1, 4, 8, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 8, 4, 1], 'thread': [1, 8, 4, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 16, 2, 1], 'thread': [1, 16, 2, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 32, 1, 1], 'thread': [1, 32, 1, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 16, 1], 'thread': [1, 1, 16, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 8, 1], 'thread': [1, 2, 8, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 4, 1], 'thread': [1, 4, 4, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 8, 2, 1], 'thread': [1, 8, 2, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 16, 1, 1], 'thread': [1, 16, 1, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['max_11', 'subtract_exp_12', 'sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 2, 1, 4096], 'thread': [1, 2, 1, 64], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, subtract_exp_12>: {'block': [1, 2, 1, 4096], 'thread': [1, 2, 1, 64], 'rstep': [], 'step': [1, 1, 1, 2]}, <Node, sum_13>: {'block': [1, 2, 1, 4096], 'thread': [1, 2, 1, 64], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 1, 4096], 'thread': [2, 1, 64], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, divide_cast_cast_reshape_14>, Tensor(shape=[1, 64, 4096, 4096], op.name=p0), 16384, 67108864)

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 4, 1], 'thread': [1, 1, 4, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 2, 1], 'thread': [1, 1, 2, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 1, 1], 'thread': [1, 1, 1, 1], 'rstep': [4096], 'reduce_thread': [128], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 8, 1], 'thread': [1, 1, 8, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 64, 1], 'thread': [1, 1, 64, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 32, 1], 'thread': [1, 1, 32, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 16, 1], 'thread': [1, 1, 16, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 32, 1], 'thread': [1, 2, 32, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 16, 1], 'thread': [1, 4, 16, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 16, 1], 'thread': [1, 2, 16, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 8, 8, 1], 'thread': [1, 8, 8, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 8, 1], 'thread': [1, 4, 8, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 8, 1], 'thread': [1, 2, 8, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 4, 1], 'thread': [1, 2, 4, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 16, 4, 1], 'thread': [1, 16, 4, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 8, 4, 1], 'thread': [1, 8, 4, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 4, 1], 'thread': [1, 4, 4, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 2, 1], 'thread': [1, 2, 2, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 2, 1], 'thread': [1, 4, 2, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 32, 2, 1], 'thread': [1, 32, 2, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 16, 2, 1], 'thread': [1, 16, 2, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 8, 2, 1], 'thread': [1, 8, 2, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 1, 1], 'thread': [1, 4, 1, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 1, 1], 'thread': [1, 2, 1, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 8, 1, 1], 'thread': [1, 8, 1, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14']
{'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 64, 1, 1], 'thread': [1, 64, 1, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'input0': 8}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['sum_13', 'divide_cast_cast_reshape_14', 'ladder_perfect_quant_linear_15', 'layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16', 'nn_batch_matmul_17']
None
object of type 'NoneType' has no len()
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 128], 'warp': [1, 32, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': ((floormod(block_idx, 64)*64) + floordiv(block_idx, 64)), 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 8, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 64], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 128), 2)*64) + (floordiv(block_idx, 128)*2)) + floormod(block_idx, 2)), 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 128), 2)*128) + (floordiv(block_idx, 128)*2)) + floormod(block_idx, 2)), 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 4, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 128], 'warp': [1, 64, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': ((floormod(block_idx, 64)*32) + floordiv(block_idx, 64)), 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 8, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 32], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 256), 4)*64) + (floordiv(block_idx, 256)*4)) + floormod(block_idx, 4)), 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [16, 2, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 128], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': ((floormod(block_idx, 64)*128) + floordiv(block_idx, 64)), 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 8, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 256), 4)*128) + (floordiv(block_idx, 256)*4)) + floormod(block_idx, 4)), 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 64], 'warp': [1, 128, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 128), 2)*32) + (floordiv(block_idx, 128)*2)) + floormod(block_idx, 2)), 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [16, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 128), 2)*256) + (floordiv(block_idx, 128)*2)) + floormod(block_idx, 2)), 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 4, 16, 16], 'thread': [2, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 256), 4)*256) + (floordiv(block_idx, 256)*4)) + floormod(block_idx, 4)), 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 2, 16, 16], 'thread': [4, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 64, 128], 'warp': [2, 32, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': ((floormod(block_idx, 32)*64) + floordiv(block_idx, 32)), 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 16, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 34816, 71303168)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 32, 128], 'warp': [2, 16, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': ((floormod(block_idx, 32)*128) + floordiv(block_idx, 32)), 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 16, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 17408, 71303168)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 256), 4)*512) + (floordiv(block_idx, 256)*4)) + floormod(block_idx, 4)), 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 2, 16, 16], 'thread': [2, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 16], 'warp': [1, 128, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 512), 8)*128) + (floordiv(block_idx, 512)*8)) + floormod(block_idx, 8)), 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [16, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 12288, 72876032)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 512, 32], 'warp': [1, 128, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 256), 4)*32) + (floordiv(block_idx, 256)*4)) + floormod(block_idx, 4)), 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [32, 2, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 16, 128], 'warp': [2, 8, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': ((floormod(block_idx, 32)*256) + floordiv(block_idx, 32)), 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 16, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 8704, 71303168)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 16, 128], 'warp': [1, 8, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': ((floormod(block_idx, 64)*256) + floordiv(block_idx, 64)), 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 4352, 71303168)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 16], 'warp': [1, 64, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 512), 8)*256) + (floordiv(block_idx, 512)*8)) + floormod(block_idx, 8)), 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 6144, 72876032)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 16, 64], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 128), 2)*512) + (floordiv(block_idx, 128)*2)) + floormod(block_idx, 2)), 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 4, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 2304, 74973184)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 16], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 512), 8)*512) + (floordiv(block_idx, 512)*8)) + floormod(block_idx, 8)), 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 1, 16, 16], 'thread': [4, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 3072, 72876032)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 16, 32], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 256), 4)*1024) + (floordiv(block_idx, 256)*4)) + floormod(block_idx, 4)), 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 2, 16, 16], 'thread': [1, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 16], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 512), 8)*1024) + (floordiv(block_idx, 512)*8)) + floormod(block_idx, 8)), 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 1, 16, 16], 'thread': [2, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 16, 16], 'warp': [1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 512), 8)*2048) + (floordiv(block_idx, 512)*8)) + floormod(block_idx, 8)), 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 1, 16, 16], 'thread': [1, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 8], 'warp': [1, 64, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 1024), 16)*256) + (floordiv(block_idx, 1024)*16)) + floormod(block_idx, 16)), 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [16, 1, 8, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 8192, 71303168)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 512, 16], 'warp': [1, 256, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 512), 8)*64) + (floordiv(block_idx, 512)*8)) + floormod(block_idx, 8)), 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [32, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 24576, 72876032)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 8], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 1024), 16)*512) + (floordiv(block_idx, 1024)*16)) + floormod(block_idx, 16)), 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 1, 8, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 4096, 71303168)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 8], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 1024), 16)*1024) + (floordiv(block_idx, 1024)*16)) + floormod(block_idx, 16)), 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 1, 8, 16], 'thread': [4, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [4, 16, 128], 'warp': [4, 8, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': ((floormod(block_idx, 16)*256) + floordiv(block_idx, 16)), 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 32, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 17408, 71303168)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 8], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 1024), 16)*2048) + (floordiv(block_idx, 1024)*16)) + floormod(block_idx, 16)), 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 1, 8, 16], 'thread': [2, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Compiler timeout.
['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [4, 32, 128], 'warp': [4, 16, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': ((floormod(block_idx, 16)*128) + floordiv(block_idx, 16)), 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 32, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 34816, 71303168)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 512, 8], 'warp': [1, 128, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'block_order': (((floordiv(floormod(block_idx, 1024), 16)*128) + (floordiv(block_idx, 1024)*16)) + floormod(block_idx, 16)), 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [32, 1, 8, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 16384, 71303168)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 128], 'thread': [1, 16, 8], 'rstep': [64], 'block_order': ((floormod(block_idx, 64)*32) + floordiv(block_idx, 64)), 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 8, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 32768, 67108864)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 128], 'thread': [1, 8, 16], 'rstep': [64], 'block_order': ((floormod(block_idx, 64)*64) + floordiv(block_idx, 64)), 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 8, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 16384, 67108864)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 64], 'thread': [1, 16, 8], 'rstep': [64], 'block_order': (((floordiv(floormod(block_idx, 128), 2)*64) + (floordiv(block_idx, 128)*2)) + floormod(block_idx, 2)), 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 16384, 67108864)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 64], 'thread': [1, 16, 8], 'rstep': [64], 'block_order': (((floordiv(floormod(block_idx, 128), 2)*128) + (floordiv(block_idx, 128)*2)) + floormod(block_idx, 2)), 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 4, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 8192, 67108864)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 128], 'thread': [1, 8, 16], 'rstep': [64], 'block_order': ((floormod(block_idx, 64)*128) + floordiv(block_idx, 64)), 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 8, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 8192, 67108864)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 32], 'thread': [1, 32, 4], 'rstep': [64], 'block_order': (((floordiv(floormod(block_idx, 256), 4)*128) + (floordiv(block_idx, 256)*4)) + floormod(block_idx, 4)), 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 8192, 67108864)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 64], 'thread': [1, 32, 4], 'rstep': [64], 'block_order': (((floordiv(floormod(block_idx, 128), 2)*32) + (floordiv(block_idx, 128)*2)) + floormod(block_idx, 2)), 'step': [1, 1, 2], 'vectorize': {'p0': 8, 'p1': 8}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [16, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 32768, 67108864)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 120], 'thread': [1, 16, 8], 'rstep': [64], 'block_order': ((floordiv(floormod(block_idx, 128), 2)*64) + (floordiv(block_idx, 128)*2)), 'step': [1, 2, 1], 'vectorize': {'p0': 8, 'p1': 4}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 8, 8, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 30720, 67108864)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 120], 'thread': [1, 16, 8], 'rstep': [64], 'block_order': ((floordiv(floormod(block_idx, 128), 2)*128) + (floordiv(block_idx, 128)*2)), 'step': [1, 2, 1], 'vectorize': {'p0': 8, 'p1': 4}}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 8, 8, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, Tensor(shape=[64, 4096, 128], op.name=p0), 15360, 67108864)

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 128, 128], 'warp': [1, 64, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 64, 128], 'warp': [1, 32, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 256, 64], 'warp': [1, 128, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 128, 64], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 32, 128], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 256, 128], 'warp': [1, 128, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 64, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 32, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 16, 128], 'warp': [1, 8, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 256, 32], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 128, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 512, 64], 'warp': [1, 256, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 16, 64], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 512, 128], 'warp': [1, 256, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 64, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 32, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 16, 32], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 8, 128], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 8, 64], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 512, 32], 'warp': [1, 128, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 256, 16], 'warp': [1, 128, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 128, 16], 'warp': [1, 64, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 64, 16], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 32, 16], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 8, 32], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 16, 16], 'warp': [1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 512, 16], 'warp': [1, 256, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 24>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 256, 8], 'warp': [1, 64, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 128, 8], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 64, 8], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 32, 8], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 128, 128], 'warp': [2, 64, 64], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 64, 128], 'warp': [2, 32, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 128, 64], 'warp': [2, 64, 32], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 512, 8], 'warp': [1, 128, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 16>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 64, 64], 'warp': [2, 32, 32], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 64, 64], 'warp': [4, 32, 32], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 256, 32], 'warp': [2, 128, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 256, 64], 'warp': [2, 128, 32], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 32, 128], 'warp': [2, 16, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 37, in call_build
    cpresult.code,
AttributeError: 'NoneType' object has no attribute 'code'

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 128], 'warp': [1, 64, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 64], 'warp': [1, 128, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 128], 'warp': [1, 32, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 64], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 128], 'warp': [1, 128, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 32], 'warp': [1, 64, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 512, 64], 'warp': [1, 256, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 128], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 512, 128], 'warp': [1, 256, 64], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 136>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 64], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 72>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 32], 'warp': [1, 32, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['nn_batch_matmul_17']
{'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 512, 32], 'warp': [1, 128, 32], 'wmma': [32, 32, 4], 'use_cutlass': True, 'rstep': [32], 'use_tc': '70', 'strides': {2: <Stride, 1, 40>}}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 109, in compile
    return self._compile_single_node(
  File "/root/Ladder/python/ladder/code_generator.py", line 261, in _compile_single_node
    code, _, _ = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 183, in tvm_build
    src = sch.build(target)
  File "/root/Ladder/python/ladder/schedule/tir_base.py", line 81, in build
    mod = tvm.build(self.sche.mod["main"], self.args, target=target)
  File "/root/Ladder/3rdparty/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/root/Ladder/3rdparty/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
ValueError: Traceback (most recent call last):
  53: TVMFuncCall
  52: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  51: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  50: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  49: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  48: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  47: tvm::codegen::CodeGenC::AddFunction(tvm::tir::PrimFunc const&)
  46: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  44: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  43: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  41: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  39: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  37: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  35: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AllocateNode const*)
  33: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  31: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::AttrStmtNode const*)
  28: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::AttrStmtNode const*)
  27: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::SeqStmtNode const*)
  25: tvm::codegen::CodeGenCUDA::VisitStmt_(tvm::tir::ForNode const*)
  24: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::ForNode const*)
  23: tvm::tir::StmtFunctor<void (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::codegen::CodeGenC::VisitStmt_(tvm::tir::BufferStoreNode const*)
  21: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  20: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  19: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  18: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  17: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  16: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  15: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  14: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  13: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  12: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  11: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::AddNode const*, std::ostream&)
  10: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  9: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  8: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  7: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::LetNode const*, std::ostream&)
  6: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  5: tvm::codegen::CodeGenC::VisitExpr_(tvm::tir::ModNode const*, std::ostream&)
  4: void tvm::codegen::PrintBinaryExpr<tvm::tir::ModNode>(tvm::tir::ModNode const*, char const*, std::ostream&, tvm::codegen::CodeGenC*)
  3: tvm::codegen::CodeGenCUDA::PrintVecBinaryOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::DataType, tvm::PrimExpr, tvm::PrimExpr, std::ostream&)
  2: tvm::codegen::CodeGenC::PrintExpr[abi:cxx11](tvm::PrimExpr const&)
  1: tvm::codegen::CodeGenC::PrintExpr(tvm::PrimExpr const&, std::ostream&)
  0: tvm::codegen::CodeGenCUDA::VisitExpr_(tvm::tir::RampNode const*, std::ostream&)
  File "/root/Ladder/3rdparty/tvm/src/target/source/codegen_cuda.cc", line 1210
ValueError: Check failed: op->lanes <= 4 (8 vs. 4) : Ramp of more than 4 lanes is not allowed.

['ladder_perfect_quant_linear_19', 'layout_transform_reshape_reshape_add_20']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_19>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 16, 16], 'thread': [1, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 128, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [8, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 16384, 2097152)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 256, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [16, 2, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 16384, 4194304)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 4, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 2048, 262144)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 2, 16, 16], 'thread': [2, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 2048, 524288)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 64, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [4, 8, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 16384, 1048576)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 64, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [4, 4, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 8192, 1048576)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 128, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [8, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 8192, 2097152)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 256], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 16, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 16384, 524288)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 8, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 8192, 524288)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 4, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 4096, 524288)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 64, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [4, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 4096, 1048576)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 2, 16, 16], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 1024, 262144)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 512], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 32, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 16384, 262144)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 256], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 16, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 8192, 262144)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 4096, 262144)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 256, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [16, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 32768, 4194304)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 512, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [32, 2, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 32768, 8388608)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 128, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [8, 8, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 32768, 2097152)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 64, 256], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [4, 16, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 32768, 1048576)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 512], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 32, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 32768, 524288)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 1024], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 64, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 32768, 262144)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 64, 16], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [4, 1, 16, 16], 'thread': [4, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 2048, 1048576)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 16], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 1, 16, 16], 'thread': [2, 1, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 1024, 524288)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 512, 16], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [32, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 16384, 8388608)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 256, 16], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [16, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 8192, 4194304)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 128, 16], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [8, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 4096, 2097152)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 16], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 1, 16, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 512, 262144)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 1024, 16], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [64, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 32768, 16777216)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 2, 8184], 'thread': [1, 2, 8], 'rstep': [], 'block_order': ((floordiv(block_idx, 16)*16) + (floormod(block_idx, 8)*2))}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 512, 1, 16], 'thread': [1, 8, 1, 2], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 32736, 32768)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 64, 8], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [4, 1, 8, 16], 'thread': [4, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 1024, 1048576)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 128, 8], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [8, 1, 8, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 2048, 2097152)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 8], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 1, 8, 16], 'thread': [2, 1, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 512, 524288)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 1024, 8], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [64, 1, 8, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 16384, 16777216)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 512, 8], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [32, 1, 8, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 8192, 8388608)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 256, 8], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 2, 1]}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [16, 1, 8, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 4096, 4194304)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 8], 'thread': [1, 16, 8], 'rstep': []}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 1, 8, 16], 'thread': [1, 1, 8, 16], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 256, 262144)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 8, 8], 'thread': [1, 8, 8], 'rstep': [], 'block_order': (((floordiv(block_idx, 2048)*2048) + (floormod(block_idx, 2)*1024)) + floordiv(floormod(block_idx, 2048), 2))}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 1, 4, 16], 'thread': [1, 1, 4, 16], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 128, 262144)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 4, 8], 'thread': [1, 4, 8], 'rstep': [], 'block_order': (((floordiv(block_idx, 4096)*4096) + (floormod(block_idx, 4)*1024)) + floordiv(floormod(block_idx, 4096), 4))}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 1, 2, 16], 'thread': [1, 1, 2, 16], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 64, 262144)

['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
{'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 2, 8], 'thread': [1, 2, 8], 'rstep': [], 'block_order': (((floordiv(block_idx, 8192)*8192) + (floormod(block_idx, 8)*1024)) + floordiv(floormod(block_idx, 8192), 8))}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 1, 1, 16], 'thread': [1, 1, 1, 16], 'rstep': []}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 199, in compile
    raise Exception(
Exception: ('Shared memory mismatched', <Node, reshape_layout_transform_ladder_layout_transform_24>, Tensor(shape=[1, 4096, 8192], op.name=p0), 32, 32768)

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 16], 'thread': [1, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 128, 32], 'thread': [1, 128, 1], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 2, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 128, 64], 'thread': [1, 128, 1], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 64, 64], 'thread': [1, 64, 2], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 4, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 64, 32], 'thread': [1, 64, 2], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 128, 16], 'thread': [1, 128, 1], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 64, 128], 'thread': [1, 64, 2], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 8, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 256, 32], 'thread': [1, 128, 1], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 2, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [4, 7, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 64, 112], 'thread': [1, 64, 2], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 7, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 7, 16, 16], 'thread': [4, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 256, 16], 'thread': [1, 128, 1], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 64, 16], 'thread': [1, 64, 2], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 1, 16, 16], 'thread': [4, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 128], 'thread': [1, 32, 4], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 8, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [2, 7, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 112], 'thread': [1, 32, 7], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 7, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 7, 16, 16], 'thread': [2, 7, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 64], 'thread': [1, 32, 4], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 4, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 32], 'thread': [1, 32, 4], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 2, 16, 16], 'thread': [2, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [8, 7, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 128, 112], 'thread': [1, 128, 1], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [8, 7, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 7, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [2, 14, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 224], 'thread': [1, 32, 4], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 14, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 14, 16, 16], 'thread': [2, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 16], 'thread': [1, 32, 2], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 1, 16, 16], 'thread': [2, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [1, 14, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 224], 'thread': [1, 16, 14], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 14, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 14, 16, 16], 'thread': [1, 14, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 256], 'thread': [1, 32, 4], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 16, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [1, 7, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 112], 'thread': [1, 16, 14], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 7, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 7, 16, 16], 'thread': [1, 7, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 4, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [4, 14, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 64, 224], 'thread': [1, 64, 2], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 14, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 14, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 32], 'thread': [1, 16, 4], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 2, 16, 16], 'thread': [1, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [32, 1, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 512, 16], 'thread': [1, 128, 1], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [32, 1, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [32, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 16], 'thread': [1, 16, 2], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 1, 16, 16], 'thread': [1, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 256], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 16, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [1, 28, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 448], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 28, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 28, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_25>: {'block': [1, 32, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 512], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 32, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 32, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 4, 16, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 2, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 8, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [8, 7, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 7, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 2, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 8, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 7, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 7, 16, 16], 'thread': [4, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 4, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [32, 2, 16, 16], 'warp': [16, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [32, 2, 16, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 16, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 14, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 14, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [16, 7, 16, 16], 'warp': [4, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 7, 16, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 1, 16, 16], 'thread': [4, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 16, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 14, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 14, 16, 16], 'thread': [2, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 8, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 7, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 7, 16, 16], 'thread': [2, 7, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 4, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [32, 1, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [32, 1, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [8, 14, 16, 16], 'warp': [4, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 14, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 2, 16, 16], 'thread': [2, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 28, 16, 16], 'warp': [1, 14, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 28, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 1, 16, 16], 'thread': [2, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 16, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [2, 32, 16, 16], 'warp': [1, 16, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 32, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 14, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 14, 16, 16], 'thread': [1, 14, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 7, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 7, 16, 16], 'thread': [1, 7, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [4, 28, 16, 16], 'warp': [2, 14, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 28, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 4, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 2, 16, 16], 'thread': [1, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 1, 16, 16], 'thread': [1, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 28, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 28, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 32, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 32, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_27>: {'block': [1, 56, 16, 16], 'warp': [1, 14, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 56, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 177, in compile
    ) = tvm_build(
  File "/root/Ladder/python/ladder/tvm_build.py", line 206, in tvm_build
    flat_block_code = get_block_flatten_code(sch.block_size)
  File "/root/Ladder/python/ladder/tvm_build.py", line 56, in get_block_flatten_code
    raise NotImplementedError()
NotImplementedError

['ladder_perfect_quant_linear_29', 'layout_transform_reshape_reshape_add_30']
{'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_quant_linear_29>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '70', 'strides': {2: <Stride, 2, 16>}}, <Node, layout_transform_reshape_reshape_add_30>: {'block': [1, 16, 16], 'thread': [1, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Traceback (most recent call last):
  File "/root/Ladder/3rdparty/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/root/Ladder/python/ladder/engine/multiproc_tunner.py", line 33, in call_build
    cpresult = cgen.compile(
  File "/root/Ladder/python/ladder/code_generator.py", line 190, in compile
    assert (
AssertionError

