2024-05-07 03:38:44 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 4], 'thread': [1, 4], 'rstep': [1024], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:45 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:45 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 2], 'thread': [1, 2], 'rstep': [2048], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:45 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:45 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 1], 'thread': [1, 1], 'rstep': [2048], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:45 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:45 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 8], 'thread': [1, 8], 'rstep': [512], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:45 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:45 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 16], 'thread': [1, 16], 'rstep': [256], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B_decode': 4}}
2024-05-07 03:38:45 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:46 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 32], 'thread': [1, 32], 'rstep': [128], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:38:46 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:46 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 64], 'thread': [1, 64], 'rstep': [128], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:38:46 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
{<Node, ladder_matmul>: {'block': [1, 4], 'thread': [1, 4], 'rstep': [1024], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.016725333407521248
{<Node, ladder_matmul>: {'block': [1, 2], 'thread': [1, 2], 'rstep': [2048], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.011264000087976456
{<Node, ladder_matmul>: {'block': [1, 1], 'thread': [1, 1], 'rstep': [2048], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.010705455206334591
{<Node, ladder_matmul>: {'block': [1, 8], 'thread': [1, 8], 'rstep': [512], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.02037760056555271
{<Node, ladder_matmul>: {'block': [1, 16], 'thread': [1, 16], 'rstep': [256], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B_decode': 4}}}
0.036026183515787125
{<Node, ladder_matmul>: {'block': [1, 32], 'thread': [1, 32], 'rstep': [128], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.0714007318019867
{<Node, ladder_matmul>: {'block': [1, 64], 'thread': [1, 64], 'rstep': [128], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.13445119559764862
__global__ void __launch_bounds__(128) Fused(float* __restrict__ A, int8_t* __restrict__ B, uint8_t* __restrict__ Scales, float* __restrict__ C) {
  
  float in_thread_C_local[1];
  float A_local[4];
  signed char B_local[4];
  float B_decode_local[4];
  __shared__ float red_buf0[128];
  in_thread_C_local[0] = 0.000000e+00f;
  for (int k_0 = 0; k_0 < 16; ++k_0) {
    *(float4*)(A_local + 0) = *(float4*)(A + ((k_0 * 512) + (((int)threadIdx.x) * 4)));
    *(int*)(B_local + 0) = *(int*)(B + (((((int)blockIdx.x) * 8192) + (k_0 * 512)) + (((int)threadIdx.x) * 4)));
    for (int ax1 = 0; ax1 < 4; ++ax1) {
        uint __1 = (((max((((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)2) & (uint)31) + ((uint)Scales[(((k_0 * 16384) + ((((int)threadIdx.x) >> 3) * 1024)) + ((int)blockIdx.x))])), (uint)63) | ((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)7) << (uint)8)) << (uint)2) | (((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)2) & (uint)31) & (uint)2)) << (uint)25;
      B_decode_local[ax1] = (*(float *)(&(__1)));
    }
    for (int k_2 = 0; k_2 < 4; ++k_2) {
      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * B_decode_local[k_2]));
    }
  }
  __syncthreads();
  ((volatile float*)red_buf0)[((int)threadIdx.x)] = in_thread_C_local[0];
  __syncthreads();
  if (((int)threadIdx.x) < 64) {
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 64)]);
  }
  __syncthreads();
  if (((int)threadIdx.x) < 32) {
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 32)]);
  }
  __syncthreads();
  if (((int)threadIdx.x) < 16) {
    float w_16_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 16)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_16_0;
    float w_8_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 8)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_8_0;
    float w_4_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 4)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_4_0;
    float w_2_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 2)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_2_0;
    float w_1_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 1)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_1_0;
  }
  __syncthreads();
  C[((int)blockIdx.x)] = ((volatile float*)red_buf0)[0];
}


top1: 0.016725333407521248 	top10: 0.010705455206334591
--------------------------------------------------------------------------------
best config: {<Node, ladder_matmul>: {'block': [1, 1], 'thread': [1, 1], 'rstep': [2048], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
best latency: 0.010705455206334591
2024-05-07 03:38:52 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 8], 'thread': [1, 8], 'rstep': [512], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:52 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:52 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 4], 'thread': [1, 4], 'rstep': [1024], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:52 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:52 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 32], 'thread': [1, 32], 'rstep': [128], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:38:52 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:52 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 16], 'thread': [1, 16], 'rstep': [256], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B_decode': 4}}
2024-05-07 03:38:52 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:52 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 2], 'thread': [1, 2], 'rstep': [2048], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:52 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:52 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 1], 'thread': [1, 1], 'rstep': [2048], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:52 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:52 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 64], 'thread': [1, 64], 'rstep': [128], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:38:52 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
{<Node, ladder_matmul>: {'block': [1, 8], 'thread': [1, 8], 'rstep': [512], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.06745599955320358
{<Node, ladder_matmul>: {'block': [1, 4], 'thread': [1, 4], 'rstep': [1024], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.07065600156784058
{<Node, ladder_matmul>: {'block': [1, 32], 'thread': [1, 32], 'rstep': [128], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.17151999473571777
{<Node, ladder_matmul>: {'block': [1, 16], 'thread': [1, 16], 'rstep': [256], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B_decode': 4}}}
0.1071104034781456
{<Node, ladder_matmul>: {'block': [1, 2], 'thread': [1, 2], 'rstep': [2048], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.061849601566791534
{<Node, ladder_matmul>: {'block': [1, 1], 'thread': [1, 1], 'rstep': [2048], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.0578560009598732
{<Node, ladder_matmul>: {'block': [1, 64], 'thread': [1, 64], 'rstep': [128], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.2523135840892792
__global__ void __launch_bounds__(128) Fused(float* __restrict__ A, int8_t* __restrict__ B, uint8_t* __restrict__ Scales, float* __restrict__ C) {
  
  float in_thread_C_local[1];
  float A_local[4];
  signed char B_local[4];
  float B_decode_local[4];
  __shared__ float red_buf0[128];
  in_thread_C_local[0] = 0.000000e+00f;
  for (int k_0 = 0; k_0 < 16; ++k_0) {
    *(float4*)(A_local + 0) = *(float4*)(A + ((k_0 * 512) + (((int)threadIdx.x) * 4)));
    *(int*)(B_local + 0) = *(int*)(B + (((((int)blockIdx.x) * 8192) + (k_0 * 512)) + (((int)threadIdx.x) * 4)));
    for (int ax1 = 0; ax1 < 4; ++ax1) {
        uint __1 = (((max((((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)2) & (uint)31) + ((uint)Scales[(((k_0 * 131072) + ((((int)threadIdx.x) >> 3) * 8192)) + ((int)blockIdx.x))])), (uint)63) | ((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)7) << (uint)8)) << (uint)2) | (((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)2) & (uint)31) & (uint)2)) << (uint)25;
      B_decode_local[ax1] = (*(float *)(&(__1)));
    }
    for (int k_2 = 0; k_2 < 4; ++k_2) {
      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * B_decode_local[k_2]));
    }
  }
  __syncthreads();
  ((volatile float*)red_buf0)[((int)threadIdx.x)] = in_thread_C_local[0];
  __syncthreads();
  if (((int)threadIdx.x) < 64) {
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 64)]);
  }
  __syncthreads();
  if (((int)threadIdx.x) < 32) {
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 32)]);
  }
  __syncthreads();
  if (((int)threadIdx.x) < 16) {
    float w_16_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 16)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_16_0;
    float w_8_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 8)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_8_0;
    float w_4_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 4)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_4_0;
    float w_2_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 2)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_2_0;
    float w_1_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 1)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_1_0;
  }
  __syncthreads();
  C[((int)blockIdx.x)] = ((volatile float*)red_buf0)[0];
}


top1: 0.06745599955320358 	top10: 0.0578560009598732
--------------------------------------------------------------------------------
best config: {<Node, ladder_matmul>: {'block': [1, 1], 'thread': [1, 1], 'rstep': [2048], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
best latency: 0.0578560009598732
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 14], 'thread': [1, 14], 'rstep': [256], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 8], 'thread': [1, 8], 'rstep': [512], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 16], 'thread': [1, 16], 'rstep': [256], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 7], 'thread': [1, 7], 'rstep': [512], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 28], 'thread': [1, 28], 'rstep': [128], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 32], 'thread': [1, 32], 'rstep': [128], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 4], 'thread': [1, 4], 'rstep': [1024], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 56], 'thread': [1, 56], 'rstep': [128], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 2], 'thread': [1, 2], 'rstep': [2048], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 64], 'thread': [1, 64], 'rstep': [128], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 1], 'thread': [1, 1], 'rstep': [2048], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:38:56 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_simt.TIRSIMTScheduler'> config: {'block': [1, 112], 'thread': [1, 112], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:38:56 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
{<Node, ladder_matmul>: {'block': [1, 14], 'thread': [1, 14], 'rstep': [256], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.2977280020713806
{<Node, ladder_matmul>: {'block': [1, 8], 'thread': [1, 8], 'rstep': [512], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.19262577593326569
{<Node, ladder_matmul>: {'block': [1, 16], 'thread': [1, 16], 'rstep': [256], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B_decode': 4}}}
0.3059484362602234
{<Node, ladder_matmul>: {'block': [1, 7], 'thread': [1, 7], 'rstep': [512], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.19740444421768188
{<Node, ladder_matmul>: {'block': [1, 28], 'thread': [1, 28], 'rstep': [128], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.281497597694397
{<Node, ladder_matmul>: {'block': [1, 32], 'thread': [1, 32], 'rstep': [128], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.27381759881973267
{<Node, ladder_matmul>: {'block': [1, 4], 'thread': [1, 4], 'rstep': [1024], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.18370559811592102
{<Node, ladder_matmul>: {'block': [1, 56], 'thread': [1, 56], 'rstep': [128], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.32010239362716675
{<Node, ladder_matmul>: {'block': [1, 2], 'thread': [1, 2], 'rstep': [2048], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.17892073094844818
{<Node, ladder_matmul>: {'block': [1, 64], 'thread': [1, 64], 'rstep': [128], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.3466239869594574
{<Node, ladder_matmul>: {'block': [1, 1], 'thread': [1, 1], 'rstep': [2048], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
0.18189963698387146
{<Node, ladder_matmul>: {'block': [1, 112], 'thread': [1, 112], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.3376128077507019
__global__ void __launch_bounds__(128) Fused(float* __restrict__ A, int8_t* __restrict__ B, uint8_t* __restrict__ Scales, float* __restrict__ C) {
  
  float in_thread_C_local[1];
  float A_local[4];
  signed char B_local[4];
  float B_decode_local[4];
  __shared__ float red_buf0[128];
  in_thread_C_local[0] = 0.000000e+00f;
  for (int k_0 = 0; k_0 < 32; ++k_0) {
    *(float4*)(A_local + 0) = *(float4*)(A + ((k_0 * 256) + (((int)threadIdx.x) * 4)));
    *(int*)(B_local + 0) = *(int*)(B + ((((((int)blockIdx.x) * 16384) + (((int)threadIdx.y) * 8192)) + (k_0 * 256)) + (((int)threadIdx.x) * 4)));
    for (int ax1 = 0; ax1 < 4; ++ax1) {
        uint __1 = (((max((((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)2) & (uint)31) + ((uint)Scales[((((k_0 * 229376) + ((((int)threadIdx.x) >> 3) * 28672)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.y))])), (uint)63) | ((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)7) << (uint)8)) << (uint)2) | (((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)2) & (uint)31) & (uint)2)) << (uint)25;
      B_decode_local[ax1] = (*(float *)(&(__1)));
    }
    for (int k_2 = 0; k_2 < 4; ++k_2) {
      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * B_decode_local[k_2]));
    }
  }
  __syncthreads();
  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = in_thread_C_local[0];
  __syncthreads();
  if (((int)threadIdx.x) < 32) {
    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);
  }
  __syncthreads();
  if (((int)threadIdx.x) < 16) {
    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);
    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;
    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);
    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;
    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);
    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;
    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);
    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;
    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);
    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;
  }
  __syncthreads();
  C[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)];
}


top1: 0.2977280020713806 	top10: 0.17892073094844818
--------------------------------------------------------------------------------
best config: {<Node, ladder_matmul>: {'block': [1, 2], 'thread': [1, 2], 'rstep': [2048], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B_decode': 4}}}
best latency: 0.17892073094844818
2024-05-07 03:39:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 8], 'thread': [1, 8], 'rstep': [448], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:39:00 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:39:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 4], 'thread': [1, 4], 'rstep': [896], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:39:00 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:39:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 32], 'thread': [1, 32], 'rstep': [112], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:39:00 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:39:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 16], 'thread': [1, 16], 'rstep': [224], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:39:00 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:39:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 2], 'thread': [1, 2], 'rstep': [1792], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B_decode': 4}}
2024-05-07 03:39:00 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:39:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 1], 'thread': [1, 1], 'rstep': [1792], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B_decode': 2}}
2024-05-07 03:39:00 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:39:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 64], 'thread': [1, 64], 'rstep': [112], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:39:00 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
2024-05-07 03:39:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_simt.TIRSIMTScheduler'> config: {'block': [1, 128], 'thread': [1, 128], 'rstep': [112], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}
2024-05-07 03:39:00 [ladder:DEBUG]: the computation is inconsistent, is_a_consistent: True, is_b_consistent: False
{<Node, ladder_matmul>: {'block': [1, 8], 'thread': [1, 8], 'rstep': [448], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.22438399493694305
{<Node, ladder_matmul>: {'block': [1, 4], 'thread': [1, 4], 'rstep': [896], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.2490595430135727
{<Node, ladder_matmul>: {'block': [1, 32], 'thread': [1, 32], 'rstep': [112], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.6048426628112793
{<Node, ladder_matmul>: {'block': [1, 16], 'thread': [1, 16], 'rstep': [224], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.37216711044311523
{<Node, ladder_matmul>: {'block': [1, 2], 'thread': [1, 2], 'rstep': [1792], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B_decode': 4}}}
0.21493759751319885
{<Node, ladder_matmul>: {'block': [1, 1], 'thread': [1, 1], 'rstep': [1792], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B_decode': 2}}}
0.24746666848659515
{<Node, ladder_matmul>: {'block': [1, 64], 'thread': [1, 64], 'rstep': [112], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.8887182474136353
{<Node, ladder_matmul>: {'block': [1, 128], 'thread': [1, 128], 'rstep': [112], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
0.1932288110256195
__global__ void __launch_bounds__(128) Fused(float* __restrict__ A, int8_t* __restrict__ B, uint8_t* __restrict__ Scales, float* __restrict__ C) {
  
  float in_thread_C_local[1];
  float A_local[4];
  signed char B_local[4];
  float B_decode_local[4];
  __shared__ float red_buf0[128];
  in_thread_C_local[0] = 0.000000e+00f;
  for (int k_0 = 0; k_0 < 56; ++k_0) {
    *(float4*)(A_local + 0) = *(float4*)(A + ((k_0 * 512) + (((int)threadIdx.x) * 4)));
    *(int*)(B_local + 0) = *(int*)(B + (((((int)blockIdx.x) * 28672) + (k_0 * 512)) + (((int)threadIdx.x) * 4)));
    for (int ax1 = 0; ax1 < 4; ++ax1) {
        uint __1 = (((max((((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)2) & (uint)31) + ((uint)Scales[(((k_0 * 131072) + ((((int)threadIdx.x) >> 3) * 8192)) + ((int)blockIdx.x))])), (uint)63) | ((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)7) << (uint)8)) << (uint)2) | (((((((uint)B_local[ax1]) >> (uint)0) & (uint)255) >> (uint)2) & (uint)31) & (uint)2)) << (uint)25;
      B_decode_local[ax1] = (*(float *)(&(__1)));
    }
    for (int k_2 = 0; k_2 < 4; ++k_2) {
      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * B_decode_local[k_2]));
    }
  }
  __syncthreads();
  ((volatile float*)red_buf0)[((int)threadIdx.x)] = in_thread_C_local[0];
  __syncthreads();
  if (((int)threadIdx.x) < 64) {
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 64)]);
  }
  __syncthreads();
  if (((int)threadIdx.x) < 32) {
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 32)]);
  }
  __syncthreads();
  if (((int)threadIdx.x) < 16) {
    float w_16_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 16)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_16_0;
    float w_8_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 8)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_8_0;
    float w_4_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 4)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_4_0;
    float w_2_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 2)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_2_0;
    float w_1_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 1)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_1_0;
  }
  __syncthreads();
  C[((int)blockIdx.x)] = ((volatile float*)red_buf0)[0];
}


top1: 0.22438399493694305 	top10: 0.1932288110256195
--------------------------------------------------------------------------------
best config: {<Node, ladder_matmul>: {'block': [1, 128], 'thread': [1, 128], 'rstep': [112], 'block_order': <NoRasterization>, 'vectorize': {'B_decode': 4}}}
best latency: 0.1932288110256195
1_1024_8192	0.010705455206334591
1_8192_8192	0.0578560009598732
1_28672_8192	0.17892073094844818
1_8192_28672	0.1932288110256195
