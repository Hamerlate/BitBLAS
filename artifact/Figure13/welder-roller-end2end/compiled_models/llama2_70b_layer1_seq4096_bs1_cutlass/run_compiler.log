Tuning ['Reshape_28']
result: 0.002730666659772396
Fusion group created:  0 ['Reshape_28']
Tuning ['Convert_27', 'Broadcast_Power_152', 'Sum_Reshape_Broadcast_Divide_151', 'Reshape_45', 'Reshape_37', 'Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Broadcast_Multiply_154']
Tuning ['Convert_27']
result: 0.13380266726016998
Fusion group created:  1 ['Convert_27']
Tuning ['Broadcast_Power_152', 'Sum_Reshape_Broadcast_Divide_151']
result: 0.5171200037002563
Tuning ['Broadcast_Power_152']
result: 0.22809599339962006
Tuning ['Sum_Reshape_Broadcast_Divide_151']
result: 0.08482133597135544
Tuning ['Broadcast_Power_152']
Found in cache
Fusion group created:  2 ['Broadcast_Power_152']
Tuning ['Sum_Reshape_Broadcast_Divide_151', 'Reshape_45', 'Reshape_37', 'Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Broadcast_Multiply_154']
result: 0.4326399862766266
Tuning ['Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Broadcast_Multiply_154']
result: 0.1496746689081192
Tuning ['Sum_Reshape_Broadcast_Divide_151']
Found in cache
Fusion group created:  3 ['Sum_Reshape_Broadcast_Divide_151']
Tuning ['Reshape_45', 'Reshape_37', 'Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Broadcast_Multiply_154']
Found in cache
Tuning ['Reshape_45', 'Reshape_37', 'Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Broadcast_Multiply_154', 'Dot_68', 'Dot_48', 'Dot_92']
result: 95.17977905273438
Tuning ['Dot_68']
result: 2.062335968017578
Tuning ['Dot_48']
result: 0.3266560137271881
Tuning ['Dot_92']
Found in cache
Fusion group created:  4 ['Reshape_45', 'Reshape_37', 'Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Broadcast_Multiply_154']
Tuning ['Dot_68', 'Reshape_Reshape_Reshape_Broadcast_Multiply_147']
result: 2.1964800357818604
Tuning ['Reshape_Reshape_Reshape_Broadcast_Multiply_147']
result: 0.14996479451656342
Tuning ['Dot_68', 'Reshape_Reshape_Reshape_Broadcast_Multiply_147', 'Slice_Negative_155', 'Slice_73']
Tuning ['Dot_68', 'Reshape_Reshape_Reshape_Broadcast_Multiply_147', 'Slice_Negative_155', 'Slice_73', 'Concat_Reshape_Broadcast_Multiply_153', 'Add_Reshape_Broadcast_156']
Fusion group created:  5 ['Dot_68', 'Reshape_Reshape_Reshape_Broadcast_Multiply_147']
Tuning ['Slice_Negative_155', 'Slice_73', 'Concat_Reshape_Broadcast_Multiply_153']
result: 0.09542399644851685
Tuning ['Slice_Negative_155']
result: 0.045456696301698685
Tuning ['Slice_73']
result: 0.04541217163205147
Tuning ['Concat_Reshape_Broadcast_Multiply_153']
result: 0.09252141416072845
Tuning ['Slice_Negative_155', 'Slice_73', 'Concat_Reshape_Broadcast_Multiply_153', 'Add_Reshape_Broadcast_156']
result: 0.13914352655410767
Tuning ['Add_Reshape_Broadcast_156']
result: 0.1367039978504181
Tuning ['Slice_Negative_155', 'Slice_73', 'Concat_Reshape_Broadcast_Multiply_153', 'Add_Reshape_Broadcast_156', 'Dot_48', 'Reshape_Reshape_Reshape_Broadcast_Multiply_157', 'Slice_Negative_159', 'Slice_53', 'Concat_Reshape_Broadcast_Multiply_158', 'Add_Reshape_Reshape_Broadcast_Reshape_Reshape_Reshape_Broadcast_150', 'BatchMatMul_Reshape_Reshape_Broadcast_Divide_149']
Fusion group created:  6 ['Slice_Negative_155', 'Slice_73', 'Concat_Reshape_Broadcast_Multiply_153', 'Add_Reshape_Broadcast_156']
Tuning ['Dot_48', 'Reshape_Reshape_Reshape_Broadcast_Multiply_157']
result: 0.3604480028152466
Tuning ['Reshape_Reshape_Reshape_Broadcast_Multiply_157']
result: 0.015493565239012241
Tuning ['Dot_48']
Found in cache
Fusion group created:  7 ['Dot_48']
Tuning ['Reshape_Reshape_Reshape_Broadcast_Multiply_157', 'Slice_Negative_159', 'Slice_53']
Tuning ['Reshape_Reshape_Reshape_Broadcast_Multiply_157', 'Slice_Negative_159', 'Slice_53', 'Concat_Reshape_Broadcast_Multiply_158', 'Add_Reshape_Reshape_Broadcast_Reshape_Reshape_Reshape_Broadcast_150']
Tuning ['Reshape_Reshape_Reshape_Broadcast_Multiply_157']
Found in cache
Fusion group created:  8 ['Reshape_Reshape_Reshape_Broadcast_Multiply_157']
Tuning ['Slice_Negative_159', 'Slice_53', 'Concat_Reshape_Broadcast_Multiply_158']
result: 0.01158399973064661
Tuning ['Slice_Negative_159']
result: 0.005973333492875099
Tuning ['Slice_53']
result: 0.006527999881654978
Tuning ['Concat_Reshape_Broadcast_Multiply_158']
result: 0.00897219032049179
Tuning ['Slice_Negative_159', 'Slice_53', 'Concat_Reshape_Broadcast_Multiply_158', 'Add_Reshape_Reshape_Broadcast_Reshape_Reshape_Reshape_Broadcast_150']
result: 0.34303998947143555
Tuning ['Add_Reshape_Reshape_Broadcast_Reshape_Reshape_Reshape_Broadcast_150']
result: 0.11508183926343918
Fusion group created:  9 ['Slice_Negative_159', 'Slice_53', 'Concat_Reshape_Broadcast_Multiply_158']
Tuning ['Add_Reshape_Reshape_Broadcast_Reshape_Reshape_Reshape_Broadcast_150', 'BatchMatMul_Reshape_Reshape_Broadcast_Divide_149']
Tuning ['Add_Reshape_Reshape_Broadcast_Reshape_Reshape_Reshape_Broadcast_150']
Found in cache
Fusion group created:  10 ['Add_Reshape_Reshape_Broadcast_Reshape_Reshape_Reshape_Broadcast_150']
Tuning ['BatchMatMul_Reshape_Reshape_Broadcast_Divide_149', 'SoftmaxBasic_139', 'SoftmaxBasic_140']
Tuning ['BatchMatMul_Reshape_Reshape_Broadcast_Divide_149']
result: 2.885632038116455
Fusion group created:  11 ['BatchMatMul_Reshape_Reshape_Broadcast_Divide_149']
Tuning ['SoftmaxBasic_139', 'SoftmaxBasic_140']
result: 4.2488322257995605
Tuning ['SoftmaxBasic_139']
result: 1.2650495767593384
Tuning ['SoftmaxBasic_140']
result: 3.3751041889190674
Tuning ['SoftmaxBasic_139', 'SoftmaxBasic_140', 'SoftmaxBasic_141', 'Reshape_Broadcast_SoftmaxBasic_161']
result: 4.0023040771484375
Tuning ['SoftmaxBasic_141']
result: 1.265663981437683
Tuning ['Reshape_Broadcast_SoftmaxBasic_161']
result: 3.3294334411621094
Tuning ['SoftmaxBasic_139', 'SoftmaxBasic_140', 'SoftmaxBasic_141', 'Reshape_Broadcast_SoftmaxBasic_161', 'Dot_92', 'Reshape_Reshape_Reshape_Reshape_Broadcast_Reshape_Reshape_Broadcast_148', 'BatchMatMul_Reshape_162']
Fusion group created:  12 ['SoftmaxBasic_139', 'SoftmaxBasic_140', 'SoftmaxBasic_141', 'Reshape_Broadcast_SoftmaxBasic_161']
Tuning ['Dot_92', 'Reshape_Reshape_Reshape_Reshape_Broadcast_Reshape_Reshape_Broadcast_148']
Tuning ['Dot_92']
Found in cache
Fusion group created:  13 ['Dot_92']
Tuning ['Reshape_Reshape_Reshape_Reshape_Broadcast_Reshape_Reshape_Broadcast_148', 'BatchMatMul_Reshape_162']
Tuning ['Reshape_Reshape_Reshape_Reshape_Broadcast_Reshape_Reshape_Broadcast_148']
result: 0.05783373862504959
Fusion group created:  14 ['Reshape_Reshape_Reshape_Reshape_Broadcast_Reshape_Reshape_Broadcast_148']
Tuning ['BatchMatMul_Reshape_162', 'Reshape_Reshape_160']
('Shared memory mismatched', <Node, Reshape_Reshape_160>, Tensor(shape=[1, 64, 4096, 128], op.name=input0), 18432, 36608)
('Shared memory mismatched', <Node, Reshape_Reshape_160>, Tensor(shape=[1, 64, 4096, 128], op.name=input0), 9216, 18304)
('Shared memory mismatched', <Node, Reshape_Reshape_160>, Tensor(shape=[1, 64, 4096, 128], op.name=input0), 36864, 73216)
('Shared memory mismatched', <Node, Reshape_Reshape_160>, Tensor(shape=[1, 64, 4096, 128], op.name=input0), 10240, 40192)
('Shared memory mismatched', <Node, Reshape_Reshape_160>, Tensor(shape=[1, 64, 4096, 128], op.name=input0), 4608, 9152)
('Shared memory mismatched', <Node, Reshape_Reshape_160>, Tensor(shape=[1, 64, 4096, 128], op.name=input0), 5120, 20096)
('Shared memory mismatched', <Node, Reshape_Reshape_160>, Tensor(shape=[1, 64, 4096, 128], op.name=input0), 2304, 4576)
('Shared memory mismatched', <Node, Reshape_Reshape_160>, Tensor(shape=[1, 64, 4096, 128], op.name=input0), 20480, 80384)
('Shared memory mismatched', <Node, Reshape_Reshape_160>, Tensor(shape=[1, 64, 4096, 128], op.name=input0), 6144, 35584)
('Shared memory mismatched', <Node, Reshape_Reshape_160>, Tensor(shape=[1, 64, 4096, 128], op.name=input0), 2560, 10048)
('Shared memory mismatched', <Node, Reshape_Reshape_160>, Tensor(shape=[1, 64, 4096, 128], op.name=input0), 3072, 17792)
result: 2.550784111022949
Tuning ['BatchMatMul_Reshape_162']
result: 1.539072036743164
Tuning ['Reshape_Reshape_160']
result: 0.09082434326410294
Tuning ['BatchMatMul_Reshape_162']
Found in cache
Fusion group created:  15 ['BatchMatMul_Reshape_162']
Tuning ['Reshape_Reshape_160', 'Dot_Add_Convert_Reshape_Broadcast_Power_163']
result: 78.75993347167969
Tuning ['Dot_Add_Convert_Reshape_Broadcast_Power_163']
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(64, 64):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 128 + i1_1_i2_1_fused // 2 * 64 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 128)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 128)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(64, 64, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 128 + i1_1_i2_1_fused // 2 * 64 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(64, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 128 + i1_1_i2_1_fused // 2 * 64 + (ax1_0 * 2 + ax1_1) % 16 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + (ax1_0 * 2 + ax1_1) // 16 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(4096, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(32, 64):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 64 + i1_1_i2_1_fused // 2 * 32 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 128)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 128)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(32, 64, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 64 + i1_1_i2_1_fused // 2 * 32 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(32, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 64 + i1_1_i2_1_fused // 2 * 32 + (ax1_0 * 2 + ax1_1) % 8 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + (ax1_0 * 2 + ax1_1) // 8 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(4096, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(64, 32):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 128 + i1_1_i2_1_fused // 2 * 64 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(64, 32, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 128 + i1_1_i2_1_fused // 2 * 64 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(32, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 128 + i1_1_i2_1_fused // 2 * 64 + (ax1_0 * 2 + ax1_1) % 16 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + (ax1_0 * 2 + ax1_1) // 16 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(64, 128):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 128 + i1_1_i2_1_fused // 2 * 64 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + i1_1_i2_1_fused % 2 * 128 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 256)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 256)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(64, 128, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 128 + i1_1_i2_1_fused // 2 * 64 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + i1_1_i2_1_fused % 2 * 128 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(128, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 128 + i1_1_i2_1_fused // 2 * 64 + (ax1_0 * 2 + ax1_1) % 16 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + i1_1_i2_1_fused % 2 * 128 + (ax1_0 * 2 + ax1_1) // 16 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(128, 64):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 256 + i1_1_i2_1_fused // 2 * 128 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 256 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 128)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 128)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(128, 64, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 256 + i1_1_i2_1_fused // 2 * 128 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(128, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 256 + i1_1_i2_1_fused // 2 * 128 + (ax1_0 * 2 + ax1_1) % 32 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + (ax1_0 * 2 + ax1_1) // 32 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(8192, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(32, 32):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 64 + i1_1_i2_1_fused // 2 * 32 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(32, 32, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 64 + i1_1_i2_1_fused // 2 * 32 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(16, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 64 + i1_1_i2_1_fused // 2 * 32 + (ax1_0 * 2 + ax1_1) % 8 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + (ax1_0 * 2 + ax1_1) // 8 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(8192, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(16, 64):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 32 + i1_1_i2_1_fused // 2 * 16 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 128)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 128)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(16, 64, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 32 + i1_1_i2_1_fused // 2 * 16 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(16, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 32 + i1_1_i2_1_fused // 2 * 16 + (ax1_0 * 2 + ax1_1) % 4 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused % 2 * 64 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(32, 128):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 64 + i1_1_i2_1_fused // 2 * 32 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + i1_1_i2_1_fused % 2 * 128 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 256)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 256)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(32, 128, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 64 + i1_1_i2_1_fused // 2 * 32 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + i1_1_i2_1_fused % 2 * 128 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(64, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 64 + i1_1_i2_1_fused // 2 * 32 + (ax1_0 * 2 + ax1_1) % 8 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + i1_1_i2_1_fused % 2 * 128 + (ax1_0 * 2 + ax1_1) // 8 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(128, 32):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 256 + i1_1_i2_1_fused // 2 * 128 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 256 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(128, 32, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 256 + i1_1_i2_1_fused // 2 * 128 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(64, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 256 + i1_1_i2_1_fused // 2 * 128 + (ax1_0 * 2 + ax1_1) % 32 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + (ax1_0 * 2 + ax1_1) // 32 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(16384, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(16, 32):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 32 + i1_1_i2_1_fused // 2 * 16 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                        v2 = T.axis.spatial(8192, i3_0 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(16, 32, 64):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 32 + i1_1_i2_1_fused // 2 * 16 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 64 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 32 + i1_1_i2_1_fused // 2 * 16 + (ax1_0 * 2 + ax1_1) % 4 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused % 2 * 32 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(8192, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(64, 16):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 128 + i1_1_i2_1_fused // 2 * 64 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        T.block_attr({"buffer_dim_align":[[0, 0, 39, 40]]})
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(64, 16, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 128 + i1_1_i2_1_fused // 2 * 64 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(16, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 128 + i1_1_i2_1_fused // 2 * 64 + (ax1_0 * 2 + ax1_1) % 16 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + (ax1_0 * 2 + ax1_1) // 16 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(16384, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(32, 16):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 64 + i1_1_i2_1_fused // 2 * 32 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        T.block_attr({"buffer_dim_align":[[0, 0, 39, 40]]})
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(32, 16, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 64 + i1_1_i2_1_fused // 2 * 32 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 64 + i1_1_i2_1_fused // 2 * 32 + (ax1_0 * 2 + ax1_1) % 8 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + (ax1_0 * 2 + ax1_1) // 8 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(16384, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(16, 32):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 16 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused * 32 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 2)
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 16 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 128)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 128)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(16, 32, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 16 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused * 32 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 64 * 16 + (ax1_0 * 2 + ax1_1) % 4 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 64 * 128 + i1_1_i2_1_fused * 32 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(4096, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(16, 128):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 32 + i1_1_i2_1_fused // 2 * 16 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + i1_1_i2_1_fused % 2 * 128 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 256)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 256)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(16, 128, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 32 + i1_1_i2_1_fused // 2 * 16 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + i1_1_i2_1_fused % 2 * 128 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(32, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 32 * 32 + i1_1_i2_1_fused // 2 * 16 + (ax1_0 * 2 + ax1_1) % 4 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 32 * 256 + i1_1_i2_1_fused % 2 * 128 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(32768, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(16, 16):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 16 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused * 16 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 16 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                        v2 = T.axis.spatial(8192, i3_0 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(16, 16, 64):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 16 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused * 16 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 64 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 128 * 16 + (ax1_0 * 2 + ax1_1) % 4 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 128 * 64 + i1_1_i2_1_fused * 16 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(4096, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(128, 16):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 256 + i1_1_i2_1_fused // 2 * 128 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 256 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        T.block_attr({"buffer_dim_align":[[0, 0, 39, 40]]})
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(128, 16, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 256 + i1_1_i2_1_fused // 2 * 128 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(32, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 256 + i1_1_i2_1_fused // 2 * 128 + (ax1_0 * 2 + ax1_1) % 32 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + (ax1_0 * 2 + ax1_1) // 32 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(32768, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(16, 16):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 32 + i1_1_i2_1_fused // 2 * 16 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                        v2 = T.axis.spatial(8192, i3_0 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        T.block_attr({"buffer_dim_align":[[0, 0, 39, 40]]})
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(16, 16, 64):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 32 + i1_1_i2_1_fused // 2 * 16 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 64 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 256 * 32 + i1_1_i2_1_fused // 2 * 16 + (ax1_0 * 2 + ax1_1) % 4 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 256 * 32 + i1_1_i2_1_fused % 2 * 16 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(16384, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(64, 8):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 512 * 128 + i1_1_i2_1_fused // 2 * 64 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 512 * 16 + i1_1_i2_1_fused % 2 * 8 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(256):
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 512 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(8192, i3_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 2)
                                        v0 = T.axis.spatial(8192, i3_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 16)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 512 * 16 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 16)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        T.block_attr({"buffer_dim_align":[[0, 0, 23, 24]]})
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(64, 8, 32):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 512 * 128 + i1_1_i2_1_fused // 2 * 64 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 512 * 16 + i1_1_i2_1_fused % 2 * 8 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 32 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 512 * 128 + i1_1_i2_1_fused // 2 * 64 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 512 * 16 + i1_1_i2_1_fused % 2 * 8 + (ax1_0 * 2 + ax1_1) // 16 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy
Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::BlockRV (tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, tvm::tir::BlockRV, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&, void>(tvm::tir::BlockRV (tvm::tir::ScheduleNode::*)(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&))::{lambda(tvm::tir::Schedule, tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&)
  0: tvm::tir::ConcreteScheduleNode::GetBlock(tvm::runtime::String const&, tvm::runtime::Optional<tvm::runtime::String> const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'get-block'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[1, "float32"], input1: T.Buffer[(1, 4096, 8192), "float16"], input2: T.Buffer[(8192, 8192), "float16"], input3: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v0: T.Buffer[(1, 4096, 8192), "float16"], output_proxy_v1: T.Buffer[(1, 4096, 8192), "float32"], output_proxy_v2: T.Buffer[(1, 4096, 8192), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate2 = T.alloc_buffer([1, 4096, 8192], dtype="float16")
        input1_shared = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="shared")
        input2_shared = T.alloc_buffer([8192, 8192], dtype="float16", scope="shared")
        mediate2_cutlass_warp_mma = T.alloc_buffer([1, 4096, 8192], dtype="float16", scope="cutlass.warp.mma")
        for i0_i1_0_i2_0_fused in T.thread_binding(32768, thread="blockIdx.x"):
            for i1_1_i2_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i1_2_init, i2_2_init in T.grid(32, 8):
                    with T.block("mediate2_init"):
                        S0 = T.axis.spatial(1, 0)
                        N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 512 * 64 + i1_1_i2_1_fused // 2 * 32 + i1_2_init)
                        M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 512 * 16 + i1_1_i2_1_fused % 2 * 8 + i2_2_init)
                        T.reads()
                        T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                        mediate2_cutlass_warp_mma[S0, N, M] = T.float16(0)
                for i3_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 512 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                        v2 = T.axis.spatial(8192, i3_0 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input2_shared"):
                                        v0 = T.axis.spatial(8192, i3_0 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 16)
                                        v1 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 512 * 16 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 16)
                                        T.reads(input2[v0, v1])
                                        T.writes(input2_shared[v0, v1])
                                        T.block_attr({"buffer_dim_align":[[0, 0, 23, 24]]})
                                        input2_shared[v0, v1] = input2[v0, v1]
                    for i1_2, i2_2, i3_1 in T.grid(32, 8, 64):
                        with T.block("mediate2_update"):
                            S0 = T.axis.spatial(1, 0)
                            N = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 512 * 64 + i1_1_i2_1_fused // 2 * 32 + i1_2)
                            M = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 512 * 16 + i1_1_i2_1_fused % 2 * 8 + i2_2)
                            K = T.axis.reduce(8192, i3_0 * 64 + i3_1)
                            T.reads(mediate2_cutlass_warp_mma[S0, N, M], input1_shared[S0, N, K], input2_shared[K, M])
                            T.writes(mediate2_cutlass_warp_mma[S0, N, M])
                            mediate2_cutlass_warp_mma[S0, N, M] = mediate2_cutlass_warp_mma[S0, N, M] + input1_shared[S0, N, K] * input2_shared[K, M]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate2_cutlass.warp.mma"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(4096, i0_i1_0_i2_0_fused // 512 * 64 + i1_1_i2_1_fused // 2 * 32 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(8192, i0_i1_0_i2_0_fused % 512 * 16 + i1_1_i2_1_fused % 2 * 8 + (ax1_0 * 2 + ax1_1) // 8 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate2_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate2[v0, v1, v2])
                                mediate2[v0, v1, v2] = mediate2_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2 in T.grid(1, 4096, 8192):
            with T.block("output_proxy_v0"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v0[N0, N1, N2])
                output_proxy_v0[N0, N1, N2] = input3[N0, N1, N2] + mediate2[N0, N1, N2]
            with T.block("output_proxy_v1"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2])
                T.writes(output_proxy_v1[N0, N1, N2])
                output_proxy_v1[N0, N1, N2] = T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32")
            with T.block("output_proxy_v2"):
                N0, N1, N2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input3[N0, N1, N2], mediate2[N0, N1, N2], input0[0])
                T.writes(output_proxy_v2[N0, N1, N2])
                output_proxy_v2[N0, N1, N2] = T.pow(T.cast(input3[N0, N1, N2] + mediate2[N0, N1, N2], "float32"), input0[0], dtype="float32")
    
Error message: Cannot find a block with the name: output_proxy/tmp/tmpzhyqalq7.cu(339): error: too many arguments in function call

1 error detected in the compilation of "/tmp/tmpzhyqalq7.cu".

Tuning ['Reshape_Reshape_160', 'Dot_Add_Convert_Reshape_Broadcast_Power_163', 'Sum_Reshape_Broadcast_Divide_145', 'Reshape_Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Reshape_Broadcast_Multiply_146', 'Dot_Sigmoid_Multiply_144', 'Dot_131', 'Multiply_135', 'Dot_Add_143']
Tuning ['Reshape_Reshape_160', 'Dot_Add_Convert_Reshape_Broadcast_Power_163', 'Sum_Reshape_Broadcast_Divide_145', 'Reshape_Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Reshape_Broadcast_Multiply_146']
Tuning ['Reshape_Reshape_160', 'Dot_Add_Convert_Reshape_Broadcast_Power_163', 'Sum_Reshape_Broadcast_Divide_145']
Fusion group created:  16 ['Reshape_Reshape_160', 'Dot_Add_Convert_Reshape_Broadcast_Power_163']
Tuning ['Sum_Reshape_Broadcast_Divide_145', 'Reshape_Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Reshape_Broadcast_Multiply_146']
result: 0.43827199935913086
Tuning ['Sum_Reshape_Broadcast_Divide_145']
Found in cache
Tuning ['Reshape_Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Reshape_Broadcast_Multiply_146']
result: 0.14204342663288116
Tuning ['Sum_Reshape_Broadcast_Divide_145']
Found in cache
Fusion group created:  17 ['Sum_Reshape_Broadcast_Divide_145']
Tuning ['Reshape_Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Reshape_Broadcast_Multiply_146', 'Dot_Sigmoid_Multiply_144', 'Dot_131']
Tuning ['Reshape_Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Reshape_Broadcast_Multiply_146']
Found in cache
Fusion group created:  18 ['Reshape_Reshape_Broadcast_Add_Sqrt_Reshape_Broadcast_Divide_Convert_Reshape_Broadcast_Multiply_146']
Tuning ['Dot_Sigmoid_Multiply_144', 'Dot_131', 'Multiply_135']
result: 23.037952423095703
Tuning ['Dot_Sigmoid_Multiply_144']
result: 7.796735763549805
Tuning ['Dot_131']
result: 7.015423774719238
Tuning ['Multiply_135']
result: 0.44856587052345276
Tuning ['Dot_Sigmoid_Multiply_144']
Found in cache
Fusion group created:  19 ['Dot_Sigmoid_Multiply_144']
Tuning ['Dot_131', 'Multiply_135']
result: 8.471551895141602
Tuning ['Dot_131']
Found in cache
Fusion group created:  20 ['Dot_131']
Tuning ['Multiply_135', 'Dot_Add_143']
Tuning ['Multiply_135']
Found in cache
Fusion group created:  21 ['Multiply_135']
Tuning ['Dot_Add_143']
result: 7.0830078125
Fusion group created:  22 ['Dot_Add_143']
Fusion gain: 9926.770498986822ms
Total run time:  318.6667070388794
