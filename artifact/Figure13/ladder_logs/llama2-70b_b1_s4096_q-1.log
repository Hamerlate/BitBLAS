Testing model: llama2-70b
/home/t-leiwang/ladder_workspace/LadderTVM/python/tvm/target/target.py:397: UserWarning: Try specifying cuda arch by adding 'arch=sm_xx' to your target.
  warnings.warn("Try specifying cuda arch by adding 'arch=sm_xx' to your target.")
2024-01-15 02:10:30 [ladder:DEBUG]: lhs or rhs is constant
2024-01-15 02:10:30 [ladder:DEBUG]: lhs or rhs is constant
32
Output115, 
reshape113, reshape112, add114, layout_transform111, 
ladder.perfect_matmul110, 
exp53, subtract52, 
Constant49, divide50, reshape48, 
reshape45, broadcast_to42, add40, multiply39, concatenate38, reshape43, expand_dims41, strided_slice37, negative36, transpose46, transpose44, strided_slice35, multiply34, 
transpose33, reshape32, reshape31, reshape30, layout_transform29, 
max51, 
reshape27, multiply25, concatenate24, strided_slice23, add26, negative22, strided_slice21, multiply20, 
nn.batch_matmul47, 
reshape18, transpose19, reshape17, reshape16, layout_transform15, 
ladder.perfect_matmul28, 
ladder.layout_transform13, reshape11, multiply10, cast9, layout_transform12, multiply8, 
divide7, sqrt6, add5, Constant4, mean3, Constant1, 
reshape99, reshape98, layout_transform97, 
ladder.perfect_matmul14, 
ladder.layout_transform109, multiply106, layout_transform108, reshape107, reshape105, reshape104, layout_transform103, multiply101, sigmoid100, 
multiply2, cast0, 
sum54, 
reshape58, cast57, cast56, divide55, 
ladder.perfect_matmul59, 
transpose69, reshape68, reshape67, broadcast_to66, expand_dims65, reshape63, reshape62, reshape61, transpose64, layout_transform60, 
nn.batch_matmul70, 
layout_transform75, ladder.layout_transform76, reshape74, reshape73, transpose72, reshape71, 
ladder.perfect_matmul77, 
add81, reshape80, reshape79, layout_transform78, 
multiply84, cast82, 
sqrt88, add87, Constant86, divide89, mean85, Constant83, 
multiply92, cast91, multiply90, 
ladder.layout_transform95, layout_transform94, reshape93, 
ladder.perfect_matmul96, 
ladder.perfect_matmul102, 
batch_matmul is not optimized for this platform.
batch_matmul is not optimized for this platform.
2024-01-15 02:10:31 [ladder:DEBUG]: candidate nodes: [<Node, cast_multiply_0>, <Node, mean_add_sqrt_divide_1>, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>, <Node, ladder_perfect_matmul_3>, <Node, layout_transform_reshape_reshape_reshape_transpose_4>, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, <Node, ladder_perfect_matmul_6>, <Node, layout_transform_reshape_reshape_reshape_transpose_7>, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>, <Node, nn_batch_matmul_9>, <Node, reshape_divide_10>, <Node, max_11>, <Node, subtract_exp_12>, <Node, sum_13>, <Node, divide_cast_cast_reshape_14>, <Node, ladder_perfect_matmul_15>, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>, <Node, nn_batch_matmul_17>, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, <Node, ladder_perfect_matmul_19>, <Node, layout_transform_reshape_reshape_add_20>, <Node, cast_multiply_21>, <Node, mean_add_sqrt_divide_22>, <Node, multiply_cast_multiply_23>, <Node, reshape_layout_transform_ladder_layout_transform_24>, <Node, ladder_perfect_matmul_25>, <Node, layout_transform_reshape_reshape_26>, <Node, ladder_perfect_matmul_27>, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>, <Node, ladder_perfect_matmul_29>, <Node, layout_transform_reshape_reshape_add_30>, <Node, Output >]
2024-01-15 02:10:31 [ladder:DEBUG]: tune candidate nodes: [<Node, PlaceHolder >, <Node, cast_multiply_0>, <Node, mean_add_sqrt_divide_1>, <Node, PlaceHolder >, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>, <Node, PlaceHolder >, <Node, ladder_perfect_matmul_3>, <Node, layout_transform_reshape_reshape_reshape_transpose_4>, <Node, PlaceHolder >, <Node, PlaceHolder >, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>, <Node, PlaceHolder >, <Node, ladder_perfect_matmul_6>, <Node, layout_transform_reshape_reshape_reshape_transpose_7>, <Node, PlaceHolder >, <Node, PlaceHolder >, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>, <Node, nn_batch_matmul_9>, <Node, reshape_divide_10>, <Node, max_11>, <Node, subtract_exp_12>, <Node, sum_13>, <Node, divide_cast_cast_reshape_14>, <Node, PlaceHolder >, <Node, ladder_perfect_matmul_15>, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>, <Node, nn_batch_matmul_17>, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, <Node, PlaceHolder >, <Node, ladder_perfect_matmul_19>, <Node, PlaceHolder >, <Node, layout_transform_reshape_reshape_add_20>, <Node, cast_multiply_21>, <Node, mean_add_sqrt_divide_22>, <Node, PlaceHolder >, <Node, multiply_cast_multiply_23>, <Node, reshape_layout_transform_ladder_layout_transform_24>, <Node, PlaceHolder >, <Node, ladder_perfect_matmul_25>, <Node, layout_transform_reshape_reshape_26>, <Node, PlaceHolder >, <Node, ladder_perfect_matmul_27>, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>, <Node, PlaceHolder >, <Node, ladder_perfect_matmul_29>, <Node, layout_transform_reshape_reshape_add_30>, <Node, Output >]
Processing:   0%|          | 0/47 [00:00<?, ?it/s]                                                  2024-01-15 02:10:31 [ladder:INFO]: Tuning ['cast_multiply_0', 'mean_add_sqrt_divide_1', 'multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2']
Processing:   0%|          | 0/47 [00:00<?, ?it/s]                                                  2024-01-15 02:10:31 [ladder:INFO]: Tuning ['cast_multiply_0', 'mean_add_sqrt_divide_1']
Processing:   0%|          | 0/47 [00:00<?, ?it/s]                                                  2024-01-15 02:10:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 1, 8192], 'thread': [1, 1, 128], 'rstep': []}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 1, 1], 'thread': [1, 1, 1], 'rstep': [8192], 'reduce_thread': [128], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:05<?, ?it/s]                                                  2024-01-15 02:10:39 [ladder:DEBUG]: 0.13783040642738342
Processing:   0%|          | 0/47 [00:07<?, ?it/s]                                                  2024-01-15 02:10:39 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 1, 8192], 'thread': [1, 1, 128], 'rstep': []}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 1, 1], 'thread': [1, 1, 1], 'rstep': [8192], 'reduce_thread': [128], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:07<?, ?it/s]                                                  2024-01-15 02:10:39 [ladder:INFO]: result: 0.13783040642738342
Processing:   0%|          | 0/47 [00:07<?, ?it/s]                                                  2024-01-15 02:10:39 [ladder:INFO]: Tuning ['cast_multiply_0']
Processing:   0%|          | 0/47 [00:07<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 1, 128], 'thread': [1, 1, 128], 'rstep': []}}
Processing:   0%|          | 0/47 [00:24<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: 0.22809599339962006
Processing:   0%|          | 0/47 [00:24<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 2, 64], 'thread': [1, 2, 64], 'rstep': []}}
Processing:   0%|          | 0/47 [00:24<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: 0.23014399409294128
Processing:   0%|          | 0/47 [00:24<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 1, 64], 'thread': [1, 1, 64], 'rstep': []}}
Processing:   0%|          | 0/47 [00:24<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: 0.3924480080604553
Processing:   0%|          | 0/47 [00:24<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 1, 4096], 'thread': [1, 1, 128], 'rstep': []}}
Processing:   0%|          | 0/47 [00:24<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: 0.2490367889404297
Processing:   0%|          | 0/47 [00:24<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 2, 2048], 'thread': [1, 2, 64], 'rstep': []}}
Processing:   0%|          | 0/47 [00:24<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: 0.24806399643421173
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 4, 1024], 'thread': [1, 4, 32], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: 0.24186880886554718
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 8, 512], 'thread': [1, 8, 16], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: 0.2443263977766037
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 16, 256], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: 0.24965119361877441
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 32, 128], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: 0.2572287917137146
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 64, 64], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.26009601354599
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 1, 2048], 'thread': [1, 1, 128], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.2457599937915802
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 2, 1024], 'thread': [1, 2, 64], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.24191999435424805
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 4, 512], 'thread': [1, 4, 32], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.24038399755954742
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 8, 256], 'thread': [1, 8, 16], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.2385919988155365
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 16, 128], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.25727999210357666
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 32, 64], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.25804799795150757
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 1, 1024], 'thread': [1, 1, 128], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.2385919988155365
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 2, 512], 'thread': [1, 2, 64], 'rstep': []}}
Processing:   0%|          | 0/47 [00:25<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.24043519794940948
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 4, 256], 'thread': [1, 4, 32], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.23347200453281403
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 8, 128], 'thread': [1, 8, 16], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.24191999435424805
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 16, 64], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: 0.24934400618076324
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 1, 512], 'thread': [1, 1, 128], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: 0.23731200397014618
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 2, 256], 'thread': [1, 2, 64], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: 0.23603199422359467
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 4, 128], 'thread': [1, 4, 32], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: 0.23475199937820435
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 8, 64], 'thread': [1, 8, 16], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: 0.2393600046634674
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 1, 256], 'thread': [1, 1, 128], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: 0.23270399868488312
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 2, 128], 'thread': [1, 2, 64], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: 0.23731200397014618
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 4, 64], 'thread': [1, 4, 32], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: 0.23552000522613525
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 1, 8192], 'thread': [1, 1, 128], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: 0.2566143870353699
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 2, 4096], 'thread': [1, 2, 64], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: 0.25026559829711914
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 4, 2048], 'thread': [1, 4, 32], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: 0.24760320782661438
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 8, 1024], 'thread': [1, 8, 16], 'rstep': []}}
Processing:   0%|          | 0/47 [00:26<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: 0.24883201718330383
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 16, 512], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: 0.24944639205932617
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 32, 256], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: 0.24944639205932617
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 64, 128], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: 0.2562047839164734
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 128, 64], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: 0.2547712028026581
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 4, 32], 'thread': [1, 4, 32], 'rstep': []}}
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: 0.23577600717544556
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 2, 32], 'thread': [1, 2, 32], 'rstep': []}}
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: 0.392192006111145
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 128, 32], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: 0.2728959918022156
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 64, 32], 'thread': [1, 16, 8], 'rstep': []}}
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: 0.27033600211143494
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, cast_multiply_0>: {'block': [1, 1, 128], 'thread': [1, 1, 128], 'rstep': []}}
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:INFO]: result: 0.22809599339962006
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:10:59 [ladder:INFO]: Tuning ['mean_add_sqrt_divide_1']
Processing:   0%|          | 0/47 [00:27<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 2, 1], 'thread': [1, 2, 1], 'rstep': [2048], 'reduce_thread': [64], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: 0.08806400001049042
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 1, 1], 'thread': [1, 1, 1], 'rstep': [4096], 'reduce_thread': [128], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: 0.08478720486164093
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 4, 1], 'thread': [1, 4, 1], 'rstep': [1024], 'reduce_thread': [32], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: 0.08867840468883514
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 16, 1], 'thread': [1, 16, 1], 'rstep': [256], 'reduce_thread': [8], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: 0.09338880330324173
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 8, 1], 'thread': [1, 8, 1], 'rstep': [512], 'reduce_thread': [16], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: 0.08683519810438156
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 32, 1], 'thread': [1, 32, 1], 'rstep': [128], 'reduce_thread': [4], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: 0.14192639291286469
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 64, 1], 'thread': [1, 64, 1], 'rstep': [64], 'reduce_thread': [2], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: 0.2949120104312897
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 128, 1], 'thread': [1, 128, 1], 'rstep': [32], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:32<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: 0.3993600010871887
Processing:   0%|          | 0/47 [00:33<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 256, 1], 'thread': [1, 128, 1], 'rstep': [32], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:33<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: 0.6324223875999451
Processing:   0%|          | 0/47 [00:33<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, mean_add_sqrt_divide_1>: {'block': [1, 1, 1], 'thread': [1, 1, 1], 'rstep': [4096], 'reduce_thread': [128], 'vectorize': {'p0': 4}}}
Processing:   0%|          | 0/47 [00:33<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:INFO]: result: 0.08478720486164093
Processing:   0%|          | 0/47 [00:33<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:INFO]: Tuning ['cast_multiply_0', 'mean_add_sqrt_divide_1', 'multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2']
Processing:   0%|          | 0/47 [00:33<?, ?it/s]                                                  2024-01-15 02:11:04 [ladder:INFO]: Fusion group created: 0 ['cast_multiply_0', 'mean_add_sqrt_divide_1']
Processing:   0%|          | 0/47 [00:33<?, ?it/s]Processing:   4%|▍         | 2/47 [00:33<12:24, 16.55s/it]                                                          2024-01-15 02:11:04 [ladder:INFO]: Tuning ['multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2', 'ladder_perfect_matmul_3', 'ladder_perfect_matmul_6', 'ladder_perfect_matmul_15']
Processing:   4%|▍         | 2/47 [00:33<12:24, 16.55s/it]                                                          2024-01-15 02:11:04 [ladder:INFO]: Tuning ['multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2']
Processing:   4%|▍         | 2/47 [00:33<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [4, 4, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:51<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: 0.15912958979606628
Processing:   4%|▍         | 2/47 [00:51<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [8, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:51<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: 0.16035839915275574
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [2, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: 0.1658879965543747
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [2, 4, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: 0.13388800621032715
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [4, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: 0.13209599256515503
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 16, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: 0.1740799993276596
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.13286399841308594
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 4, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.13312000036239624
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [2, 2, 16, 16], 'thread': [2, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.13132800161838531
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 2, 16, 16], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.13875199854373932
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [16, 4, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.2873343825340271
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [32, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.3131392002105713
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [8, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.42618879675865173
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [8, 4, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.40857601165771484
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [16, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.4098048210144043
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [4, 16, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.44974079728126526
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [4, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.2830336093902588
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [2, 32, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.4284416139125824
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [2, 16, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.42967039346694946
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 64, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:52<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.38461440801620483
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 32, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.2840575873851776
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [8, 4, 8, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.13516800105571747
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [4, 4, 8, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.13388800621032715
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [2, 4, 8, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: 0.13363200426101685
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 4, 8, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.13977600634098053
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 4, 4, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.13926400244235992
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [32, 4, 8, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.2453504055738449
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [16, 4, 8, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.14602240920066833
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 4, 2, 16], 'thread': [1, 4, 2, 16], 'rstep': []}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.19814400374889374
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [4, 8, 8, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.13557758927345276
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [2, 8, 8, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.13439999520778656
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 8, 8, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.13363200426101685
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 8, 4, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.13491199910640717
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [2, 16, 8, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.1372160017490387
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 16, 8, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.13439999520778656
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [16, 8, 8, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.16199681162834167
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [8, 8, 8, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.15073280036449432
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 8, 2, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.1382399946451187
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 16, 4, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.13439999520778656
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [1, 4, 1, 16], 'thread': [1, 4, 1, 16], 'rstep': []}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: 0.3919360041618347
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2>: {'block': [2, 2, 16, 16], 'thread': [2, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:INFO]: result: 0.13132800161838531
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]                                                          2024-01-15 02:11:25 [ladder:INFO]: Fusion group created: 1 ['multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2']
Processing:   4%|▍         | 2/47 [00:53<12:24, 16.55s/it]Processing:  11%|█         | 5/47 [00:53<06:59,  9.99s/it]                                                          2024-01-15 02:11:25 [ladder:INFO]: Tuning ['ladder_perfect_matmul_3', 'layout_transform_reshape_reshape_reshape_transpose_4']
Processing:  11%|█         | 5/47 [00:53<06:59,  9.99s/it]                                                          2024-01-15 02:11:30 [ladder:INFO]: Tuning ['ladder_perfect_matmul_3', 'layout_transform_reshape_reshape_reshape_transpose_4']
Processing:  11%|█         | 5/47 [00:58<06:59,  9.99s/it]                                                          2024-01-15 02:11:50 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:18<06:59,  9.99s/it]                                                          2024-01-15 02:11:50 [ladder:DEBUG]: 2.251161575317383
Processing:  11%|█         | 5/47 [01:18<06:59,  9.99s/it]                                                          2024-01-15 02:11:50 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:18<06:59,  9.99s/it]                                                          2024-01-15 02:11:50 [ladder:DEBUG]: 2.5849854946136475
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:50 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:50 [ladder:DEBUG]: 2.005401611328125
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:50 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [16, 8, 16, 16], 'warp': [8, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: 2.187878370285034
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: 2.568601608276367
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: 4.210892677307129
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: 3.4174976348876953
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: 2.3058431148529053
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: 5.093171119689941
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: 4.165222644805908
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: 2.297241687774658
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: 3.8739967346191406
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:19<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: 7.3461761474609375
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: 5.020467281341553
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: 8.336793899536133
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: 6.78481912612915
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: 9.961267471313477
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: 3.8289408683776855
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: 7.26999044418335
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: 7.099596977233887
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: 8.379801750183105
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: 9.734963417053223
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:20<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: 13.040435791015625
Processing:  11%|█         | 5/47 [01:21<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:21<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: 6.998630523681641
Processing:  11%|█         | 5/47 [01:21<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3__layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:21<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:INFO]: result: 2.005401611328125
Processing:  11%|█         | 5/47 [01:21<06:59,  9.99s/it]                                                          2024-01-15 02:11:52 [ladder:INFO]: Tuning ['ladder_perfect_matmul_3']
Processing:  11%|█         | 5/47 [01:21<06:59,  9.99s/it]                                                          2024-01-15 02:12:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:34<06:59,  9.99s/it]                                                          2024-01-15 02:12:06 [ladder:DEBUG]: 1.9945472478866577
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:06 [ladder:DEBUG]: 2.5800702571868896
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:06 [ladder:DEBUG]: 2.5774080753326416
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: 2.2018046379089355
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [16, 8, 16, 16], 'warp': [8, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: 2.157158374786377
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: 3.349299192428589
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: 4.145766258239746
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: 4.107059001922607
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: 2.2966272830963135
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: 2.2431743144989014
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: 5.070643424987793
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:35<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: 5.016780853271484
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: 6.808985710144043
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: 7.426662445068359
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: 7.305010795593262
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: 3.8416385650634766
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: 3.800678253173828
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: 8.337613105773926
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: 8.246477127075195
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: 9.820159912109375
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: 9.921740531921387
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: 13.04719352722168
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:36<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: 7.05597448348999
Processing:  11%|█         | 5/47 [01:37<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:37<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: 7.028531074523926
Processing:  11%|█         | 5/47 [01:37<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_3>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  11%|█         | 5/47 [01:37<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:INFO]: result: 1.9945472478866577
Processing:  11%|█         | 5/47 [01:37<06:59,  9.99s/it]                                                          2024-01-15 02:12:08 [ladder:INFO]: Tuning ['layout_transform_reshape_reshape_reshape_transpose_4']
Processing:  11%|█         | 5/47 [01:37<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 32, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.10623999685049057
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 16, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.10572800040245056
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 4, 8, 128], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.11494400352239609
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 8, 4, 128], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.13875199854373932
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 64, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.10342399775981903
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 128, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.11084800213575363
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 256, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:51<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.1295360028743744
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 16, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.11084800213575363
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 8, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.11494400352239609
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 4, 4, 128], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.11878400295972824
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 32, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.10623999685049057
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 64, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: 0.10623999685049057
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 128, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.11392000317573547
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 8, 128], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.08908800035715103
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 4, 128], 'thread': [1, 2, 4, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.09087999910116196
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 16, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.10521599650382996
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 32, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.10803200304508209
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 64, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.11084800213575363
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 4, 32], 'thread': [1, 1, 4, 32], 'rstep': []}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.19814400374889374
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 8, 16], 'thread': [1, 1, 8, 16], 'rstep': []}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.19788800179958344
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 4, 128], 'thread': [1, 1, 4, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.09011200070381165
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 8, 64], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.08908800035715103
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 16, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.09548799693584442
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 32, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.10649599879980087
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 4, 64], 'thread': [1, 1, 4, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.10188800096511841
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 8, 32], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.1013759970664978
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 16, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.10291200131177902
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 4, 16], 'thread': [1, 1, 4, 16], 'rstep': []}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.392192006111145
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 128, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.1744896024465561
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 64, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.17203199863433838
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 256, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.16957440972328186
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 512, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.20541438460350037
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 1024, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:52<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.25169920921325684
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 4, 32, 128], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.1740799993276596
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 8, 16, 128], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.18903039395809174
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 16, 8, 128], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: 0.19640320539474487
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 32, 4, 128], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:25 [ladder:DEBUG]: 0.2523135840892792
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 64, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:25 [ladder:DEBUG]: 0.13066241145133972
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 2, 32, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:25 [ladder:DEBUG]: 0.12349440157413483
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 4, 16, 128], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:25 [ladder:DEBUG]: 0.12390400469303131
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:25 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_4>: {'block': [1, 1, 8, 128], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:25 [ladder:INFO]: result: 0.08908800035715103
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:25 [ladder:INFO]: Tuning ['ladder_perfect_matmul_3', 'layout_transform_reshape_reshape_reshape_transpose_4', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]                                                          2024-01-15 02:12:25 [ladder:INFO]: Fusion group created: 2 ['ladder_perfect_matmul_3', 'layout_transform_reshape_reshape_reshape_transpose_4']
Processing:  11%|█         | 5/47 [01:53<06:59,  9.99s/it]Processing:  15%|█▍        | 7/47 [01:53<11:54, 17.87s/it]                                                          2024-01-15 02:12:25 [ladder:INFO]: Tuning ['multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5', 'ladder_perfect_matmul_6', 'layout_transform_reshape_reshape_reshape_transpose_7', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8', 'nn_batch_matmul_9']
Processing:  15%|█▍        | 7/47 [01:53<11:54, 17.87s/it]                                                          2024-01-15 02:12:25 [ladder:INFO]: Tuning ['multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
Processing:  15%|█▍        | 7/47 [01:53<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 1, 128], 'thread': [1, 1, 128], 'rstep': []}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.19840000569820404
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 32, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.19374080002307892
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 16, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.19804160296916962
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 8, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.19476480782032013
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 4, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.21237759292125702
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 2, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.21954560279846191
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [32, 1, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.16773119568824768
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.14975999295711517
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 8, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.1472512036561966
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 4, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.14847999811172485
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 2, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.15564800798892975
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 1, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.156876802444458
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 8, 128], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.10547199845314026
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 4, 128], 'thread': [2, 4, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.10265599936246872
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 2, 128], 'thread': [4, 2, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.09830400347709656
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 1, 128], 'thread': [8, 1, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:10<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.09804800152778625
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 4, 128], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.09625600278377533
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 2, 128], 'thread': [2, 2, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.09471999853849411
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 1, 128], 'thread': [4, 1, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.09574399888515472
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 2, 128], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: 0.10751999914646149
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 1, 128], 'thread': [2, 1, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: 0.11187200248241425
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 128, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: 0.32645121216773987
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 64, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: 0.32296961545944214
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 32, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: 0.31068161129951477
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 16, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: 0.3653631806373596
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 8, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: 0.3667967915534973
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [32, 4, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:11<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: 0.2639872133731842
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [64, 2, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: 0.3303423821926117
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 64, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: 0.23818239569664001
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 32, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: 0.2213887870311737
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [4, 16, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: 0.21647360920906067
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [8, 8, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: 0.23347198963165283
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [16, 4, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: 0.23470079898834229
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [32, 2, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: 0.2951168119907379
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [64, 1, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: 0.2295808047056198
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 2, 64], 'thread': [1, 2, 64], 'rstep': []}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: 0.19891199469566345
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 1, 64], 'thread': [2, 1, 64], 'rstep': []}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: 0.19840000569820404
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 1, 64], 'thread': [1, 1, 64], 'rstep': []}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: 0.3924480080604553
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [1, 64, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: 0.21872639656066895
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 32, 64], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: 0.18943999707698822
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5>: {'block': [2, 2, 128], 'thread': [2, 2, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:INFO]: result: 0.09471999853849411
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]                                                          2024-01-15 02:12:44 [ladder:INFO]: Fusion group created: 3 ['multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5']
Processing:  15%|█▍        | 7/47 [02:12<11:54, 17.87s/it]Processing:  23%|██▎       | 11/47 [02:12<06:33, 10.92s/it]                                                           2024-01-15 02:12:44 [ladder:INFO]: Tuning ['ladder_perfect_matmul_6', 'layout_transform_reshape_reshape_reshape_transpose_7']
Processing:  23%|██▎       | 11/47 [02:12<06:33, 10.92s/it]                                                           2024-01-15 02:12:46 [ladder:INFO]: Tuning ['ladder_perfect_matmul_6', 'layout_transform_reshape_reshape_reshape_transpose_7']
Processing:  23%|██▎       | 11/47 [02:14<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:32<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: 0.3375104069709778
Processing:  23%|██▎       | 11/47 [02:32<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:32<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: 0.45588478446006775
Processing:  23%|██▎       | 11/47 [02:32<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:32<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: 0.34263041615486145
Processing:  23%|██▎       | 11/47 [02:32<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:32<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: 0.518553614616394
Processing:  23%|██▎       | 11/47 [02:32<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [16, 8, 16, 16], 'warp': [8, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:32<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: 0.46551042795181274
Processing:  23%|██▎       | 11/47 [02:32<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:32<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: 0.3409920036792755
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: 0.6211584210395813
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: 0.659660816192627
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 0.6592512130737305
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 0.6238207817077637
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 0.3596287965774536
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 0.8728575706481934
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 0.3475455939769745
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 1.006592035293579
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 1.0160127878189087
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 1.0489856004714966
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 0.6254591941833496
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 1.110425591468811
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 1.2562432289123535
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 0.6236159801483154
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 1.2824575901031494
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:33<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 1.6691200733184814
Processing:  23%|██▎       | 11/47 [02:34<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:34<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 1.0473471879959106
Processing:  23%|██▎       | 11/47 [02:34<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:34<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: 1.0475519895553589
Processing:  23%|██▎       | 11/47 [02:34<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6__layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:34<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:INFO]: result: 0.3375104069709778
Processing:  23%|██▎       | 11/47 [02:34<06:33, 10.92s/it]                                                           2024-01-15 02:13:05 [ladder:INFO]: Tuning ['ladder_perfect_matmul_6']
Processing:  23%|██▎       | 11/47 [02:34<06:33, 10.92s/it]                                                           2024-01-15 02:13:20 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:48<06:33, 10.92s/it]                                                           2024-01-15 02:13:20 [ladder:DEBUG]: 0.3321855962276459
Processing:  23%|██▎       | 11/47 [02:48<06:33, 10.92s/it]                                                           2024-01-15 02:13:20 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:48<06:33, 10.92s/it]                                                           2024-01-15 02:13:20 [ladder:DEBUG]: 0.5283840298652649
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:20 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:20 [ladder:DEBUG]: 0.34119680523872375
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:20 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:20 [ladder:DEBUG]: 0.34078720211982727
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:20 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 0.4218880236148834
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [16, 8, 16, 16], 'warp': [8, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 0.4591616094112396
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 0.6582272052764893
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 0.6565887928009033
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 0.6164480447769165
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 0.6209536194801331
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 0.8689664006233215
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 0.35123199224472046
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 0.35061758756637573
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 1.0096639394760132
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 1.0141695737838745
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 1.0461183786392212
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:49<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 1.050214409828186
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 0.6244351863861084
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: 0.6252543926239014
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:DEBUG]: 1.2736512422561646
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:DEBUG]: 1.2619775533676147
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:DEBUG]: 1.6723968982696533
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:DEBUG]: 1.046732783317566
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:DEBUG]: 1.0487807989120483
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_6>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:INFO]: result: 0.3321855962276459
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:22 [ladder:INFO]: Tuning ['layout_transform_reshape_reshape_reshape_transpose_7']
Processing:  23%|██▎       | 11/47 [02:50<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 4, 128], 'thread': [1, 1, 4, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.011776000261306763
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 8, 64], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.011008000001311302
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 16, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.014336000196635723
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 32, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.016383999958634377
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 4, 64], 'thread': [1, 1, 4, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.015615999698638916
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 8, 32], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.015359999611973763
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 16, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.015615999698638916
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 4, 32], 'thread': [1, 1, 4, 32], 'rstep': []}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.02739199995994568
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 8, 16], 'thread': [1, 1, 8, 16], 'rstep': []}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.02739199995994568
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 4, 16], 'thread': [1, 1, 4, 16], 'rstep': []}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.051711998879909515
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 16, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.01740800030529499
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 2, 8, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.017664000391960144
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 32, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.01664000004529953
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 64, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.01664000004529953
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 128, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:04<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.017152000218629837
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 4, 4, 128], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.018432000651955605
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 8, 128], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.013055999763309956
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 2, 4, 128], 'thread': [1, 2, 4, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.014336000196635723
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 16, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: 0.016127999871969223
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 32, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.016127999871969223
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 64, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.017152000218629837
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 2, 4, 64], 'thread': [1, 2, 4, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.011008000001311302
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 32, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.01740800030529499
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 2, 16, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.01664000004529953
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 4, 8, 128], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.01740800030529499
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 64, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.016383999958634377
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 128, 32], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.016896000131964684
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 256, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.01817600056529045
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 8, 4, 128], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.02022399939596653
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 2, 16, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.01664000004529953
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 2, 8, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.016896000131964684
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 4, 8, 64], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.017152000218629837
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 4, 4, 64], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.016896000131964684
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 2, 2, 128], 'thread': [1, 2, 2, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.012032000347971916
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 2, 128], 'thread': [1, 1, 2, 64], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.015615999698638916
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 2, 64], 'thread': [1, 1, 2, 64], 'rstep': []}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.02739199995994568
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 2, 32], 'thread': [1, 1, 2, 32], 'rstep': []}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.05196800082921982
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 2, 16], 'thread': [1, 1, 2, 16], 'rstep': []}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.1008640006184578
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 8, 4, 64], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.019711999222636223
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 2, 32, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: 0.01664000004529953
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_7>: {'block': [1, 1, 8, 64], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:INFO]: result: 0.011008000001311302
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:INFO]: Tuning ['ladder_perfect_matmul_6', 'layout_transform_reshape_reshape_reshape_transpose_7', 'multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8']
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]                                                           2024-01-15 02:13:37 [ladder:INFO]: Fusion group created: 4 ['ladder_perfect_matmul_6', 'layout_transform_reshape_reshape_reshape_transpose_7']
Processing:  23%|██▎       | 11/47 [03:05<06:33, 10.92s/it]Processing:  28%|██▊       | 13/47 [03:05<08:37, 15.21s/it]                                                           2024-01-15 02:13:37 [ladder:INFO]: Tuning ['multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8', 'nn_batch_matmul_9']
Processing:  28%|██▊       | 13/47 [03:05<08:37, 15.21s/it]                                                           2024-01-15 02:13:37 [ladder:INFO]: Tuning ['multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8']
Processing:  28%|██▊       | 13/47 [03:06<08:37, 15.21s/it]                                                           2024-01-15 02:13:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 4, 128], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:55 [ladder:DEBUG]: 0.07628799974918365
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [16, 2, 128], 'thread': [2, 2, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:55 [ladder:DEBUG]: 0.08320000022649765
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [32, 1, 128], 'thread': [4, 1, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:55 [ladder:DEBUG]: 0.0796160027384758
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 2, 128], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.07536640018224716
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [16, 1, 128], 'thread': [2, 1, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.07741440087556839
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 1, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [2, 1, 1]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.05734400078654289
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.23408639430999756
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [16, 8, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.18575359880924225
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [32, 4, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.17510399222373962
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [64, 2, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.19189760088920593
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 8, 128], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.11960320174694061
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [16, 4, 128], 'thread': [2, 4, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.12759040296077728
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [32, 2, 128], 'thread': [4, 2, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.13086719810962677
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [64, 1, 128], 'thread': [8, 1, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.14069759845733643
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 8, 64], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.11489280313253403
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [16, 4, 64], 'thread': [2, 4, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.12062720209360123
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [32, 2, 64], 'thread': [4, 2, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.10240000486373901
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [64, 1, 64], 'thread': [8, 1, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:24<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.1343487948179245
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 4, 64], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.07372800260782242
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [16, 2, 64], 'thread': [2, 2, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.07720959931612015
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [32, 1, 64], 'thread': [4, 1, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: 0.0770048052072525
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 2, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [2, 1, 1]}}
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: 0.059647999703884125
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [16, 1, 64], 'thread': [2, 1, 64], 'rstep': [], 'step': [2, 1, 1]}}
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: 0.06067200005054474
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 1, 64], 'thread': [2, 1, 64], 'rstep': [], 'step': [2, 1, 1]}}
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: 0.05657599866390228
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 32, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: 0.2117631882429123
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [16, 16, 64], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: 0.20869119465351105
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [32, 8, 64], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: 0.18923519551753998
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [64, 4, 64], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: 0.23552000522613525
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 16, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:25<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: 0.2553855776786804
Processing:  28%|██▊       | 13/47 [03:26<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [16, 8, 64], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:26<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: 0.1974271982908249
Processing:  28%|██▊       | 13/47 [03:26<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [32, 4, 64], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:26<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: 0.17776639759540558
Processing:  28%|██▊       | 13/47 [03:26<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [64, 2, 64], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:26<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: 0.1984511911869049
Processing:  28%|██▊       | 13/47 [03:26<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [4, 8, 128], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:26<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: 0.09359359741210938
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [4, 4, 128], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: 0.06732799857854843
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [4, 2, 128], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: 0.06451199948787689
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [4, 1, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [2, 1, 1]}}
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: 0.062463998794555664
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [4, 32, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:59 [ladder:DEBUG]: 0.20234239101409912
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [4, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:59 [ladder:DEBUG]: 0.158720001578331
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [4, 16, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:59 [ladder:DEBUG]: 0.15237119793891907
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 16, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:59 [ladder:DEBUG]: 0.16076800227165222
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:59 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8>: {'block': [8, 1, 64], 'thread': [2, 1, 64], 'rstep': [], 'step': [2, 1, 1]}}
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:59 [ladder:INFO]: result: 0.05657599866390228
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]                                                           2024-01-15 02:13:59 [ladder:INFO]: Fusion group created: 5 ['multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8']
Processing:  28%|██▊       | 13/47 [03:27<08:37, 15.21s/it]Processing:  36%|███▌      | 17/47 [03:27<05:26, 10.88s/it]                                                           2024-01-15 02:13:59 [ladder:INFO]: Tuning ['nn_batch_matmul_9', 'reshape_divide_10']
Processing:  36%|███▌      | 17/47 [03:27<05:26, 10.88s/it]                                                           2024-01-15 02:14:01 [ladder:INFO]: Tuning ['nn_batch_matmul_9', 'reshape_divide_10']
Processing:  36%|███▌      | 17/47 [03:30<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 256], 'warp': [1, 64, 128], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 264>}}}
Processing:  36%|███▌      | 17/47 [03:42<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: 4.694835186004639
Processing:  36%|███▌      | 17/47 [03:42<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 128], 'warp': [1, 32, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [03:42<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: 3.5940353870391846
Processing:  36%|███▌      | 17/47 [03:42<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 128], 'warp': [1, 16, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [03:42<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: 3.7095425128936768
Processing:  36%|███▌      | 17/47 [03:42<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 256], 'warp': [1, 32, 128], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 264>}}}
Processing:  36%|███▌      | 17/47 [03:42<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: 3.9813122749328613
Processing:  36%|███▌      | 17/47 [03:42<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 256], 'warp': [1, 16, 128], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 264>}}}
Processing:  36%|███▌      | 17/47 [03:42<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: 3.725926637649536
Processing:  36%|███▌      | 17/47 [03:42<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 16, 128], 'warp': [1, 16, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [03:42<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: 4.499046325683594
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 128], 'warp': [1, 64, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: 4.174233436584473
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 16, 256], 'warp': [1, 16, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 264>}}}
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: 4.773683071136475
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 256, 128], 'warp': [1, 128, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: 5.266636848449707
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 64], 'warp': [1, 64, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: 3.1938560009002686
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 64], 'warp': [1, 32, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: 3.1852545738220215
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 64], 'warp': [1, 16, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: 3.6603903770446777
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 16, 64], 'warp': [1, 16, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: 4.907212734222412
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 32], 'warp': [1, 64, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: 4.206387519836426
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 256, 64], 'warp': [1, 128, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: 4.179762840270996
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 32], 'warp': [1, 32, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [03:43<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: 4.302643299102783
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 32], 'warp': [1, 16, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: 5.290802955627441
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 16, 32], 'warp': [1, 16, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: 8.59545612335205
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 256, 32], 'warp': [1, 128, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: 4.1443328857421875
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 16], 'warp': [1, 64, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: 7.434444427490234
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 16], 'warp': [1, 32, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: 8.006246566772461
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 16], 'warp': [1, 16, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: 9.938124656677246
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 16, 16], 'warp': [1, 16, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: 12.35763168334961
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 256, 16], 'warp': [1, 128, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: 7.111680030822754
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 128, 8], 'warp': [1, 32, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 16>}}}
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: 14.092493057250977
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 8], 'warp': [1, 16, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 16>}}}
Processing:  36%|███▌      | 17/47 [03:44<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: 14.741094589233398
Processing:  36%|███▌      | 17/47 [03:45<05:26, 10.88s/it]                                                           2024-01-15 02:14:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 32, 8], 'warp': [1, 16, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 16>}}}
Processing:  36%|███▌      | 17/47 [03:45<05:26, 10.88s/it]                                                           2024-01-15 02:14:17 [ladder:DEBUG]: 17.325056076049805
Processing:  36%|███▌      | 17/47 [03:45<05:26, 10.88s/it]                                                           2024-01-15 02:14:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 256, 8], 'warp': [1, 64, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 16>}}}
Processing:  36%|███▌      | 17/47 [03:45<05:26, 10.88s/it]                                                           2024-01-15 02:14:17 [ladder:DEBUG]: 13.990297317504883
Processing:  36%|███▌      | 17/47 [03:45<05:26, 10.88s/it]                                                           2024-01-15 02:14:17 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9__reshape_divide_10>: {'block': [1, 64, 64], 'warp': [1, 32, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [03:45<05:26, 10.88s/it]                                                           2024-01-15 02:14:17 [ladder:INFO]: result: 3.1852545738220215
Processing:  36%|███▌      | 17/47 [03:45<05:26, 10.88s/it]                                                           2024-01-15 02:14:17 [ladder:INFO]: Tuning ['nn_batch_matmul_9']
Processing:  36%|███▌      | 17/47 [03:45<05:26, 10.88s/it]                                                           2024-01-15 02:14:31 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 128], 'warp': [1, 32, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: 3.727769374847412
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 64], 'warp': [1, 64, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: 2.9616129398345947
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 128], 'warp': [1, 64, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: 3.5526657104492188
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 64], 'warp': [1, 32, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: 3.2327678203582764
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 256], 'warp': [1, 64, 128], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 264>}}}
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: 4.226457595825195
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 128], 'warp': [1, 128, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: 5.836185455322266
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 64], 'warp': [2, 32, 32], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: 4.313497543334961
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 128], 'warp': [2, 32, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [04:00<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: 3.778355121612549
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 128, 64], 'warp': [2, 64, 32], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: 3.9702529907226562
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 128], 'warp': [1, 16, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: 3.9458816051483154
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 32], 'warp': [1, 64, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: 4.20474910736084
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 128, 128], 'warp': [2, 64, 64], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: 3.887104034423828
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 64], 'warp': [2, 16, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: 5.809356689453125
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 32], 'warp': [2, 32, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: 5.966643333435059
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 64], 'warp': [1, 16, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: 3.7396481037139893
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 32], 'warp': [1, 32, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: 4.631552219390869
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 256], 'warp': [1, 32, 128], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 264>}}}
Processing:  36%|███▌      | 17/47 [04:01<05:26, 10.88s/it]                                                           2024-01-15 02:14:34 [ladder:DEBUG]: 3.8254592418670654
Processing:  36%|███▌      | 17/47 [04:02<05:26, 10.88s/it]                                                           2024-01-15 02:14:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 64], 'warp': [1, 128, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:02<05:26, 10.88s/it]                                                           2024-01-15 02:14:34 [ladder:DEBUG]: 4.287283420562744
Processing:  36%|███▌      | 17/47 [04:02<05:26, 10.88s/it]                                                           2024-01-15 02:14:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 128], 'warp': [2, 16, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [04:02<05:26, 10.88s/it]                                                           2024-01-15 02:14:34 [ladder:DEBUG]: 4.943052768707275
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 128, 32], 'warp': [2, 64, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:34 [ladder:DEBUG]: 5.114265441894531
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 32, 32], 'warp': [4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: 7.348223686218262
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 32], 'warp': [2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: 6.992076873779297
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 32], 'warp': [1, 16, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: 5.317427158355713
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 32, 64], 'warp': [4, 16, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: 5.676236629486084
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 64, 32], 'warp': [4, 32, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: 6.5720319747924805
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 16, 128], 'warp': [2, 8, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: 8.025087356567383
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 128, 16], 'warp': [2, 64, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: 8.002764701843262
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 16, 128], 'warp': [1, 16, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: 4.528332710266113
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 16], 'warp': [1, 64, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  36%|███▌      | 17/47 [04:03<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: 7.404953956604004
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [4, 64, 64], 'warp': [4, 32, 32], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: 4.709990501403809
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 256], 'warp': [1, 16, 128], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 264>}}}
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: 3.955712080001831
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 256, 32], 'warp': [1, 128, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: 3.7484543323516846
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 16, 64], 'warp': [2, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: 8.652390480041504
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 64, 16], 'warp': [2, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: 8.593202590942383
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 16, 64], 'warp': [1, 16, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: 4.86891508102417
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 64, 16], 'warp': [1, 32, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: 8.10700798034668
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 32, 256], 'warp': [2, 16, 128], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 264>}}}
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: 4.8922624588012695
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [2, 256, 32], 'warp': [2, 128, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: 5.208064079284668
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 16, 32], 'warp': [1, 16, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  36%|███▌      | 17/47 [04:04<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: 8.606719970703125
Processing:  36%|███▌      | 17/47 [04:05<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 32, 16], 'warp': [1, 16, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  36%|███▌      | 17/47 [04:05<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: 9.91887378692627
Processing:  36%|███▌      | 17/47 [04:05<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_9>: {'block': [1, 128, 64], 'warp': [1, 64, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  36%|███▌      | 17/47 [04:05<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:INFO]: result: 2.9616129398345947
Processing:  36%|███▌      | 17/47 [04:05<05:26, 10.88s/it]                                                           2024-01-15 02:14:36 [ladder:INFO]: Tuning ['reshape_divide_10']
Processing:  36%|███▌      | 17/47 [04:05<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 1, 4096], 'thread': [1, 1, 1, 128], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.0507519245147705
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 2, 2048], 'thread': [1, 1, 2, 64], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.008512020111084
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 2, 1, 2048], 'thread': [1, 2, 1, 64], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.0115840435028076
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 4, 1024], 'thread': [1, 1, 4, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.0502400398254395
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 2, 2, 1024], 'thread': [1, 2, 2, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.038975954055786
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 4, 1, 1024], 'thread': [1, 4, 1, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.11680006980896
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 8, 512], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.0602240562438965
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 2, 4, 512], 'thread': [1, 2, 4, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.092736005783081
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 4, 2, 512], 'thread': [1, 4, 2, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.1288321018218994
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 8, 1, 512], 'thread': [1, 8, 1, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.295743942260742
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 16, 256], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.4009599685668945
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 2, 8, 256], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.4165759086608887
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 4, 4, 256], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:19<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.664383888244629
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 8, 2, 256], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 3.910912036895752
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 16, 1, 256], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: 4.443647861480713
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:51 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 32, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 3.322880268096924
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 2, 16, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 3.342745542526245
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 4, 8, 128], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 3.5287041664123535
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 8, 4, 128], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 3.907379150390625
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 16, 2, 128], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 4.408115386962891
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 32, 1, 128], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 5.230591773986816
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 64, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 3.2811520099639893
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 2, 32, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 3.3162240982055664
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 4, 16, 64], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 3.5333120822906494
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 8, 8, 64], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 3.941375970840454
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 16, 4, 64], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 4.404736042022705
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 32, 2, 64], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 5.215487957000732
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 64, 1, 64], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 4.930816173553467
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 1, 2048], 'thread': [1, 1, 1, 128], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 2.8372480869293213
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 2, 1024], 'thread': [1, 1, 2, 64], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:20<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 2.802432060241699
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 2, 1, 1024], 'thread': [1, 2, 1, 64], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 2.802432060241699
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 4, 512], 'thread': [1, 1, 4, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 2.8382720947265625
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 2, 2, 512], 'thread': [1, 2, 2, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: 2.84006404876709
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 4, 1, 512], 'thread': [1, 4, 1, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: 2.8940799236297607
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 8, 256], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: 2.8482561111450195
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 2, 4, 256], 'thread': [1, 2, 4, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: 2.8526079654693604
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 4, 2, 256], 'thread': [1, 4, 2, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: 2.9176321029663086
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 8, 1, 256], 'thread': [1, 8, 1, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: 3.019263982772827
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 16, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: 3.551232099533081
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 2, 8, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: 3.5678720474243164
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_divide_10>: {'block': [1, 1, 2, 1024], 'thread': [1, 1, 2, 64], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:INFO]: result: 2.802432060241699
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:INFO]: Tuning ['nn_batch_matmul_9', 'reshape_divide_10', 'max_11', 'subtract_exp_12']
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]                                                           2024-01-15 02:14:53 [ladder:INFO]: Fusion group created: 6 ['nn_batch_matmul_9', 'reshape_divide_10']
Processing:  36%|███▌      | 17/47 [04:21<05:26, 10.88s/it]Processing:  38%|███▊      | 18/47 [04:21<08:05, 16.76s/it]                                                           2024-01-15 02:14:53 [ladder:INFO]: Tuning ['max_11', 'subtract_exp_12']
Processing:  38%|███▊      | 18/47 [04:21<08:05, 16.76s/it]                                                           2024-01-15 02:14:54 [ladder:INFO]: Tuning ['max_11', 'subtract_exp_12']
Processing:  38%|███▊      | 18/47 [04:22<08:05, 16.76s/it]2024-01-15 02:10:34 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, T_divide_7_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float32), float32, [1, 4096, 8192], []),
             T_divide_7: Buffer(T_divide_7_2: Pointer(float32), float32, [1, 4096, 1], [])}
  buffer_map = {p0_1: p0, T_divide_7_1: T_divide_7} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(p0.shared: Pointer(shared float32), float32, [8192]), storage_scope = shared;
  allocate(normal_reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local {
    for (ax0.ax1.fused.ax2.fused.outer.outer.outer: int32, 0, 64) {
      let cse_var_1: int32 = (ax0.ax1.fused.ax2.fused.outer.outer.outer*128)
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 1;
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 128;
      p0.shared_1: Buffer(p0.shared, float32, [8192], [], scope="shared")[(cse_var_1 + threadIdx.x)] = p0_3: Buffer(p0_2, float32, [33554432], [])[(((blockIdx.x*8192) + cse_var_1) + threadIdx.x)]
    }
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 1 {
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 128 {
        normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float32, [1], [], scope="local")[0] = 0f32
        for (k2.inner.outer: int32, 0, 64) {
          normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[((k2.inner.outer*128) + threadIdx.x_1)])
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x_1, dtype=handle)
      }
      T_divide_7_3: Buffer(T_divide_7_2, float32, [4096], [])[blockIdx.x] = (1f32 / @tir.sqrt(((reduce_temp0_2: Buffer(reduce_temp0, float32, [1], [], scope="local", align=4)[0]*0.00012207f32) + 1e-05f32), dtype=float32))
    }
  }
}


2024-01-15 02:10:59 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, T_divide_7_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float32), float32, [1, 4096, 8192], []),
             T_divide_7: Buffer(T_divide_7_2: Pointer(float32), float32, [1, 4096, 1], [])}
  buffer_map = {p0_1: p0, T_divide_7_1: T_divide_7} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1024;
  allocate(normal_reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float32), float32, [4096]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4 {
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
      normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float32, [1], [], scope="local")[0] = 0f32
      for (k2.outer: int32, 0, 8) {
        for (ax0.ax1.fused.ax2.fused.outer.outer.outer: int32, 0, 8) {
          attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
          attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
          p0.shared_1: Buffer(p0.shared, float32, [4096], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.outer.outer.outer*512) + (threadIdx.y_1*128)) + (threadIdx.x_1*4)), 1, 4)] = p0_3: Buffer(p0_2, float32, [33554432], [])[ramp(((((blockIdx.x*32768) + (floordiv(ax0.ax1.fused.ax2.fused.outer.outer.outer, 2)*8192)) + (k2.outer*1024)) + floormod((((ax0.ax1.fused.ax2.fused.outer.outer.outer*512) + (threadIdx.y_1*128)) + (threadIdx.x_1*4)), 1024)), 1, 4)]
        }
        for (k2.inner.outer: int32, 0, 32) {
          normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*1024) + (k2.inner.outer*32)) + threadIdx.x)])
        }
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    }
    T_divide_7_3: Buffer(T_divide_7_2, float32, [4096], [])[((blockIdx.x*4) + threadIdx.y)] = (1f32 / @tir.sqrt(((reduce_temp0_2: Buffer(reduce_temp0, float32, [1], [], scope="local", align=4)[0]*0.00012207f32) + 1e-05f32), dtype=float32))
  }
}


2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:10:59 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, T_divide_7_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float32), float32, [1, 4096, 8192], []),
             T_divide_7: Buffer(T_divide_7_2: Pointer(float32), float32, [1, 4096, 1], [])}
  buffer_map = {p0_1: p0, T_divide_7_1: T_divide_7} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 64;
  allocate(normal_reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float32), float32, [4096]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64 {
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
      normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float32, [1], [], scope="local")[0] = 0f32
      for (k2.outer: int32, 0, 128) {
        for (ax0.ax1.fused.ax2.fused.outer.outer.outer: int32, 0, 8) {
          attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
          attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
          p0.shared_1: Buffer(p0.shared, float32, [4096], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.outer.outer.outer*512) + (threadIdx.y_1*8)) + (threadIdx.x_1*4)), 1, 4)] = p0_3: Buffer(p0_2, float32, [33554432], [])[ramp(((((((blockIdx.x*524288) + (ax0.ax1.fused.ax2.fused.outer.outer.outer*65536)) + (floordiv(threadIdx.y_1, 8)*8192)) + (k2.outer*64)) + (floormod(threadIdx.y_1, 8)*8)) + (threadIdx.x_1*4)), 1, 4)]
        }
        for (k2.inner.outer: int32, 0, 32) {
          normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*64) + (k2.inner.outer*2)) + threadIdx.x)])
        }
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    }
    T_divide_7_3: Buffer(T_divide_7_2, float32, [4096], [])[((blockIdx.x*64) + threadIdx.y)] = (1f32 / @tir.sqrt(((reduce_temp0_2: Buffer(reduce_temp0, float32, [1], [], scope="local", align=4)[0]*0.00012207f32) + 1e-05f32), dtype=float32))
  }
}


2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:10:59 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, T_divide_7_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float32), float32, [1, 4096, 8192], []),
             T_divide_7: Buffer(T_divide_7_2: Pointer(float32), float32, [1, 4096, 1], [])}
  buffer_map = {p0_1: p0, T_divide_7_1: T_divide_7} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 2048;
  allocate(normal_reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float32), float32, [4096]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2 {
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64 {
      normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float32, [1], [], scope="local")[0] = 0f32
      for (k2.outer: int32, 0, 4) {
        for (ax0.ax1.fused.ax2.fused.outer.outer.outer: int32, 0, 8) {
          attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
          attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64;
          p0.shared_1: Buffer(p0.shared, float32, [4096], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.outer.outer.outer*512) + (threadIdx.y_1*256)) + (threadIdx.x_1*4)), 1, 4)] = p0_3: Buffer(p0_2, float32, [33554432], [])[ramp(((((blockIdx.x*16384) + (floordiv(ax0.ax1.fused.ax2.fused.outer.outer.outer, 4)*8192)) + (k2.outer*2048)) + floormod((((ax0.ax1.fused.ax2.fused.outer.outer.outer*512) + (threadIdx.y_1*256)) + (threadIdx.x_1*4)), 2048)), 1, 4)]
        }
        for (k2.inner.outer: int32, 0, 32) {
          normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*2048) + (k2.inner.outer*64)) + threadIdx.x)])
        }
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    }
    T_divide_7_3: Buffer(T_divide_7_2, float32, [4096], [])[((blockIdx.x*2) + threadIdx.y)] = (1f32 / @tir.sqrt(((reduce_temp0_2: Buffer(reduce_temp0, float32, [1], [], scope="local", align=4)[0]*0.00012207f32) + 1e-05f32), dtype=float32))
  }
}


2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_ax3_1_fused)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_fused_0_0 in T.unroll(8):
                        for ax0_ax1_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + (ax0_ax1_fused_0_0 * 1024 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_fused_0_0 * 1024 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_ax3_1_fused)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_ax3_1_fused)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:14:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_9__reshape_divide_10>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 16)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 16)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 8 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 1024)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 1024 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 16)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 16)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 16)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 16)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 128, 1], 'thread': [1, 1, 128, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.0559871196746826
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 64, 1], 'thread': [1, 2, 64, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.0547585487365723
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 32, 1], 'thread': [1, 4, 32, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.0549631118774414
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 8, 16, 1], 'thread': [1, 8, 16, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.0554239749908447
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 16, 8, 1], 'thread': [1, 16, 8, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.0549120903015137
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 32, 4, 1], 'thread': [1, 32, 4, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.0541439056396484
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 64, 2, 1], 'thread': [1, 64, 2, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.0525057315826416
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 1, 256, 1], 'thread': [1, 1, 128, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.1526527404785156
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 2, 128, 1], 'thread': [1, 2, 64, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.1534719467163086
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 4, 64, 1], 'thread': [1, 4, 32, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.1508097648620605
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 8, 32, 1], 'thread': [1, 8, 16, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.1508097648620605
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 16, 16, 1], 'thread': [1, 16, 8, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:28<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: 2.150604724884033
Processing:  38%|███▊      | 18/47 [04:29<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11__subtract_exp_12>: {'block': [1, 64, 2, 1], 'thread': [1, 64, 2, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  38%|███▊      | 18/47 [04:29<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:INFO]: result: 2.0525057315826416
Processing:  38%|███▊      | 18/47 [04:29<08:05, 16.76s/it]                                                           2024-01-15 02:15:00 [ladder:INFO]: Tuning ['max_11']
Processing:  38%|███▊      | 18/47 [04:29<08:05, 16.76s/it]2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 64)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 64)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 2 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 4096)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 4096 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 64)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 64)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 64)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 64)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
2024-01-15 02:10:59 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, T_divide_7_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float32), float32, [1, 4096, 8192], []),
             T_divide_7: Buffer(T_divide_7_2: Pointer(float32), float32, [1, 4096, 1], [])}
  buffer_map = {p0_1: p0, T_divide_7_1: T_divide_7} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float32), float32, [4096]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 1 {
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 128 {
      normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float32, [1], [], scope="local")[0] = 0f32
      for (k2.outer: int32, 0, 2) {
        for (ax0.ax1.fused.ax2.fused.outer.outer.outer: int32, 0, 8) {
          let cse_var_1: int32 = (ax0.ax1.fused.ax2.fused.outer.outer.outer*512)
          attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 1;
          attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 128;
          p0.shared_1: Buffer(p0.shared, float32, [4096], [], scope="shared")[ramp((cse_var_1 + (threadIdx.x_1*4)), 1, 4)] = p0_3: Buffer(p0_2, float32, [33554432], [])[ramp(((((blockIdx.x*8192) + (k2.outer*4096)) + cse_var_1) + (threadIdx.x_1*4)), 1, 4)]
        }
        for (k2.inner.outer: int32, 0, 32) {
          normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[((k2.inner.outer*128) + threadIdx.x)])
        }
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    }
    T_divide_7_3: Buffer(T_divide_7_2, float32, [4096], [])[blockIdx.x] = (1f32 / @tir.sqrt(((reduce_temp0_2: Buffer(reduce_temp0, float32, [1], [], scope="local", align=4)[0]*0.00012207f32) + 1e-05f32), dtype=float32))
  }
}


2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 16)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 16 * 256 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_fused_0_0 in T.unroll(16):
                        for ax0_ax1_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 16)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 16 * 256 + (ax0_ax1_fused_0_0 * 1024 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_fused_0_0 * 1024 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 16)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 16 * 256 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 16)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 16 * 256 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 32)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 32)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 4 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 2048)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 2048 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 32)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 32)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 32)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 32)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
2024-01-15 02:10:59 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, T_divide_7_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float32), float32, [1, 4096, 8192], []),
             T_divide_7: Buffer(T_divide_7_2: Pointer(float32), float32, [1, 4096, 1], [])}
  buffer_map = {p0_1: p0, T_divide_7_1: T_divide_7} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 256;
  allocate(normal_reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float32), float32, [4096]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16 {
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
      normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float32, [1], [], scope="local")[0] = 0f32
      for (k2.outer: int32, 0, 32) {
        for (ax0.ax1.fused.ax2.fused.outer.outer.outer: int32, 0, 8) {
          attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
          attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
          p0.shared_1: Buffer(p0.shared, float32, [4096], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.outer.outer.outer*512) + (threadIdx.y_1*32)) + (threadIdx.x_1*4)), 1, 4)] = p0_3: Buffer(p0_2, float32, [33554432], [])[ramp(((((((blockIdx.x*131072) + (ax0.ax1.fused.ax2.fused.outer.outer.outer*16384)) + (floordiv(threadIdx.y_1, 8)*8192)) + (k2.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*4)), 1, 4)]
        }
        for (k2.inner.outer: int32, 0, 32) {
          normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*256) + (k2.inner.outer*8)) + threadIdx.x)])
        }
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    }
    T_divide_7_3: Buffer(T_divide_7_2, float32, [4096], [])[((blockIdx.x*16) + threadIdx.y)] = (1f32 / @tir.sqrt(((reduce_temp0_2: Buffer(reduce_temp0, float32, [1], [], scope="local", align=4)[0]*0.00012207f32) + 1e-05f32), dtype=float32))
  }
}


2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 8)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 8)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 16 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 512)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 512 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 8)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 8)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 8)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 8)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 8 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 16)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 16 * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(16):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 8 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 2048)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 2048 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 8 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 16)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 16 * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 8 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 16)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 16 * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
2024-01-15 02:10:59 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, T_divide_7_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float32), float32, [1, 4096, 8192], []),
             T_divide_7: Buffer(T_divide_7_2: Pointer(float32), float32, [1, 4096, 1], [])}
  buffer_map = {p0_1: p0, T_divide_7_1: T_divide_7} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 128;
  allocate(normal_reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float32), float32, [4096]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32 {
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
      normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float32, [1], [], scope="local")[0] = 0f32
      for (k2.outer: int32, 0, 64) {
        for (ax0.ax1.fused.ax2.fused.outer.outer.outer: int32, 0, 8) {
          attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
          attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
          p0.shared_1: Buffer(p0.shared, float32, [4096], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.outer.outer.outer*512) + (threadIdx.y_1*16)) + (threadIdx.x_1*4)), 1, 4)] = p0_3: Buffer(p0_2, float32, [33554432], [])[ramp(((((((blockIdx.x*262144) + (ax0.ax1.fused.ax2.fused.outer.outer.outer*32768)) + (floordiv(threadIdx.y_1, 8)*8192)) + (k2.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*4)), 1, 4)]
        }
        for (k2.inner.outer: int32, 0, 32) {
          normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*128) + (k2.inner.outer*4)) + threadIdx.x)])
        }
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    }
    T_divide_7_3: Buffer(T_divide_7_2, float32, [4096], [])[((blockIdx.x*32) + threadIdx.y)] = (1f32 / @tir.sqrt(((reduce_temp0_2: Buffer(reduce_temp0, float32, [1], [], scope="local", align=4)[0]*0.00012207f32) + 1e-05f32), dtype=float32))
  }
}


2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32 * 2 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 64)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 64 * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(16):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32 * 2 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 8192)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 8192 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32 * 2 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 64)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 64 * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32 * 2 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 64)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 64 * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
2024-01-15 02:10:59 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, T_divide_7_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float32), float32, [1, 4096, 8192], []),
             T_divide_7: Buffer(T_divide_7_2: Pointer(float32), float32, [1, 4096, 1], [])}
  buffer_map = {p0_1: p0, T_divide_7_1: T_divide_7} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 512;
  allocate(normal_reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float32), float32, [4096]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8 {
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16 {
      normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float32, [1], [], scope="local")[0] = 0f32
      for (k2.outer: int32, 0, 16) {
        for (ax0.ax1.fused.ax2.fused.outer.outer.outer: int32, 0, 8) {
          attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
          attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16;
          p0.shared_1: Buffer(p0.shared, float32, [4096], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.outer.outer.outer*512) + (threadIdx.y_1*64)) + (threadIdx.x_1*4)), 1, 4)] = p0_3: Buffer(p0_2, float32, [33554432], [])[ramp((((((blockIdx.x*65536) + (ax0.ax1.fused.ax2.fused.outer.outer.outer*8192)) + (k2.outer*512)) + (threadIdx.y_1*64)) + (threadIdx.x_1*4)), 1, 4)]
        }
        for (k2.inner.outer: int32, 0, 32) {
          normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*512) + (k2.inner.outer*16)) + threadIdx.x)])
        }
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    }
    T_divide_7_3: Buffer(T_divide_7_2, float32, [4096], [])[((blockIdx.x*8) + threadIdx.y)] = (1f32 / @tir.sqrt(((reduce_temp0_2: Buffer(reduce_temp0, float32, [1], [], scope="local", align=4)[0]*0.00012207f32) + 1e-05f32), dtype=float32))
  }
}


2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_1_ax1_1_ax2_1_ax3_1_fused // 2)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 2)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 128)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 2 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 128 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_1_ax1_1_ax2_1_ax3_1_fused // 2)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 2)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_1_ax1_1_ax2_1_ax3_1_fused // 2)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 2)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:ERROR]: Fail to create schedule for <Node, max_11__subtract_exp_12>, the error is Schedule not implemented
2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 1024 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 4)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 1024 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 4)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 1024 * 32 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 256)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 1024 * 4 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 256 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 1024 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 4)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 1024 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 4)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 1024 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 4)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 1024 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 4)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 4 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 32)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 32 * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(16):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 4 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 4096)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 4096 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 4 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 32)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 32 * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 4 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 32)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 32 * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
2024-01-15 02:14:55 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(1, 64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 8)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 8 * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(-65504)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(16):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 16 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 1024)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 1024 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 8)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 8 * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 8)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 8 * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(input1[v_ax0, v_ax1, v_ax2, v_ax3], mediate0[v_ax0, v_ax1, v_ax2, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2, v_ax3])
                output0[v_ax0, v_ax1, v_ax2, v_ax3] = T.exp(input1[v_ax0, v_ax1, v_ax2, v_ax3] - mediate0[v_ax0, v_ax1, v_ax2, 0], dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 32, 1], 'thread': [1, 1, 32, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 2.063769578933716
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 2, 16, 1], 'thread': [1, 2, 16, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 2.063769578933716
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 16, 1], 'thread': [1, 1, 16, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.342259168624878
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 4, 8, 1], 'thread': [1, 4, 8, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 2.066431999206543
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 2, 8, 1], 'thread': [1, 2, 8, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.3527039289474487
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 8, 1], 'thread': [1, 1, 8, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.2935168743133545
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 8, 4, 1], 'thread': [1, 8, 4, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 2.0678656101226807
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 4, 4, 1], 'thread': [1, 4, 4, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.3479936122894287
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 2, 4, 1], 'thread': [1, 2, 4, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.2904447317123413
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 4, 1], 'thread': [1, 1, 4, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.2683264017105103
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 16, 2, 1], 'thread': [1, 16, 2, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 2.068070411682129
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 8, 2, 1], 'thread': [1, 8, 2, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.323622465133667
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 4, 2, 1], 'thread': [1, 4, 2, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.2910592555999756
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 2, 2, 1], 'thread': [1, 2, 2, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.2816383838653564
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 2, 1], 'thread': [1, 1, 2, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:43<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.2654591798782349
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 1, 1], 'thread': [1, 1, 1, 1], 'rstep': [4096], 'reduce_thread': [128], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.2701696157455444
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 32, 1, 1], 'thread': [1, 32, 1, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 2.0692992210388184
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 16, 1, 1], 'thread': [1, 16, 1, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: 1.3207552433013916
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 8, 1, 1], 'thread': [1, 8, 1, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 1.286348819732666
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 4, 1, 1], 'thread': [1, 4, 1, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 1.2791807651519775
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 2, 1, 1], 'thread': [1, 2, 1, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 1.2636159658432007
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 128, 1], 'thread': [1, 1, 128, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 2.061311960220337
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 2, 64, 1], 'thread': [1, 2, 64, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 2.06213116645813
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 4, 32, 1], 'thread': [1, 4, 32, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 2.063155174255371
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 8, 16, 1], 'thread': [1, 8, 16, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 2.063155174255371
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 64, 1], 'thread': [1, 1, 64, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 3.8270976543426514
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 2, 32, 1], 'thread': [1, 2, 32, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 3.825049638748169
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 4, 16, 1], 'thread': [1, 4, 16, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 3.8256640434265137
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 16, 8, 1], 'thread': [1, 16, 8, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 2.06213116645813
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 8, 8, 1], 'thread': [1, 8, 8, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 3.826892852783203
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 32, 4, 1], 'thread': [1, 32, 4, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 2.0619263648986816
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 16, 4, 1], 'thread': [1, 16, 4, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:44<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 3.8264832496643066
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 64, 2, 1], 'thread': [1, 64, 2, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 2.063769578933716
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 32, 2, 1], 'thread': [1, 32, 2, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: 3.8287360668182373
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 64, 1, 1], 'thread': [1, 64, 1, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: 3.832831859588623
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 256, 1], 'thread': [1, 1, 128, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: 2.1508097648620605
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 2, 128, 1], 'thread': [1, 2, 64, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: 2.1534719467163086
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 4, 64, 1], 'thread': [1, 4, 32, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: 2.1516289710998535
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 8, 32, 1], 'thread': [1, 8, 16, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: 2.153676748275757
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 16, 16, 1], 'thread': [1, 16, 8, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: 2.1526527404785156
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 2, 1, 1], 'thread': [1, 2, 1, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:INFO]: result: 1.2636159658432007
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:17 [ladder:INFO]: Tuning ['subtract_exp_12']
Processing:  38%|███▊      | 18/47 [04:45<08:05, 16.76s/it]                                                           2024-01-15 02:15:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 1, 4096], 'thread': [1, 1, 1, 128], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:32 [ladder:DEBUG]: 3.370598316192627
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 2, 2048], 'thread': [1, 1, 2, 64], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:32 [ladder:DEBUG]: 3.3906688690185547
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 2, 1, 2048], 'thread': [1, 2, 1, 64], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: 3.389235258102417
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 4, 1024], 'thread': [1, 1, 4, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: 3.3556480407714844
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 2, 2, 1024], 'thread': [1, 2, 2, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: 3.3853440284729004
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 4, 1, 1024], 'thread': [1, 4, 1, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: 3.4560000896453857
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 8, 512], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: 3.4582526683807373
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 2, 4, 512], 'thread': [1, 2, 4, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: 3.5737597942352295
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 4, 2, 512], 'thread': [1, 4, 2, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: 3.7890048027038574
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 16, 256], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: 4.112793922424316
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 2, 8, 256], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:01<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: 4.226662635803223
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 4, 4, 256], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: 5.319884777069092
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 8, 1, 512], 'thread': [1, 8, 1, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: 4.101529598236084
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 8, 2, 256], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: 5.858304023742676
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 16, 1, 256], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: 6.272614479064941
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 32, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: 3.977011203765869
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 2, 16, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: 4.052172660827637
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 4, 8, 128], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: 4.719615936279297
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 8, 4, 128], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: 5.269708633422852
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 16, 2, 128], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: 5.71514892578125
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 32, 1, 128], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: 6.138470649719238
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 64, 64], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: 3.952230453491211
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 2, 32, 64], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:02<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: 4.0398850440979
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 4, 16, 64], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: 4.518092632293701
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 8, 8, 64], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 5.008998394012451
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 16, 4, 64], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 5.43006706237793
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 32, 2, 64], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 5.83454704284668
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 64, 1, 64], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 5.694873332977295
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 1, 2048], 'thread': [1, 1, 1, 128], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 2.8749823570251465
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 2, 1024], 'thread': [1, 1, 2, 64], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 2.853273630142212
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 2, 1, 1024], 'thread': [1, 2, 1, 64], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 2.871091365814209
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 4, 512], 'thread': [1, 1, 4, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 2.889113664627075
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 2, 2, 512], 'thread': [1, 2, 2, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 2.894028902053833
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 8, 256], 'thread': [1, 1, 8, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 2.976153612136841
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 2, 4, 256], 'thread': [1, 2, 4, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 3.0146560668945312
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 4, 1, 512], 'thread': [1, 4, 1, 32], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:03<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 2.984755039215088
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 4, 2, 256], 'thread': [1, 4, 2, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 3.239116668701172
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 8, 1, 256], 'thread': [1, 8, 1, 16], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: 3.404595136642456
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 16, 128], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:36 [ladder:DEBUG]: 3.631103992462158
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 2, 8, 128], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:36 [ladder:DEBUG]: 3.6925442218780518
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:36 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, subtract_exp_12>: {'block': [1, 1, 2, 1024], 'thread': [1, 1, 2, 64], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:36 [ladder:INFO]: result: 2.853273630142212
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:36 [ladder:INFO]: Tuning ['max_11', 'subtract_exp_12', 'sum_13', 'divide_cast_cast_reshape_14']
Processing:  38%|███▊      | 18/47 [05:04<08:05, 16.76s/it]                                                           2024-01-15 02:15:40 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 1, 4096], 'thread': [1, 1, 1, 128], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, subtract_exp_12>: {'block': [1, 1, 1, 4096], 'thread': [1, 1, 1, 128], 'rstep': [], 'step': [1, 1, 1, 2]}, <Node, sum_13>: {'block': [1, 1, 1, 4096], 'thread': [1, 1, 1, 128], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 1, 4096], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:08<08:05, 16.76s/it]                                                           2024-01-15 02:15:40 [ladder:DEBUG]: 59.0331916809082
Processing:  38%|███▊      | 18/47 [05:08<08:05, 16.76s/it]                                                           2024-01-15 02:15:40 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 2, 4096], 'thread': [1, 1, 2, 64], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, subtract_exp_12>: {'block': [1, 1, 2, 4096], 'thread': [1, 1, 2, 64], 'rstep': [], 'step': [1, 1, 1, 2]}, <Node, sum_13>: {'block': [1, 1, 2, 4096], 'thread': [1, 1, 2, 64], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 2, 4096], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:08<08:05, 16.76s/it]                                                           2024-01-15 02:15:41 [ladder:DEBUG]: 81.89112854003906
Processing:  38%|███▊      | 18/47 [05:09<08:05, 16.76s/it]                                                           2024-01-15 02:15:41 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, max_11>: {'block': [1, 1, 1, 4096], 'thread': [1, 1, 1, 128], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, subtract_exp_12>: {'block': [1, 1, 1, 4096], 'thread': [1, 1, 1, 128], 'rstep': [], 'step': [1, 1, 1, 2]}, <Node, sum_13>: {'block': [1, 1, 1, 4096], 'thread': [1, 1, 1, 128], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 1, 4096], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:09<08:05, 16.76s/it]                                                           2024-01-15 02:15:41 [ladder:INFO]: result: 59.0331916809082
Processing:  38%|███▊      | 18/47 [05:09<08:05, 16.76s/it]                                                           2024-01-15 02:15:41 [ladder:INFO]: Tuning ['sum_13']
Processing:  38%|███▊      | 18/47 [05:09<08:05, 16.76s/it]@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16384;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 8) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*64)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((blockIdx.x*65536) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*8192)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*512)) + (floormod(threadIdx.y_1, 8)*64)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*512) + (k3.inner.outer*8)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*16) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 32768;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 4) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 2048)*67108864) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16777216)) + (floormod(blockIdx.x, 2048)*8192)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*4096)) + (k3.outer*1024)) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*1024) + (k3.inner.outer*16)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 2048)*16384) + (floordiv(threadIdx.y, 2)*4096)) + (floormod(blockIdx.x, 2048)*2)) + floormod(threadIdx.y, 2))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 2048)*536870912) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*67108864)) + (floordiv(threadIdx.y_1, 16)*16777216)) + (floormod(blockIdx.x, 2048)*8192)) + (floordiv(floormod(threadIdx.y_1, 16), 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 2048)*131072) + (floordiv(threadIdx.y, 2)*4096)) + (floormod(blockIdx.x, 2048)*2)) + floormod(threadIdx.y, 2))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:36 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
2024-01-15 02:15:36 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 32768;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 4) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((blockIdx.x*32768) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*4096)) + (k3.outer*1024)) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*1024) + (k3.inner.outer*16)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*8) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 65536;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 2) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((floordiv(blockIdx.x, 4096)*67108864) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16777216)) + (floormod(blockIdx.x, 4096)*4096)) + (k3.outer*2048)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 2048)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*2048) + (k3.inner.outer*32)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[(((floordiv(blockIdx.x, 4096)*16384) + (threadIdx.y*4096)) + floormod(blockIdx.x, 4096))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 256)*67108864) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16777216)) + (floormod(blockIdx.x, 256)*65536)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*32768)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 256)*16384) + (floordiv(threadIdx.y, 16)*4096)) + (floormod(blockIdx.x, 256)*16)) + floormod(threadIdx.y, 16))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16384;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 8) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*64)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 512)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 512)*32768)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*8192)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*512)) + (floormod(threadIdx.y_1, 8)*64)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*512) + (k3.inner.outer*8)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 512)*8192) + (floordiv(threadIdx.y, 8)*4096)) + (floormod(blockIdx.x, 512)*8)) + floormod(threadIdx.y, 8))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 32768;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 4) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((floordiv(blockIdx.x, 4096)*134217728) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*16777216)) + (floormod(blockIdx.x, 4096)*4096)) + (k3.outer*1024)) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*1024) + (k3.inner.outer*16)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[(((floordiv(blockIdx.x, 4096)*32768) + (threadIdx.y*4096)) + floormod(blockIdx.x, 4096))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 128)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 128)*131072)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*32768)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 128)*8192) + (floordiv(threadIdx.y, 32)*4096)) + (floormod(blockIdx.x, 128)*32)) + floormod(threadIdx.y, 32))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 65536;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 2) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((blockIdx.x*16384) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*4096)) + (k3.outer*2048)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 2048)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*2048) + (k3.inner.outer*32)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*4) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16384;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 8) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*64)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 2048)*134217728) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*16777216)) + (floormod(blockIdx.x, 2048)*8192)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*512)) + (floormod(threadIdx.y_1, 8)*64)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*512) + (k3.inner.outer*8)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 2048)*32768) + (floordiv(threadIdx.y, 2)*4096)) + (floormod(blockIdx.x, 2048)*2)) + floormod(threadIdx.y, 2))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 1024)*268435456) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*33554432)) + (floordiv(threadIdx.y_1, 32)*16777216)) + (floormod(blockIdx.x, 1024)*16384)) + (floordiv(floormod(threadIdx.y_1, 32), 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 1024)*65536) + (floordiv(threadIdx.y, 4)*4096)) + (floormod(blockIdx.x, 1024)*4)) + floormod(threadIdx.y, 4))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 32768;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 4) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 1024)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 1024)*16384)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*4096)) + (k3.outer*1024)) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*1024) + (k3.inner.outer*16)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 1024)*8192) + (floordiv(threadIdx.y, 4)*4096)) + (floormod(blockIdx.x, 1024)*4)) + floormod(threadIdx.y, 4))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 4096)*536870912) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*67108864)) + (floordiv(threadIdx.y_1, 8)*16777216)) + (floormod(blockIdx.x, 4096)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[(((floordiv(blockIdx.x, 4096)*131072) + (threadIdx.y*4096)) + floormod(blockIdx.x, 4096))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 512)*134217728) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*16777216)) + (floormod(blockIdx.x, 512)*32768)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 512)*32768) + (floordiv(threadIdx.y, 8)*4096)) + (floormod(blockIdx.x, 512)*8)) + floormod(threadIdx.y, 8))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 512)*67108864) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16777216)) + (floormod(blockIdx.x, 512)*32768)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16384)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 512)*16384) + (floordiv(threadIdx.y, 8)*4096)) + (floormod(blockIdx.x, 512)*8)) + floormod(threadIdx.y, 8))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 262144;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [4096]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 128 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 4) {
      let cse_var_1: int32 = (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024)
      attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 1;
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 128;
      p0.shared_1: Buffer(p0.shared, float16, [4096], [], scope="shared")[ramp((cse_var_1 + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((blockIdx.x*4096) + cse_var_1) + (threadIdx.x_1*8)), 1, 8)]
    }
    for (k3.inner.outer: int32, 0, 32) {
      normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[((k3.inner.outer*128) + threadIdx.x)])
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[blockIdx.x] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 131072;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
      attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64;
      p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*512)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((floordiv(blockIdx.x, 4096)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 4096)*4096)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*512)) + (threadIdx.x_1*8)), 4096)), 1, 8)]
    }
    for (k3.inner.outer: int32, 0, 64) {
      normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*4096) + (k3.inner.outer*64)) + threadIdx.x)])
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[(((floordiv(blockIdx.x, 4096)*8192) + (threadIdx.y*4096)) + floormod(blockIdx.x, 4096))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:36 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
2024-01-15 02:15:36 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16384;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 8) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*64)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 1024)*67108864) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16777216)) + (floormod(blockIdx.x, 1024)*16384)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*8192)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*512)) + (floormod(threadIdx.y_1, 8)*64)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*512) + (k3.inner.outer*8)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 1024)*16384) + (floordiv(threadIdx.y, 4)*4096)) + (floormod(blockIdx.x, 1024)*4)) + floormod(threadIdx.y, 4))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16384;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 8) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*64)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 4096)*268435456) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*33554432)) + (floordiv(threadIdx.y_1, 8)*16777216)) + (floormod(blockIdx.x, 4096)*4096)) + (k3.outer*512)) + (floormod(threadIdx.y_1, 8)*64)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*512) + (k3.inner.outer*8)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[(((floordiv(blockIdx.x, 4096)*65536) + (threadIdx.y*4096)) + floormod(blockIdx.x, 4096))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*134217728) + (floordiv(threadIdx.y_1, 8)*16777216)) + (blockIdx.x*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((threadIdx.y*4096) + blockIdx.x)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 1024)*134217728) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*16777216)) + (floormod(blockIdx.x, 1024)*16384)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 1024)*32768) + (floordiv(threadIdx.y, 4)*4096)) + (floormod(blockIdx.x, 1024)*4)) + floormod(threadIdx.y, 4))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 65536;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 2) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((floordiv(blockIdx.x, 2048)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 2048)*8192)) + (floordiv(floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4), 2)*4096)) + (k3.outer*2048)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 2048)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*2048) + (k3.inner.outer*32)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 2048)*8192) + (floordiv(threadIdx.y, 2)*4096)) + (floormod(blockIdx.x, 2048)*2)) + floormod(threadIdx.y, 2))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((blockIdx.x*262144) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*32768)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*64) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:36 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
2024-01-15 02:15:36 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 256)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 256)*65536)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16384)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 256)*8192) + (floordiv(threadIdx.y, 16)*4096)) + (floormod(blockIdx.x, 256)*16)) + floormod(threadIdx.y, 16))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 131072;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
      let cse_var_1: int32 = (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024)
      attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64;
      p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp(((cse_var_1 + (threadIdx.y_1*512)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((blockIdx.x*8192) + cse_var_1) + (threadIdx.y_1*512)) + (threadIdx.x_1*8)), 1, 8)]
    }
    for (k3.inner.outer: int32, 0, 64) {
      normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*4096) + (k3.inner.outer*64)) + threadIdx.x)])
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*2) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16384;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 8) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*64)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((blockIdx.x*65536) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*8192)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*512)) + (floormod(threadIdx.y_1, 8)*64)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*512) + (k3.inner.outer*8)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*16) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((blockIdx.x*131072) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*16384)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*32) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:01 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = -65504f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 2048)*268435456) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*33554432)) + (floordiv(threadIdx.y_1, 16)*16777216)) + (floormod(blockIdx.x, 2048)*8192)) + (floordiv(floormod(threadIdx.y_1, 16), 8)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = max(normal_reduce_temp0_1[0], p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 2048)*65536) + (floordiv(threadIdx.y, 2)*4096)) + (floormod(blockIdx.x, 2048)*2)) + floormod(threadIdx.y, 2))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 256)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 256)*65536)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16384)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 256)*8192) + (floordiv(threadIdx.y, 16)*4096)) + (floormod(blockIdx.x, 256)*16)) + floormod(threadIdx.y, 16))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16384;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 8) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*64)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 512)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 512)*32768)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*8192)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*512)) + (floormod(threadIdx.y_1, 8)*64)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*512) + (k3.inner.outer*8)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 512)*8192) + (floordiv(threadIdx.y, 8)*4096)) + (floormod(blockIdx.x, 512)*8)) + floormod(threadIdx.y, 8))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 262144;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [4096]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 1;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 128 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 4) {
      let cse_var_1: int32 = (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024)
      attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 1;
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 128;
      p0.shared_1: Buffer(p0.shared, float16, [4096], [], scope="shared")[ramp((cse_var_1 + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((blockIdx.x*4096) + cse_var_1) + (threadIdx.x_1*8)), 1, 8)]
    }
    for (k3.inner.outer: int32, 0, 32) {
      normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[((k3.inner.outer*128) + threadIdx.x)])
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[blockIdx.x] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 131072;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
      attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64;
      p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*512)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((floordiv(blockIdx.x, 4096)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 4096)*4096)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*512)) + (threadIdx.x_1*8)), 4096)), 1, 8)]
    }
    for (k3.inner.outer: int32, 0, 64) {
      normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*4096) + (k3.inner.outer*64)) + threadIdx.x)])
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[(((floordiv(blockIdx.x, 4096)*8192) + (threadIdx.y*4096)) + floormod(blockIdx.x, 4096))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 1024)*134217728) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*16777216)) + (floormod(blockIdx.x, 1024)*16384)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 1024)*32768) + (floordiv(threadIdx.y, 4)*4096)) + (floormod(blockIdx.x, 1024)*4)) + floormod(threadIdx.y, 4))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 32768;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 4) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((floordiv(blockIdx.x, 4096)*134217728) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*16777216)) + (floormod(blockIdx.x, 4096)*4096)) + (k3.outer*1024)) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*1024) + (k3.inner.outer*16)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[(((floordiv(blockIdx.x, 4096)*32768) + (threadIdx.y*4096)) + floormod(blockIdx.x, 4096))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((blockIdx.x*262144) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*32768)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*64) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
                                                           2024-01-15 02:15:54 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 32, 1], 'thread': [1, 1, 32, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:54 [ladder:DEBUG]: 2.0654079914093018
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:54 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 2, 16, 1], 'thread': [1, 2, 16, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:54 [ladder:DEBUG]: 2.068275213241577
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:54 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 16, 1], 'thread': [1, 1, 16, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.3301759958267212
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 4, 8, 1], 'thread': [1, 4, 8, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 2.0703232288360596
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 2, 8, 1], 'thread': [1, 2, 8, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.3502464294433594
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 8, 1], 'thread': [1, 1, 8, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.2877824306488037
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 8, 4, 1], 'thread': [1, 8, 4, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 2.0692992210388184
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 4, 4, 1], 'thread': [1, 4, 4, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.347379207611084
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 2, 4, 1], 'thread': [1, 2, 4, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.2912639379501343
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 4, 1], 'thread': [1, 1, 4, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.2640255689620972
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 16, 2, 1], 'thread': [1, 16, 2, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 2.069913625717163
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 8, 2, 1], 'thread': [1, 8, 2, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.3031423091888428
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 4, 2, 1], 'thread': [1, 4, 2, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.2920831441879272
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 2, 2, 1], 'thread': [1, 2, 2, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.2799999713897705
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 2, 1], 'thread': [1, 1, 2, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.2679167985916138
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 1, 1], 'thread': [1, 1, 1, 1], 'rstep': [4096], 'reduce_thread': [128], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.2725759744644165
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 32, 1, 1], 'thread': [1, 32, 1, 1], 'rstep': [256], 'reduce_thread': [4], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 2.071347236633301
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 16, 1, 1], 'thread': [1, 16, 1, 1], 'rstep': [512], 'reduce_thread': [8], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:23<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.316864013671875
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 8, 1, 1], 'thread': [1, 8, 1, 1], 'rstep': [1024], 'reduce_thread': [16], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.289625644683838
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 4, 1, 1], 'thread': [1, 4, 1, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.2742655277252197
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 2, 1, 1], 'thread': [1, 2, 1, 1], 'rstep': [4096], 'reduce_thread': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: 1.2670975923538208
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 128, 1], 'thread': [1, 1, 128, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 2.075033664703369
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 2, 64, 1], 'thread': [1, 2, 64, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 2.0848641395568848
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 4, 32, 1], 'thread': [1, 4, 32, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 2.0865025520324707
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 8, 16, 1], 'thread': [1, 8, 16, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 2.0768768787384033
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 64, 1], 'thread': [1, 1, 64, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 3.8359038829803467
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 2, 32, 1], 'thread': [1, 2, 32, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 3.8400001525878906
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 4, 16, 1], 'thread': [1, 4, 16, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 3.8414337635040283
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 16, 8, 1], 'thread': [1, 16, 8, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 2.075852870941162
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 8, 8, 1], 'thread': [1, 8, 8, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 3.8434815406799316
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 32, 4, 1], 'thread': [1, 32, 4, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 2.0764670372009277
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 16, 4, 1], 'thread': [1, 16, 4, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 3.8367233276367188
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 64, 2, 1], 'thread': [1, 64, 2, 1], 'rstep': [64], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 2.077491283416748
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 32, 2, 1], 'thread': [1, 32, 2, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 3.837952136993408
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 64, 1, 1], 'thread': [1, 64, 1, 1], 'rstep': [128], 'reduce_thread': [2], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:24<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 3.8406143188476562
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 256, 1], 'thread': [1, 1, 128, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 2.167398452758789
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 2, 128, 1], 'thread': [1, 2, 64, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: 2.181119918823242
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 4, 64, 1], 'thread': [1, 4, 32, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:57 [ladder:DEBUG]: 2.20467209815979
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 8, 32, 1], 'thread': [1, 8, 16, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:57 [ladder:DEBUG]: 2.1999616622924805
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 16, 16, 1], 'thread': [1, 16, 8, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:57 [ladder:DEBUG]: 2.1981184482574463
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:57 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 4, 1], 'thread': [1, 1, 4, 1], 'rstep': [2048], 'reduce_thread': [32], 'vectorize': {'p0': 8}}}
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:57 [ladder:INFO]: result: 1.2640255689620972
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:15:57 [ladder:INFO]: Tuning ['divide_cast_cast_reshape_14']
Processing:  38%|███▊      | 18/47 [05:25<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 1, 4096], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: 3.3236992359161377
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 2, 2048], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: 3.321650981903076
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 1, 2048], 'thread': [2, 1, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: 3.3261566162109375
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 4, 1024], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: 3.300966262817383
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 2, 1024], 'thread': [2, 2, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: 3.3148930072784424
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [4, 1, 1024], 'thread': [4, 1, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: 3.430809736251831
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 8, 512], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: 3.516416072845459
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 4, 512], 'thread': [2, 4, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:44<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: 3.5786750316619873
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [4, 2, 512], 'thread': [4, 2, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: 3.795558452606201
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 16, 256], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: 3.907379150390625
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 8, 256], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: 4.003430366516113
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [4, 4, 256], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: 4.739891052246094
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [8, 1, 512], 'thread': [8, 1, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: 4.1019392013549805
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [8, 2, 256], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: 5.200076580047607
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [16, 1, 256], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: 5.600255966186523
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 32, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: 4.16849946975708
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 16, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: 4.211916923522949
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [4, 8, 128], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: 4.7624192237854
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [8, 4, 128], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:45<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: 5.229363441467285
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [16, 2, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: 5.6715264320373535
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [32, 1, 128], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 6.097715377807617
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 64, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 3.6382720470428467
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 32, 64], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 3.687628984451294
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [4, 16, 64], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 4.022886276245117
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [8, 8, 64], 'thread': [8, 2, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 4.440883159637451
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [16, 4, 64], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 4.867072105407715
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [32, 2, 64], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 6.058598518371582
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [64, 1, 64], 'thread': [16, 1, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 5.829836845397949
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 1, 2048], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 2.88972806930542
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 2, 1024], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 2.867814540863037
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 1, 1024], 'thread': [2, 1, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:46<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 2.8674046993255615
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 4, 512], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 2.9040639400482178
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 2, 512], 'thread': [2, 2, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 2.900991916656494
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 8, 256], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: 2.9497342109680176
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 4, 256], 'thread': [2, 4, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: 2.990694522857666
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [4, 1, 512], 'thread': [4, 1, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: 2.9954047203063965
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [4, 2, 256], 'thread': [4, 2, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: 3.1920127868652344
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [8, 1, 256], 'thread': [8, 1, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: 3.374899387359619
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: 3.5583999156951904
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 8, 128], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: 3.613900661468506
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 1, 1024], 'thread': [2, 1, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:INFO]: result: 2.8674046993255615
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]                                                           2024-01-15 02:16:19 [ladder:INFO]: Fusion group created: 7 ['max_11', 'subtract_exp_12']
Processing:  38%|███▊      | 18/47 [05:47<08:05, 16.76s/it]Processing:  43%|████▎     | 20/47 [05:47<10:50, 24.09s/it]                                                           2024-01-15 02:16:19 [ladder:INFO]: Tuning ['sum_13', 'divide_cast_cast_reshape_14']
Processing:  43%|████▎     | 20/47 [05:47<10:50, 24.09s/it]                                                           2024-01-15 02:16:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 1, 4096], 'thread': [1, 1, 1, 128], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 1, 4096], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  43%|████▎     | 20/47 [05:53<10:50, 24.09s/it]                                                           2024-01-15 02:16:25 [ladder:DEBUG]: 29.428531646728516
Processing:  43%|████▎     | 20/47 [05:54<10:50, 24.09s/it]                                                           2024-01-15 02:16:25 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 2, 4096], 'thread': [1, 1, 2, 64], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 2, 4096], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  43%|████▎     | 20/47 [05:54<10:50, 24.09s/it]                                                           2024-01-15 02:16:26 [ladder:DEBUG]: 17.71376609802246
Processing:  43%|████▎     | 20/47 [05:54<10:50, 24.09s/it]                                                           2024-01-15 02:16:26 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 2, 1, 4096], 'thread': [1, 2, 1, 64], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 1, 4096], 'thread': [2, 1, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  43%|████▎     | 20/47 [05:54<10:50, 24.09s/it]                                                           2024-01-15 02:16:26 [ladder:DEBUG]: 17.712127685546875
Processing:  43%|████▎     | 20/47 [05:54<10:50, 24.09s/it]                                                           2024-01-15 02:16:26 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 1, 4, 4096], 'thread': [1, 1, 4, 32], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [1, 4, 4096], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  43%|████▎     | 20/47 [05:54<10:50, 24.09s/it]                                                           2024-01-15 02:16:26 [ladder:DEBUG]: 19.46112060546875
Processing:  43%|████▎     | 20/47 [05:54<10:50, 24.09s/it]                                                           2024-01-15 02:16:26 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 2, 2, 4096], 'thread': [1, 2, 2, 32], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 2, 4096], 'thread': [2, 2, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  43%|████▎     | 20/47 [05:54<10:50, 24.09s/it]                                                           2024-01-15 02:16:26 [ladder:DEBUG]: 19.46112060546875
Processing:  43%|████▎     | 20/47 [05:55<10:50, 24.09s/it]                                                           2024-01-15 02:16:26 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 4, 1, 4096], 'thread': [1, 4, 1, 32], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [4, 1, 4096], 'thread': [4, 1, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  43%|████▎     | 20/47 [05:55<10:50, 24.09s/it]                                                           2024-01-15 02:16:27 [ladder:DEBUG]: 19.398042678833008
Processing:  43%|████▎     | 20/47 [05:55<10:50, 24.09s/it]                                                           2024-01-15 02:16:27 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13>: {'block': [1, 2, 1, 4096], 'thread': [1, 2, 1, 64], 'rstep': [4096], 'step': [1, 1, 1, 2], 'vectorize': {'p0': 8}}, <Node, divide_cast_cast_reshape_14>: {'block': [2, 1, 4096], 'thread': [2, 1, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  43%|████▎     | 20/47 [05:55<10:50, 24.09s/it]                                                           2024-01-15 02:16:27 [ladder:INFO]: result: 17.712127685546875
Processing:  43%|████▎     | 20/47 [05:55<10:50, 24.09s/it]                                                           2024-01-15 02:16:27 [ladder:INFO]: Tuning ['sum_13', 'divide_cast_cast_reshape_14']
Processing:  43%|████▎     | 20/47 [05:55<10:50, 24.09s/it]@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 32768;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 4) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 1024)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 1024)*16384)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*4096)) + (k3.outer*1024)) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*1024) + (k3.inner.outer*16)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 1024)*8192) + (floordiv(threadIdx.y, 4)*4096)) + (floormod(blockIdx.x, 1024)*4)) + floormod(threadIdx.y, 4))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16384;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 8) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*64)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 4096)*268435456) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*33554432)) + (floordiv(threadIdx.y_1, 8)*16777216)) + (floormod(blockIdx.x, 4096)*4096)) + (k3.outer*512)) + (floormod(threadIdx.y_1, 8)*64)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*512) + (k3.inner.outer*8)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[(((floordiv(blockIdx.x, 4096)*65536) + (threadIdx.y*4096)) + floormod(blockIdx.x, 4096))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 256)*67108864) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16777216)) + (floormod(blockIdx.x, 256)*65536)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*32768)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 256)*16384) + (floordiv(threadIdx.y, 16)*4096)) + (floormod(blockIdx.x, 256)*16)) + floormod(threadIdx.y, 16))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:16:20 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 512)*67108864) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16777216)) + (floormod(blockIdx.x, 512)*32768)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16384)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 512)*16384) + (floordiv(threadIdx.y, 8)*4096)) + (floormod(blockIdx.x, 512)*8)) + floormod(threadIdx.y, 8))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 4096)*536870912) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*67108864)) + (floordiv(threadIdx.y_1, 8)*16777216)) + (floormod(blockIdx.x, 4096)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[(((floordiv(blockIdx.x, 4096)*131072) + (threadIdx.y*4096)) + floormod(blockIdx.x, 4096))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 128)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 128)*131072)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*32768)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 128)*8192) + (floordiv(threadIdx.y, 32)*4096)) + (floormod(blockIdx.x, 128)*32)) + floormod(threadIdx.y, 32))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:16:20 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 65536;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 2) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((blockIdx.x*16384) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*4096)) + (k3.outer*2048)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 2048)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*2048) + (k3.inner.outer*32)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*4) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 32768;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 4) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 2048)*67108864) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16777216)) + (floormod(blockIdx.x, 2048)*8192)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*4096)) + (k3.outer*1024)) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*1024) + (k3.inner.outer*16)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 2048)*16384) + (floordiv(threadIdx.y, 2)*4096)) + (floormod(blockIdx.x, 2048)*2)) + floormod(threadIdx.y, 2))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:16:20 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((blockIdx.x*131072) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*16384)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*32) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 16) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 4;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*32)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 2048)*268435456) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*33554432)) + (floordiv(threadIdx.y_1, 16)*16777216)) + (floormod(blockIdx.x, 2048)*8192)) + (floordiv(floormod(threadIdx.y_1, 16), 8)*4096)) + (k3.outer*256)) + (floormod(threadIdx.y_1, 8)*32)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*256) + (k3.inner.outer*4)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 2048)*65536) + (floordiv(threadIdx.y, 2)*4096)) + (floormod(blockIdx.x, 2048)*2)) + floormod(threadIdx.y, 2))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:16:20 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16384;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 8) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*64)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 1024)*67108864) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16777216)) + (floormod(blockIdx.x, 1024)*16384)) + (floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*8192)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*512)) + (floormod(threadIdx.y_1, 8)*64)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*512) + (k3.inner.outer*8)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 1024)*16384) + (floordiv(threadIdx.y, 4)*4096)) + (floormod(blockIdx.x, 1024)*4)) + floormod(threadIdx.y, 4))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 65536;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 2) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((floordiv(blockIdx.x, 4096)*67108864) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 2)*16777216)) + (floormod(blockIdx.x, 4096)*4096)) + (k3.outer*2048)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 2048)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*2048) + (k3.inner.outer*32)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[(((floordiv(blockIdx.x, 4096)*16384) + (threadIdx.y*4096)) + floormod(blockIdx.x, 4096))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 32768;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 4) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 8;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((blockIdx.x*32768) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*4096)) + (k3.outer*1024)) + (threadIdx.y_1*128)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*1024) + (k3.inner.outer*16)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*8) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16384;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 8) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*64)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 2048)*134217728) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*16777216)) + (floormod(blockIdx.x, 2048)*8192)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*512)) + (floormod(threadIdx.y_1, 8)*64)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*512) + (k3.inner.outer*8)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 2048)*32768) + (floordiv(threadIdx.y, 2)*4096)) + (floormod(blockIdx.x, 2048)*2)) + floormod(threadIdx.y, 2))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 1024)*268435456) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*33554432)) + (floordiv(threadIdx.y_1, 32)*16777216)) + (floormod(blockIdx.x, 1024)*16384)) + (floordiv(floormod(threadIdx.y_1, 32), 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 1024)*65536) + (floordiv(threadIdx.y, 4)*4096)) + (floormod(blockIdx.x, 1024)*4)) + floormod(threadIdx.y, 4))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 131072;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
      let cse_var_1: int32 = (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024)
      attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64;
      p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp(((cse_var_1 + (threadIdx.y_1*512)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((blockIdx.x*8192) + cse_var_1) + (threadIdx.y_1*512)) + (threadIdx.x_1*8)), 1, 8)]
    }
    for (k3.inner.outer: int32, 0, 64) {
      normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*4096) + (k3.inner.outer*64)) + threadIdx.x)])
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((blockIdx.x*2) + threadIdx.y)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:15:41 [ladder:INFO]: Tir template failed because too many values to unpack (expected 3), fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp((((((((floordiv(blockIdx.x, 512)*134217728) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*16777216)) + (floormod(blockIdx.x, 512)*32768)) + (floordiv(threadIdx.y_1, 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 512)*32768) + (floordiv(threadIdx.y, 8)*4096)) + (floormod(blockIdx.x, 512)*8)) + floormod(threadIdx.y, 8))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:16:20 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((((floordiv(blockIdx.x, 2048)*536870912) + (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*67108864)) + (floordiv(threadIdx.y_1, 16)*16777216)) + (floormod(blockIdx.x, 2048)*8192)) + (floordiv(floormod(threadIdx.y_1, 16), 8)*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 2048)*131072) + (floordiv(threadIdx.y, 2)*4096)) + (floormod(blockIdx.x, 2048)*2)) + floormod(threadIdx.y, 2))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:16:20 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 16)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 16 * 256 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_fused_0_0 in T.unroll(16):
                        for ax0_ax1_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 16)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 16 * 256 + (ax0_ax1_fused_0_0 * 1024 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_fused_0_0 * 1024 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 16)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 16 * 256 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 16)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 16 * 256 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 32)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 32)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 4 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 2048)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 2048 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 32)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 32)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 32)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 32)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 16)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 16)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 8 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 1024)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 1024 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 16)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 16)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 16)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 16)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 65536;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 2) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((floordiv(blockIdx.x, 2048)*33554432) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4)*16777216)) + (floormod(blockIdx.x, 2048)*8192)) + (floordiv(floormod(ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer, 4), 2)*4096)) + (k3.outer*2048)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*256)) + (threadIdx.x_1*8)), 2048)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*2048) + (k3.inner.outer*32)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((((floordiv(blockIdx.x, 2048)*8192) + (floordiv(threadIdx.y, 2)*4096)) + (floormod(blockIdx.x, 2048)*2)) + floormod(threadIdx.y, 2))] = reduce_temp0_1[0]
  }
}


2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 8 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 16)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 16 * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(16):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 8 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 2048)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 2048 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 8 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 16)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 16 * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 128 * 8 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 16)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 128 * 32 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 16 * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 128, 1], 'thread': [1, 1, 128, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: 0.14499840140342712
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 256, 1], 'thread': [1, 1, 128, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: 0.07884799689054489
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 128, 1], 'thread': [1, 2, 64, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: 0.08396799862384796
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 2, 64, 1], 'thread': [1, 2, 64, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: 0.1584639996290207
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 64, 1], 'thread': [1, 4, 32, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: 0.08934400230646133
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 4, 32, 1], 'thread': [1, 4, 32, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: 0.14694400131702423
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 8, 32, 1], 'thread': [1, 8, 16, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:01<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: 0.08959999680519104
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 8, 16, 1], 'thread': [1, 8, 16, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: 0.15692800283432007
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 16, 16, 1], 'thread': [1, 16, 8, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: 0.14617599546909332
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 16, 8, 1], 'thread': [1, 16, 8, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: 0.2693119943141937
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 32, 8, 1], 'thread': [1, 32, 4, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: 0.27929601073265076
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 32, 4, 1], 'thread': [1, 32, 4, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:34 [ladder:DEBUG]: 0.521727979183197
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 64, 4, 1], 'thread': [1, 64, 2, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:34 [ladder:DEBUG]: 0.5491200089454651
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 64, 2, 1], 'thread': [1, 64, 2, 1], 'rstep': [64], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:34 [ladder:DEBUG]: 1.0342400074005127
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:34 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, sum_13__divide_cast_cast_reshape_14>: {'block': [1, 1, 256, 1], 'thread': [1, 1, 128, 1], 'rstep': [64], 'step': [1, 1, 2, 1], 'vectorize': {'input0': 8}}}
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:34 [ladder:INFO]: result: 0.07884799689054489
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:34 [ladder:INFO]: Tuning ['sum_13', 'divide_cast_cast_reshape_14', 'ladder_perfect_matmul_15', 'layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16', 'nn_batch_matmul_17']
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]                                                           2024-01-15 02:16:34 [ladder:INFO]: Fusion group created: 8 ['sum_13', 'divide_cast_cast_reshape_14']
Processing:  43%|████▎     | 20/47 [06:02<10:50, 24.09s/it]Processing:  47%|████▋     | 22/47 [06:02<08:03, 19.32s/it]                                                           2024-01-15 02:16:34 [ladder:INFO]: Tuning ['ladder_perfect_matmul_15', 'layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16']
Processing:  47%|████▋     | 22/47 [06:02<08:03, 19.32s/it]                                                           2024-01-15 02:16:34 [ladder:INFO]: Tuning ['ladder_perfect_matmul_15', 'layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16']
Processing:  47%|████▋     | 22/47 [06:02<08:03, 19.32s/it]                                                           2024-01-15 02:16:34 [ladder:ERROR]: Failed to get base tile: Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Map<tvm::tir::Var, tvm::PrimExpr, void, void> (tvm::runtime::Array<tvm::arith::IterSumExpr, void> const&, tvm::runtime::Array<tvm::PrimExpr, void>)>::AssignTypedLambda<tvm::runtime::Map<tvm::tir::Var, tvm::PrimExpr, void, void> (*)(tvm::runtime::Array<tvm::arith::IterSumExpr, void> const&, tvm::runtime::Array<tvm::PrimExpr, void>)>(tvm::runtime::Map<tvm::tir::Var, tvm::PrimExpr, void, void> (*)(tvm::runtime::Array<tvm::arith::IterSumExpr, void> const&, tvm::runtime::Array<tvm::PrimExpr, void>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::arith::InverseAffineIterMap(tvm::runtime::Array<tvm::arith::IterSumExpr, void> const&, tvm::runtime::Array<tvm::PrimExpr, void>)
  0: tvm::arith::InverseAffineIterMapTransformer::operator()(tvm::runtime::Array<tvm::arith::IterSumExpr, void> const&, tvm::runtime::Array<tvm::PrimExpr, void> const&)
  File "/home/t-leiwang/ladder_workspace/LadderTVM/src/arith/iter_affine_map.cc", line 2128
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (iter_map.size() == outputs.size()) is false: 
Processing:  47%|████▋     | 22/47 [06:02<08:03, 19.32s/it]                                                           2024-01-15 02:16:34 [ladder:INFO]: Tuning ['ladder_perfect_matmul_15']
Processing:  47%|████▋     | 22/47 [06:02<08:03, 19.32s/it]                                                           2024-01-15 02:16:34 [ladder:INFO]: Fusion group created: 9 ['ladder_perfect_matmul_15']
Processing:  47%|████▋     | 22/47 [06:02<08:03, 19.32s/it]                                                           2024-01-15 02:16:34 [ladder:INFO]: Tuning ['layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16', 'nn_batch_matmul_17']
Processing:  47%|████▋     | 22/47 [06:02<08:03, 19.32s/it]                                                           2024-01-15 02:16:34 [ladder:INFO]: Tuning ['layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16']
Processing:  47%|████▋     | 22/47 [06:02<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [8, 16, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.16204799711704254
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [8, 32, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.16921600699424744
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [8, 16, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.13439999520778656
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [8, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.18247678875923157
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [8, 32, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.17858560383319855
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [8, 64, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.19066879153251648
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [8, 128, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.22732798755168915
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [8, 16, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.17740799486637115
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [8, 32, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.179967999458313
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [8, 64, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.21196800470352173
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [16, 64, 16], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.2385919988155365
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 16, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.11801599711179733
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 32, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.11980800330638885
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 64, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.14131200313568115
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 16, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.10726399719715118
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 32, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.1090560033917427
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 16, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.1011200025677681
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 16, 256], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.15646719932556152
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 32, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:20<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.15482880175113678
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 64, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.15278080105781555
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 128, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: 0.2113535851240158
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:52 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.137472003698349
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 32, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.13260799646377563
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 64, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.15078400075435638
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [4, 128, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.20541438460350037
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [16, 32, 32], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.18472960591316223
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [16, 32, 16], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.20659199357032776
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [32, 32, 16], 'thread': [4, 4, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.2385919988155365
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.11084800213575363
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 32, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.10649599879980087
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 64, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.11084800213575363
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 128, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.14847999811172485
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 16, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.10521599650382996
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 32, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.1016319990158081
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 64, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.10214400291442871
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 16, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.10035199671983719
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 32, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.10035199671983719
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 16, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.09446399658918381
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [16, 16, 16], 'thread': [2, 8, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.1871359944343567
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 16, 512], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: 0.1611776053905487
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16>: {'block': [2, 16, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:INFO]: result: 0.09446399658918381
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]                                                           2024-01-15 02:16:53 [ladder:INFO]: Fusion group created: 10 ['layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16']
Processing:  47%|████▋     | 22/47 [06:21<08:03, 19.32s/it]Processing:  55%|█████▌    | 26/47 [06:21<04:29, 12.85s/it]                                                           2024-01-15 02:16:53 [ladder:INFO]: Tuning ['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
Processing:  55%|█████▌    | 26/47 [06:21<04:29, 12.85s/it]                                                           2024-01-15 02:17:00 [ladder:INFO]: Tuning ['nn_batch_matmul_17', 'reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
Processing:  55%|█████▌    | 26/47 [06:29<04:29, 12.85s/it]2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32 * 2 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 64)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 64 * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(16):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32 * 2 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 8192)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 8192 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32 * 2 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 64)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 64 * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32 * 2 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 64)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 64 * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 8)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 8)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 16 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 512)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 512 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 8)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 8)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 16 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 8)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 8)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(4096, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(32, 64):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 64 * 64 + i_1_j_1_fused // 2 * 32 + i_2_init)
                        v_j = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 64 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                                        v1 = T.axis.spatial(128, (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(32, 64, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 64 * 64 + i_1_j_1_fused // 2 * 32 + i_2)
                            v_j = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(32, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 64 * 64 + i_1_j_1_fused // 2 * 32 + (ax1_0 * 2 + ax1_1) % 8 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + (ax1_0 * 2 + ax1_1) // 8 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(64, 64):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 32)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 32 * 128 + i_1_j_1_fused // 2 * 64 + i_2_init)
                        v_j = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 32)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 32 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 32)
                                        v1 = T.axis.spatial(128, (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(64, 64, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 32)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 32 * 128 + i_1_j_1_fused // 2 * 64 + i_2)
                            v_j = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(64, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 32)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 32 * 128 + i_1_j_1_fused // 2 * 64 + (ax1_0 * 2 + ax1_1) % 16 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + (ax1_0 * 2 + ax1_1) // 16 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_1_ax1_1_ax2_1_ax3_1_fused // 2)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 2)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 128)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 2 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 128 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_1_ax1_1_ax2_1_ax3_1_fused // 2)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 2)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_1_ax1_1_ax2_1_ax3_1_fused // 2)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 2)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(128, 64):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 16)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 16 * 256 + i_1_j_1_fused // 2 * 128 + i_2_init)
                        v_j = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 16)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 16 * 256 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 16)
                                        v1 = T.axis.spatial(128, (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(128, 64, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 16)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 16 * 256 + i_1_j_1_fused // 2 * 128 + i_2)
                            v_j = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(128, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 16)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 16 * 256 + i_1_j_1_fused // 2 * 128 + (ax1_0 * 2 + ax1_1) % 32 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + (ax1_0 * 2 + ax1_1) // 32 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 8)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 8 * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(16):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 16 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 1024)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 1024 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 8)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 8 * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 8)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 256 * 16 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 8 * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(8192, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(16, 64):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 128 * 32 + i_1_j_1_fused // 2 * 16 + i_2_init)
                        v_j = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 128 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                        v1 = T.axis.spatial(128, (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(16, 64, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 128 * 32 + i_1_j_1_fused // 2 * 16 + i_2)
                            v_j = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(16, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 128 * 32 + i_1_j_1_fused // 2 * 16 + (ax1_0 * 2 + ax1_1) % 4 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, i_1_j_1_fused % 2 * 64 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.@main = primfn(p0_1: handle, p0_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {p0: Buffer(p0_2: Pointer(float16), float16, [1, 64, 4096, 4096], []),
             p0_red: Buffer(p0_red_2: Pointer(float16), float16, [1, 64, 4096, 1], [])}
  buffer_map = {p0_1: p0, p0_red_1: p0_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4096;
  allocate(normal_reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  allocate(p0.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;
  allocate(reduce_temp0: Pointer(local float16), float16, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
  attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2 {
    normal_reduce_temp0_1: Buffer(normal_reduce_temp0, float16, [1], [], scope="local")[0] = 0f16
    for (k3.outer: int32, 0, 32) {
      for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 8) {
        attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 64;
        attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 2;
        p0.shared_1: Buffer(p0.shared, float16, [8192], [], scope="shared")[ramp((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*1024) + (threadIdx.y_1*16)) + (threadIdx.x_1*8)), 1, 8)] = p0_3: Buffer(p0_2, float16, [1073741824], [])[ramp(((((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*134217728) + (floordiv(threadIdx.y_1, 8)*16777216)) + (blockIdx.x*4096)) + (k3.outer*128)) + (floormod(threadIdx.y_1, 8)*16)) + (threadIdx.x_1*8)), 1, 8)]
      }
      for (k3.inner.outer: int32, 0, 64) {
        normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + p0.shared_1[(((threadIdx.y*128) + (k3.inner.outer*2)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float16, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    p0_red_3: Buffer(p0_red_2, float16, [262144], [])[((threadIdx.y*4096) + blockIdx.x)] = reduce_temp0_1[0]
  }
}


2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 64)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 64)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 2 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 4096)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 4096 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 64)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 64)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 2 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 64)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 64)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(16384, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(16, 32):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 256 * 16 + i_2_init)
                        v_j = T.axis.spatial(128, i_1_j_1_fused * 32 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 2)
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 256 * 16 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                        v1 = T.axis.spatial(128, (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(16, 32, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 256 * 16 + i_2)
                            v_j = T.axis.spatial(128, i_1_j_1_fused * 32 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 256 * 16 + (ax1_0 * 2 + ax1_1) % 4 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, i_1_j_1_fused * 32 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 32 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 4)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 4 * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(16):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 32 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 512)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 512 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 32 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 4)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 4 * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 512 * 32 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 4)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 512 * 8 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 4 * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(4096, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(64, 32):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 64 // 2 * 128 + i_1_j_1_fused // 2 * 64 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 64 // 2 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(64, 32, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 64 // 2 * 128 + i_1_j_1_fused // 2 * 64 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(32, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 64 // 2 * 128 + i_1_j_1_fused // 2 * 64 + (ax1_0 * 2 + ax1_1) % 16 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + (ax1_0 * 2 + ax1_1) // 16 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 1024 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 4)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 1024 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 4)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(8):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 1024 * 32 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 256)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 1024 * 4 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 256 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 1024 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 4)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 1024 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 4)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 1024 * 32 + ax0_1_ax1_1_ax2_1_ax3_1_fused // 4)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 1024 * 4 + ax0_1_ax1_1_ax2_1_ax3_1_fused % 4)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(16384, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(16, 32):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 2 * 32 + i_1_j_1_fused // 2 * 16 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 2 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(16, 32, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 2 * 32 + i_1_j_1_fused // 2 * 16 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 2 * 32 + i_1_j_1_fused // 2 * 16 + (ax1_0 * 2 + ax1_1) % 4 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 2)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 4 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 2 * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in 2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                with T.block("mediate0_init"):
                    v_ax0 = T.axis.spatial(1, 0)
                    v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32)
                    v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_ax3_1_fused)
                    v_ax3 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                    mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_fused_0_0 in T.unroll(8):
                        for ax0_ax1_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + (ax0_ax1_fused_0_0 * 1024 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_fused_0_0 * 1024 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        with T.block("mediate0_update"):
                            v_ax0 = T.axis.spatial(1, 0)
                            v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32)
                            v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_ax3_1_fused)
                            v_ax3 = T.axis.spatial(1, 0)
                            v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                            T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                            T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                            mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                with T.block("mediate0_local"):
                    v0 = T.axis.spatial(1, 0)
                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 32)
                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 32 * 128 + ax0_1_ax1_1_ax2_1_ax3_1_fused)
                    v3 = T.axis.spatial(1, 0)
                    T.reads(mediate0_local[v0, v1, v2, v3])
                    T.writes(mediate0[v0, v1, v2, v3])
                    mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:17:00 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
2024-01-15 02:16:28 [ladder:INFO]: Tir template failed because Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(1, 64, 4096, 4096), "float16"], input1: T.Buffer[(1, 64, 4096, 4096), "float16"], output0: T.Buffer[(64, 4096, 4096), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([1, 64, 4096, 1], dtype="float16")
        mediate0_local = T.alloc_buffer([1, 64, 4096, 1], dtype="float16", scope="local")
        input0_shared = T.alloc_buffer([1, 64, 4096, 4096], dtype="float16", scope="shared")
        for ax0_0_ax1_0_ax2_0_ax3_0_fused in T.thread_binding(1024, thread="blockIdx.x"):
            for ax0_1_ax1_1_ax2_1_0_ax3_1_fused in T.thread_binding(128, thread="threadIdx.x"):
                for ax2_1_1_init in T.unroll(2):
                    with T.block("mediate0_init"):
                        v_ax0 = T.axis.spatial(1, 0)
                        v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 4 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 32)
                        v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 32 * 2 + ax2_1_1_init)
                        v_ax3 = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                        mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = T.float16(0)
                for k3_0 in T.serial(64):
                    for ax0_ax1_ax2_fused_0_0 in T.unroll(16):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 4 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 4096)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 4096 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 4 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 32)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 32 * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_0_ax1_0_ax2_0_ax3_0_fused // 64 * 4 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 32)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused % 64 * 64 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 32 * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
2024-01-15 02:16:28 [ladder:ERROR]: Fail to create schedule for <Node, sum_13__divide_cast_cast_reshape_14>, the error is Schedule not implemented
T.unroll(16):
                        for ax0_ax1_ax2_fused_0_1 in T.thread_binding(128, thread="threadIdx.x"):
                            for ax0_ax1_ax2_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                with T.block("input0_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) // 256)
                                    v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 4 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 256 // 64)
                                    v3 = T.axis.spatial(4096, k3_0 * 64 + (ax0_ax1_ax2_fused_0_0 * 1024 + ax0_ax1_ax2_fused_0_1 * 8 + ax0_ax1_ax2_fused_1) % 64)
                                    T.reads(input0[v0, v1, v2, v3])
                                    T.writes(input0_shared[v0, v1, v2, v3])
                                    input0_shared[v0, v1, v2, v3] = input0[v0, v1, v2, v3]
                    for k3_1 in T.serial(64):
                        for ax2_1_1 in T.unroll(2):
                            with T.block("mediate0_update"):
                                v_ax0 = T.axis.spatial(1, 0)
                                v_ax1 = T.axis.spatial(64, ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 2)
                                v_ax2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 4 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 2 * 2 + ax2_1_1)
                                v_ax3 = T.axis.spatial(1, 0)
                                v_k3 = T.axis.reduce(4096, k3_0 * 64 + k3_1)
                                T.reads(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3], input0_shared[v_ax0, v_ax1, v_ax2, v_k3])
                                T.writes(mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3])
                                mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] = mediate0_local[v_ax0, v_ax1, v_ax2, v_ax3] + input0_shared[v_ax0, v_ax1, v_ax2, v_k3]
                for ax0 in T.unroll(2):
                    with T.block("mediate0_local"):
                        v0 = T.axis.spatial(1, 0)
                        v1 = T.axis.spatial(64, ax0_1_ax1_1_ax2_1_0_ax3_1_fused // 2)
                        v2 = T.axis.spatial(4096, ax0_0_ax1_0_ax2_0_ax3_0_fused * 4 + ax0_1_ax1_1_ax2_1_0_ax3_1_fused % 2 * 2 + ax0)
                        v3 = T.axis.spatial(1, 0)
                        T.reads(mediate0_local[v0, v1, v2, v3])
                        T.writes(mediate0[v0, v1, v2, v3])
                        mediate0[v0, v1, v2, v3] = mediate0_local[v0, v1, v2, v3]
        for ax0, ax1, ax2 in T.grid(64, 4096, 4096):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096], mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0])
                T.writes(output0[v_ax0, v_ax1, v_ax2])
                output0[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T.Cast("float32", input1[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, v_ax2 % 4096] / mediate0[0, ((v_ax2 // 4096 + v_ax1) // 4096 + v_ax0) % 64, (v_ax2 // 4096 + v_ax1) % 4096, 0]))
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block., fallback to te
2024-01-15 02:17:00 [ladder:INFO]: Tir template failed because Undivisible block in TIR schedule is still buggy., fallback to te
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(2048, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(128, 32):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 32)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 32 // 2 * 256 + i_1_j_1_fused // 2 * 128 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 32)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 32 // 2 * 256 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                    2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(8192, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(32, 32):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 2 * 64 + i_1_j_1_fused // 2 * 32 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 2 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(32, 32, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 2 * 64 + i_1_j_1_fused // 2 * 32 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(16, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 2 * 64 + i_1_j_1_fused // 2 * 32 + (ax1_0 * 2 + ax1_1) % 8 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + (ax1_0 * 2 + ax1_1) // 8 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192                     T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 32)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(128, 32, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 32)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 32 // 2 * 256 + i_1_j_1_fused // 2 * 128 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(64, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 32)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 32 // 2 * 256 + i_1_j_1_fused // 2 * 128 + (ax1_0 * 2 + ax1_1) % 32 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused % 2 * 32 + (ax1_0 * 2 + ax1_1) // 32 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(8192, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(64, 16):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 4 * 128 + i_1_j_1_fused // 2 * 64 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 4 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(64, 16, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 4 * 128 + i_1_j_1_fused // 2 * 64 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(16, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 4 * 128 + i_1_j_1_fused // 2 * 64 + (ax1_0 * 2 + ax1_1) % 16 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + (ax1_0 * 2 + ax1_1) // 16 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) %// 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block. 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(32768, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(16, 16):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 2 * 16 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused * 16 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 2)
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 2 * 16 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(16, 16, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 2 * 16 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused * 16 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 2 * 16 + (ax1_0 * 2 + ax1_1) % 4 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 2 * 64 + i_1_j_1_fused * 16 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(16384, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(32, 16):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 4 * 64 + i_1_j_1_fused // 2 * 32 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 4 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(32, 16, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 4 * 64 + i_1_j_1_fused // 2 * 32 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 4 * 64 + i_1_j_1_fused // 2 * 32 + (ax1_0 * 2 + ax1_1) % 8 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + (ax1_0 * 2 + ax1_1) // 8 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(32768, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(16, 16):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 4 * 32 + i_1_j_1_fused // 2 * 16 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 4 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(16, 16, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 4 * 32 + i_1_j_1_fused // 2 * 16 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 4 * 32 + i_1_j_1_fused // 2 * 16 + (ax1_0 * 2 + ax1_1) % 4 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)

2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(4096, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(128, 16):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 64 // 4 * 256 + i_1_j_1_fused // 2 * 128 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 64 // 4 * 256 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) 

2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(131072, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(2, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(16, 8):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 2048)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 2048 // 8 * 16 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused * 8 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(2, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 2048)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 2048 // 8 * 16 + (ax0_ax1_fused_0_0_0 * 512 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 512 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(2, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 2048)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + (ax0_ax1_fused_0_0_0 * 512 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 512 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        T.block_attr({"buffer_dim_align":[[0, 1, 39, 40]]})
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(16, 8, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 2048)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 2048 // 8 * 16 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused * 8 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 2048)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 2048 // 8 * 16 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused * 8 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm:
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(32768, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(32, 8):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 16 * 128 + i_1_j_1_fused * 32 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 16 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 1)
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                       1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(65536, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(16, 8):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 4 * 16 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused * 8 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 2)
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 4 * 16 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        T.block_attr({"buffer_dim_align":[[0, 1, 39, 40]]})
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(16, 8, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 4 * 16 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused * 8 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 4 * 16 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused * 8 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.% 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(128, 16, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 64 // 4 * 256 + i_1_j_1_fused // 2 * 128 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(32, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 64)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 64 // 4 * 256 + i_1_j_1_fused // 2 * 128 + (ax1_0 * 2 + ax1_1) % 32 // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 4 * 32 + i_1_j_1_fused % 2 * 16 + (ax1_0 * 2 + ax1_1) // 32 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.

  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(32768, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(32, 8):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 8 * 64 + i_1_j_1_fused // 2 * 32 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 8 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 2)
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        T.block_attr({"buffer_dim_align":[[0, 1, 39, 40]]})
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(32, 8, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 8 * 64 + i_1_j_1_fused // 2 * 32 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 8 * 64 + i_1_j_1_fused // 2 * 32 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + (ax1_0 * 2 + ax1_1) // 8 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(65536, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(16, 8):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 8 * 32 + i_1_j_1_fused // 2 * 16 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 8 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 2)
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        T.block_attr({"buffer_dim_align":[[0, 1, 39, 40]]})
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(16, 8, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 8 * 32 + i_1_j_1_fused // 2 * 16 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 8 * 32 + i_1_j_1_fused // 2 * 16 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.:tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(8192, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(128, 8):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 8 * 256 + i_1_j_1_fused // 2 * 128 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 8 * 256 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 2)
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        T.block_attr({"buffer_dim_align":[[0, 1, 39, 40]]})
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(128, 8, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 8 * 256 + i_1_j_1_fused // 2 * 128 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(16, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 128)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 128 // 8 * 256 + i_1_j_1_fused // 2 * 128 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + (ax1_0 * 2 + ax1_1) // 32 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.                   T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        T.block_attr({"buffer_dim_align":[[0, 1, 39, 40]]})
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(32, 8, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 16 * 128 + i_1_j_1_fused * 32 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 512)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 512 // 16 * 128 + i_1_j_1_fused * 32 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + (ax1_0 * 2 + ax1_1) // 8 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tv2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(16384, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(64, 8):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 8 * 128 + i_1_j_1_fused // 2 * 64 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(4, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 8 * 128 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 2)
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        T.block_attr({"buffer_dim_align":[[0, 1, 39, 40]]})
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(64, 8, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 8 * 128 + i_1_j_1_fused // 2 * 64 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 8 * 128 + i_1_j_1_fused // 2 * 64 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 8 * 16 + i_1_j_1_fused % 2 * 8 + (ax1_0 * 2 + ax1_1) // 16 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.m::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(65536, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(16, 8):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 16 * 64 + i_1_j_1_fused * 16 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 16 * 64 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 1)
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        T.block_attr({"buffer_dim_align":[[0, 1, 39, 40]]})
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(16, 8, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 16 * 64 + i_1_j_1_fused * 16 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 1024)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 1024 // 16 * 64 + i_1_j_1_fused * 16 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<void (tvm::tir::Schedule, tvm::tir::BlockRV const&)>::AssignTypedLambda<tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}>(tvm::runtime::Registry::set_body_method<tvm::tir::Schedule, tvm::tir::ScheduleNode, void, tvm::tir::BlockRV const&, void>(void (tvm::tir::ScheduleNode::*)(tvm::tir::BlockRV const&))::{lambda(tvm::tir::Schedule, tvm::tir::BlockRV const&)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  1: t: tvm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(16384, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(4, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(64, 8):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 16 * 256 + i_1_j_1_fused * 64 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 16 * 256 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 1024 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(4, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1 < 1)
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + ((ax0_ax1_fused_0_0_0 * 4 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        T.block_attr({"buffer_dim_align":[[0, 1, 39, 40]]})
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(64, 8, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 16 * 256 + i_1_j_1_fused * 64 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(8, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 256)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 256 // 16 * 256 + i_1_j_1_fused * 64 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + (ax1_0 * 2 + ax1_1) // 16 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.vm::tir::TracedScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&)
  0: tvm::tir::ConcreteScheduleNode::ReverseComputeInline(tvm::tir::BlockRV const&) [clone .cold]
ScheduleError: An error occurred in the schedule primitive 'reverse-compute-inline'.
The IR with diagnostic is:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(input0: T.Buffer[(64, 4096, 4096), "float16"], input1: T.Buffer[(64, 128, 4096), "float16"], output0: T.Buffer[(256, 512, 16, 16), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        mediate0 = T.alloc_buffer([64, 4096, 128], dtype="float16")
        input0_shared = T.alloc_buffer([64, 4096, 4096], dtype="float16", scope="shared")
        input1_shared = T.alloc_buffer([64, 128, 4096], dtype="float16", scope="shared")
        mediate0_cutlass_warp_mma = T.alloc_buffer([64, 4096, 128], dtype="float16", scope="cutlass.warp.mma")
        for b_i_0_j_0_fused in T.thread_binding(131072, thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(2, thread="threadIdx.y"):
                for i_2_init, j_2_init in T.grid(16, 8):
                    with T.block("mediate0_init"):
                        v_b = T.axis.spatial(64, b_i_0_j_0_fused // 2048)
                        v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 2048 // 16 * 32 + i_1_j_1_fused * 16 + i_2_init)
                        v_j = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + j_2_init)
                        T.reads()
                        T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                        mediate0_cutlass_warp_mma[v_b, v_i, v_j] = T.float16(0)
                for k_0 in T.serial(128):
                    for ax0_ax1_fused_0_0_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(2, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input0_shared"):
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 2048)
                                        v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 2048 // 16 * 32 + (ax0_ax1_fused_0_0_0 * 512 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + (ax0_ax1_fused_0_0_0 * 512 + ax0_ax1_fused_0_0_1 * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input0[v0, v1, v2])
                                        T.writes(input0_shared[v0, v1, v2])
                                        input0_shared[v0, v1, v2] = input0[v0, v1, v2]
                    for ax0_ax1_fused_0_0_0 in T.unroll(1, annotations={"pragma_unroll_explicit":0}):
                        for ax0_ax1_fused_0_0_1 in T.thread_binding(2, thread="threadIdx.y"):
                            for ax0_ax1_fused_0_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_1 in T.vectorized(8, annotations={"check_vector_load":1, "remove_vector_condition":1}):
                                    with T.block("input1_shared"):
                                        T.where(ax0_ax1_fused_0_0_0 * 2 + ax0_ax1_fused_0_0_1 < 1)
                                        v0 = T.axis.spatial(64, b_i_0_j_0_fused // 2048)
                                        v1 = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + ((ax0_ax1_fused_0_0_0 * 2 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) // 32)
                                        v2 = T.axis.spatial(4096, k_0 * 32 + ((ax0_ax1_fused_0_0_0 * 2 + ax0_ax1_fused_0_0_1) * 256 + ax0_ax1_fused_0_1 * 8 + ax0_ax1_fused_1) % 32)
                                        T.reads(input1[v0, v1, v2])
                                        T.writes(input1_shared[v0, v1, v2])
                                        T.block_attr({"buffer_dim_align":[[0, 1, 39, 40]]})
                                        input1_shared[v0, v1, v2] = input1[v0, v1, v2]
                    for i_2, j_2, k_1 in T.grid(16, 8, 32):
                        with T.block("mediate0_update"):
                            v_b = T.axis.spatial(64, b_i_0_j_0_fused // 2048)
                            v_i = T.axis.spatial(4096, b_i_0_j_0_fused % 2048 // 16 * 32 + i_1_j_1_fused * 16 + i_2)
                            v_j = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + j_2)
                            v_k = T.axis.reduce(4096, k_0 * 32 + k_1)
                            T.reads(mediate0_cutlass_warp_mma[v_b, v_i, v_j], input0_shared[v_b, v_i, v_k], input1_shared[v_b, v_j, v_k])
                            T.writes(mediate0_cutlass_warp_mma[v_b, v_i, v_j])
                            mediate0_cutlass_warp_mma[v_b, v_i, v_j] = mediate0_cutlass_warp_mma[v_b, v_i, v_j] + input0_shared[v_b, v_i, v_k] * input1_shared[v_b, v_j, v_k]
                for ax0 in T.thread_binding(32, thread="threadIdx.x"):
                    for ax1_0 in T.unroll(2, annotations={"pragma_unroll_explicit":0}):
                        for ax1_1 in T.vectorized(2):
                            with T.block("mediate0_cutlass.warp.mma"):
                                v0 = T.axis.spatial(64, b_i_0_j_0_fused // 2048)
                                v1 = T.axis.spatial(4096, b_i_0_j_0_fused % 2048 // 16 * 32 + i_1_j_1_fused * 16 + (ax1_0 * 2 + ax1_1) // 2 * 8 + ax0 // 4)
                                v2 = T.axis.spatial(128, b_i_0_j_0_fused % 16 * 8 + (ax1_0 * 2 + ax1_1) // 4 * 8 + ax0 % 4 * 2 + (ax1_0 * 2 + ax1_1) % 2)
                                T.reads(mediate0_cutlass_warp_mma[v0, v1, v2])
                                T.writes(mediate0[v0, v1, v2])
                                mediate0[v0, v1, v2] = mediate0_cutlass_warp_mma[v0, v1, v2]
        for i0, i1, i2, i3 in T.grid(256, 512, 16, 16):
            # tir.Block#0
            with T.block("output0"):
            ^^^^^^^^^^^^^^^^^^^^^^^^
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(mediate0[(v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 8192 // 128, (v_i0 * 16 + (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) // 8192 + (v_i2 * 2 + v_i3 // 8) % 16) % 4096, (v_i1 * 16 + (v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) % 128])
                T.writes(output0[v_i0, v_i1, v_i2, v_i3])
                output0[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16 < 4096 and v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8) < 8192, mediate0[(((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) // 4096 + (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 8192 // 128) % 64, ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 // 128 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 // 8192 + ((v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) // 8192 + (v_i0 * 16 + (v_i2 * 2 + v_i3 // 8) % 16)) % 4096) % 4096) % 4096, (v_i1 * 16 + ((v_i2 * 2 + v_i3 // 8) // 16 * 8 + v_i3 % 8)) % 8192 % 128 % 128], T.float16(0), dtype="float16")
    
Error message: The body of the inlined block should be in form of
    `B[...] = g(i, j, k, A[f(i, j, k, ...)] ...)`,
where A is the only buffer the block consumes, whose indices are distinct atomic variables,
and there should be no variables other than the index variables), and f is a bijective affine
mapping and there should not be predicates in the inlined block. The iter domains of the inlined
block should be covered by the producer block.                                                           2024-01-15 02:17:03 [ladder:INFO]: Tuning ['nn_batch_matmul_17']
Processing:  55%|█████▌    | 26/47 [06:31<04:29, 12.85s/it]                                                           2024-01-15 02:17:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 128], 'warp': [1, 64, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  55%|█████▌    | 26/47 [06:48<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: 1.5415295362472534
Processing:  55%|█████▌    | 26/47 [06:48<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 128], 'warp': [1, 32, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  55%|█████▌    | 26/47 [06:48<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: 1.638604760169983
Processing:  55%|█████▌    | 26/47 [06:48<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 64], 'warp': [1, 64, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  55%|█████▌    | 26/47 [06:48<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: 1.6764929294586182
Processing:  55%|█████▌    | 26/47 [06:48<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 128], 'warp': [1, 128, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  55%|█████▌    | 26/47 [06:48<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: 1.5065088272094727
Processing:  55%|█████▌    | 26/47 [06:48<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 64], 'warp': [1, 32, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  55%|█████▌    | 26/47 [06:48<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: 1.876582384109497
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 64, 64], 'warp': [2, 32, 32], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: 3.3589248657226562
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:20 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 128, 128], 'warp': [2, 64, 64], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: 2.4678399562835693
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 64, 128], 'warp': [2, 32, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: 2.711961507797241
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 128, 64], 'warp': [2, 64, 32], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: 2.543001651763916
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 128], 'warp': [1, 16, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: 2.2286336421966553
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 32], 'warp': [1, 64, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: 2.208768129348755
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 64], 'warp': [1, 128, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: 1.6592895984649658
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 32, 64], 'warp': [2, 16, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: 5.35982084274292
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 64, 32], 'warp': [2, 32, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: 5.241036891937256
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 64], 'warp': [1, 16, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  55%|█████▌    | 26/47 [06:49<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: 2.620211124420166
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 32], 'warp': [1, 32, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: 2.552422285079956
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:21 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 32, 128], 'warp': [2, 16, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: 3.812556743621826
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 128, 32], 'warp': [2, 64, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: 3.812351942062378
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [4, 32, 32], 'warp': [4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: 6.613811492919922
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 32, 32], 'warp': [2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: 5.755289554595947
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 32], 'warp': [1, 16, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: 3.3529858589172363
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [4, 32, 64], 'warp': [4, 16, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: 5.60066556930542
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [4, 64, 32], 'warp': [4, 32, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: 5.361868858337402
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [4, 64, 64], 'warp': [4, 32, 32], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: 3.7498879432678223
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 16, 128], 'warp': [2, 8, 64], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  55%|█████▌    | 26/47 [06:50<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: 7.745535850524902
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 128, 16], 'warp': [2, 64, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: 7.545651435852051
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 16, 128], 'warp': [1, 16, 32], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: 3.9012351036071777
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:22 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 128, 16], 'warp': [1, 64, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: 3.867647886276245
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 32], 'warp': [1, 128, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: 2.0594687461853027
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 256, 32], 'warp': [2, 128, 16], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: 4.3522047996521
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 16, 64], 'warp': [2, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: 7.2867841720581055
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 64, 16], 'warp': [2, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: 7.328153133392334
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 16, 64], 'warp': [1, 16, 16], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 72>}}}
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: 4.316774368286133
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 64, 16], 'warp': [1, 32, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: 4.588953971862793
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [4, 16, 32], 'warp': [2, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: 9.46565055847168
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [4, 32, 16], 'warp': [2, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  55%|█████▌    | 26/47 [06:51<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: 9.24794864654541
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 16, 32], 'warp': [1, 8, 32], 'wmma': [8, 32, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: 8.413389205932617
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]                                                           2024-01-15 02:17:23 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [2, 32, 16], 'warp': [1, 32, 8], 'wmma': [32, 8, 16], 'use_cutlass': False, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]                                                           2024-01-15 02:17:24 [ladder:DEBUG]: 8.418508529663086
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]                                                           2024-01-15 02:17:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 16, 32], 'warp': [1, 16, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 40>}}}
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]                                                           2024-01-15 02:17:24 [ladder:DEBUG]: 7.229849338531494
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]                                                           2024-01-15 02:17:24 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 32, 16], 'warp': [1, 16, 8], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 24>}}}
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]                                                           2024-01-15 02:17:24 [ladder:DEBUG]: 6.012928009033203
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]                                                           2024-01-15 02:17:24 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, nn_batch_matmul_17>: {'block': [1, 256, 128], 'warp': [1, 128, 64], 'wmma': [16, 8, 16], 'use_cutlass': True, 'rstep': [32], 'use_tc': '80', 'strides': {2: <Stride, 1, 136>}}}
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]                                                           2024-01-15 02:17:24 [ladder:INFO]: result: 1.5065088272094727
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]                                                           2024-01-15 02:17:24 [ladder:INFO]: Fusion group created: 11 ['nn_batch_matmul_17']
Processing:  55%|█████▌    | 26/47 [06:52<04:29, 12.85s/it]Processing:  57%|█████▋    | 27/47 [06:52<05:05, 15.29s/it]                                                           2024-01-15 02:17:24 [ladder:INFO]: Tuning ['reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18', 'ladder_perfect_matmul_19']
Processing:  57%|█████▋    | 27/47 [06:52<05:05, 15.29s/it]                                                           2024-01-15 02:17:24 [ladder:INFO]: Tuning ['reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
Processing:  57%|█████▋    | 27/47 [06:52<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 16, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: 0.10675200074911118
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 8, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: 0.10214400291442871
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 4, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: 0.10726399719715118
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: 0.09164799749851227
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 4, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: 0.09318400174379349
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 4, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: 0.09344000369310379
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 64, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: 0.19681279361248016
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 32, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:09<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: 0.1759231984615326
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 16, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.17674240469932556
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 8, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.17715200781822205
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [16, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.1988607943058014
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 32, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.1525759994983673
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 16, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.12083200365304947
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 8, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.12083200365304947
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.14602240920066833
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 8, 1, 16], 'thread': [1, 8, 1, 16], 'rstep': []}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.19788800179958344
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 8, 8, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.11315199732780457
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 8, 8, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.11161600053310394
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 8, 8, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.09471999853849411
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 8, 4, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.08806400001049042
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 8, 2, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.10188800096511841
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 4, 2, 16], 'thread': [1, 4, 2, 16], 'rstep': []}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.19788800179958344
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 16, 8, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:10<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.1142783984541893
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 32, 8, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.11494400352239609
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 64, 4, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: 0.11084800213575363
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 128, 2, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.11161600053310394
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 4, 8, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.11929599940776825
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 256, 1, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.14602240920066833
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [8, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.1016319990158081
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 16, 8, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.11238399893045425
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 32, 4, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.09651199728250504
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 64, 2, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.10803200304508209
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 4, 8, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.11494400352239609
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 128, 1, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.11622399836778641
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [4, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.09471999853849411
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 16, 4, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.10214400291442871
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 32, 2, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.09113600105047226
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 4, 8, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.08908800035715103
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 64, 1, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.1157120019197464
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [2, 2, 16, 16], 'thread': [2, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:11<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: 0.09338880330324173
Processing:  57%|█████▋    | 27/47 [07:12<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>: {'block': [1, 8, 4, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  57%|█████▋    | 27/47 [07:12<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:INFO]: result: 0.08806400001049042
Processing:  57%|█████▋    | 27/47 [07:12<05:05, 15.29s/it]                                                           2024-01-15 02:17:43 [ladder:INFO]: Fusion group created: 12 ['reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18']
Processing:  57%|█████▋    | 27/47 [07:12<05:05, 15.29s/it]Processing:  60%|█████▉    | 28/47 [07:12<05:04, 16.00s/it]                                                           2024-01-15 02:17:43 [ladder:INFO]: Tuning ['ladder_perfect_matmul_19', 'layout_transform_reshape_reshape_add_20']
Processing:  60%|█████▉    | 28/47 [07:12<05:04, 16.00s/it]                                                           2024-01-15 02:17:44 [ladder:INFO]: Tuning ['ladder_perfect_matmul_19', 'layout_transform_reshape_reshape_add_20']
Processing:  60%|█████▉    | 28/47 [07:13<05:04, 16.00s/it]                                                           2024-01-15 02:18:01 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:29<05:04, 16.00s/it]                                                           2024-01-15 02:18:01 [ladder:DEBUG]: 2.6462206840515137
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:01 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:01 [ladder:DEBUG]: 2.0729856491088867
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:01 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: 2.0457472801208496
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: 2.6298367977142334
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [16, 8, 16, 16], 'warp': [8, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: 2.2669310569763184
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: 4.190003395080566
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: 3.3937408924102783
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: 2.3433215618133545
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: 5.10607385635376
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: 4.1570305824279785
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:30<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: 2.3373823165893555
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: 7.367270469665527
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:02 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: 3.8797314167022705
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: 5.063270568847656
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: 8.280882835388184
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: 6.754918575286865
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: 9.906789779663086
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: 3.849011182785034
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: 7.3156609535217285
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: 7.153663635253906
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: 8.215347290039062
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:31<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: 9.803980827331543
Processing:  60%|█████▉    | 28/47 [07:32<05:04, 16.00s/it]                                                           2024-01-15 02:18:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:32<05:04, 16.00s/it]                                                           2024-01-15 02:18:04 [ladder:DEBUG]: 13.018521308898926
Processing:  60%|█████▉    | 28/47 [07:32<05:04, 16.00s/it]                                                           2024-01-15 02:18:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:32<05:04, 16.00s/it]                                                           2024-01-15 02:18:04 [ladder:DEBUG]: 7.09447717666626
Processing:  60%|█████▉    | 28/47 [07:32<05:04, 16.00s/it]                                                           2024-01-15 02:18:04 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_19__layout_transform_reshape_reshape_add_20>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  60%|█████▉    | 28/47 [07:32<05:04, 16.00s/it]                                                           2024-01-15 02:18:04 [ladder:INFO]: result: 2.0457472801208496
Processing:  60%|█████▉    | 28/47 [07:32<05:04, 16.00s/it]                                                           2024-01-15 02:18:04 [ladder:INFO]: Tuning ['ladder_perfect_matmul_19']
Processing:  60%|█████▉    | 28/47 [07:32<05:04, 16.00s/it]                                                           2024-01-15 02:18:04 [ladder:INFO]: Tuning ['layout_transform_reshape_reshape_add_20']
Processing:  60%|█████▉    | 28/47 [07:32<05:04, 16.00s/it]                                                           2024-01-15 02:18:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 4, 1024], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:17 [ladder:DEBUG]: 0.1558527946472168
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 8, 512], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:17 [ladder:DEBUG]: 0.16896000504493713
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 16, 256], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:17 [ladder:DEBUG]: 0.1773568093776703
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 32, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.17960959672927856
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 64, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.17817600071430206
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 4, 512], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.13209599256515503
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 8, 256], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.130048006772995
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.1377280056476593
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 32, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.13900800049304962
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 4, 256], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.13337600231170654
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 8, 128], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.13312000036239624
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 16, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.13542400300502777
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 4, 128], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.13593600690364838
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 8, 64], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.1372160017490387
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 4, 64], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.13849599659442902
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 4, 4096], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.3786751925945282
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 8, 2048], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.4069375991821289
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 16, 1024], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.4581376016139984
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 32, 512], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.4509696066379547
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 64, 256], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:46<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.4470784068107605
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 128, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.44830718636512756
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 256, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: 0.44871678948402405
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:18 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 4, 2048], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.3270656168460846
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 8, 1024], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.3520511984825134
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 16, 512], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.30289918184280396
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 32, 256], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.2930687963962555
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 64, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.289792001247406
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 128, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.28897279500961304
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 2, 64], 'thread': [1, 2, 64], 'rstep': []}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.19865599274635315
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 4, 32], 'thread': [1, 4, 32], 'rstep': []}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.19840000569820404
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 2, 2048], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.15462400019168854
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 128, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.21299199759960175
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 2, 1024], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.1305599957704544
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 64, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.14028799533843994
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 2, 512], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.13235199451446533
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 32, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.13977600634098053
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 2, 256], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.13363200426101685
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 16, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.1379839926958084
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 2, 128], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.13414399325847626
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 8, 32], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: 0.13388800621032715
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_add_20>: {'block': [1, 8, 256], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:INFO]: result: 0.130048006772995
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:INFO]: Tuning ['ladder_perfect_matmul_19', 'layout_transform_reshape_reshape_add_20', 'cast_multiply_21', 'mean_add_sqrt_divide_22', 'multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24', 'ladder_perfect_matmul_25', 'layout_transform_reshape_reshape_26', 'ladder_perfect_matmul_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28', 'ladder_perfect_matmul_29', 'layout_transform_reshape_reshape_add_30']
Processing:  60%|█████▉    | 28/47 [07:47<05:04, 16.00s/it]                                                           2024-01-15 02:18:19 [ladder:INFO]: Fusion group created: 13 ['ladder_perfect_matmul_19', 'layout_transform_reshape_reshape_add_20']
Processing:  60%|█████▉    | 28/47 [07:48<05:04, 16.00s/it]Processing:  64%|██████▍   | 30/47 [07:48<04:42, 16.63s/it]                                                           2024-01-15 02:18:19 [ladder:INFO]: Tuning ['cast_multiply_21', 'mean_add_sqrt_divide_22', 'multiply_cast_multiply_23']
Processing:  64%|██████▍   | 30/47 [07:48<04:42, 16.63s/it]                                                           2024-01-15 02:18:19 [ladder:INFO]: Tuning ['cast_multiply_21', 'mean_add_sqrt_divide_22']
Processing:  64%|██████▍   | 30/47 [07:48<04:42, 16.63s/it]                                                           2024-01-15 02:18:19 [ladder:INFO]: Tuning ['cast_multiply_21']
Processing:  64%|██████▍   | 30/47 [07:48<04:42, 16.63s/it]                                                           2024-01-15 02:18:19 [ladder:INFO]: Tuning ['mean_add_sqrt_divide_22']
Processing:  64%|██████▍   | 30/47 [07:48<04:42, 16.63s/it]                                                           2024-01-15 02:18:19 [ladder:INFO]: Tuning ['cast_multiply_21', 'mean_add_sqrt_divide_22', 'multiply_cast_multiply_23']
Processing:  64%|██████▍   | 30/47 [07:48<04:42, 16.63s/it]                                                           2024-01-15 02:18:19 [ladder:INFO]: Fusion group created: 14 ['cast_multiply_21', 'mean_add_sqrt_divide_22']
Processing:  64%|██████▍   | 30/47 [07:48<04:42, 16.63s/it]                                                           2024-01-15 02:18:19 [ladder:INFO]: Tuning ['multiply_cast_multiply_23', 'reshape_layout_transform_ladder_layout_transform_24']
Processing:  64%|██████▍   | 30/47 [07:48<04:42, 16.63s/it]                                                           2024-01-15 02:18:30 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 1, 8184], 'thread': [1, 1, 8], 'rstep': [], 'block_order': ((floordiv(block_idx, 32)*32) + (floormod(block_idx, 16)*2))}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 512, 1, 8], 'thread': [1, 8, 1, 1], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [07:58<04:42, 16.63s/it]                                                           2024-01-15 02:18:30 [ladder:DEBUG]: 0.766975998878479
Processing:  64%|██████▍   | 30/47 [07:58<04:42, 16.63s/it]                                                           2024-01-15 02:18:30 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 1, 8184], 'thread': [1, 1, 8], 'rstep': [], 'block_order': ((floordiv(block_idx, 32)*32) + (floormod(block_idx, 16)*2))}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 512, 1, 8], 'thread': [1, 8, 1, 1], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [07:58<04:42, 16.63s/it]                                                           2024-01-15 02:18:30 [ladder:INFO]: result: 0.766975998878479
Processing:  64%|██████▍   | 30/47 [07:58<04:42, 16.63s/it]                                                           2024-01-15 02:18:30 [ladder:INFO]: Tuning ['multiply_cast_multiply_23']
Processing:  64%|██████▍   | 30/47 [07:58<04:42, 16.63s/it]                                                           2024-01-15 02:18:45 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 64, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:45 [ladder:DEBUG]: 0.13189120590686798
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:45 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 128, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: 0.13332480192184448
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: 0.131071999669075
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: 0.13260799646377563
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 64, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: 0.13209599256515503
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 256], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: 0.1297920048236847
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: 0.131071999669075
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:46 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:14<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.12984320521354675
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.13414399325847626
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 8, 512], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.13885439932346344
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 8, 256], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.12748800218105316
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 8, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.13132800161838531
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 8, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.13542400300502777
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.13542400300502777
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 4, 1024], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.1451520025730133
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 4, 512], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.12825599312782288
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 4, 256], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.12723200023174286
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 4, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.13235199451446533
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 4, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.13900800049304962
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 8, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.13926400244235992
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 256, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.13393919169902802
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 512, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:15<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.13516800105571747
Processing:  64%|██████▍   | 30/47 [08:16<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 128, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:16<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: 0.13332480192184448
Processing:  64%|██████▍   | 30/47 [08:16<04:42, 16.63s/it]                                                           2024-01-15 02:18:47 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 128, 64], 'thread': [1, 2, 64], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:16<04:42, 16.63s/it]                                                           2024-01-15 02:18:48 [ladder:DEBUG]: 0.13209600746631622
Processing:  64%|██████▍   | 30/47 [08:16<04:42, 16.63s/it]                                                           2024-01-15 02:18:48 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 256, 32], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:16<04:42, 16.63s/it]                                                           2024-01-15 02:18:48 [ladder:DEBUG]: 0.12881919741630554
Processing:  64%|██████▍   | 30/47 [08:16<04:42, 16.63s/it]                                                           2024-01-15 02:18:48 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 64, 256], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:16<04:42, 16.63s/it]                                                           2024-01-15 02:18:48 [ladder:DEBUG]: 0.13619199395179749
Processing:  64%|██████▍   | 30/47 [08:16<04:42, 16.63s/it]                                                           2024-01-15 02:18:48 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 64, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:16<04:42, 16.63s/it]                                                           2024-01-15 02:18:48 [ladder:DEBUG]: 0.12902399897575378
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:48 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 512], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:48 [ladder:DEBUG]: 0.23347198963165283
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:48 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 32, 256], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: 0.13414399325847626
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 1024], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: 0.2725887894630432
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 16, 512], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: 0.3117055892944336
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 8, 2048], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: 0.3045375943183899
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 8, 1024], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:17<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: 0.2793472111225128
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 2, 64], 'thread': [1, 2, 64], 'rstep': []}}
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: 0.19865599274635315
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 4, 32], 'thread': [1, 4, 32], 'rstep': []}}
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: 0.19865599274635315
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 2, 2048], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: 0.1529856026172638
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 2, 1024], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: 0.12902399897575378
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 2, 512], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: 0.130048006772995
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:49 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 2, 256], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:50 [ladder:DEBUG]: 0.131071999669075
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:50 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 2, 128], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 2, 1]}}
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:50 [ladder:DEBUG]: 0.13414399325847626
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:50 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, multiply_cast_multiply_23>: {'block': [1, 4, 256], 'thread': [1, 1, 128], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:50 [ladder:INFO]: result: 0.12723200023174286
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:18:50 [ladder:INFO]: Tuning ['reshape_layout_transform_ladder_layout_transform_24']
Processing:  64%|██████▍   | 30/47 [08:18<04:42, 16.63s/it]                                                           2024-01-15 02:19:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 16, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:06 [ladder:DEBUG]: 0.10239999741315842
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 8, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:06 [ladder:DEBUG]: 0.1008640006184578
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [4, 4, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:06 [ladder:DEBUG]: 0.10419200360774994
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:06 [ladder:DEBUG]: 0.09139200299978256
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 4, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.09292799979448318
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 4, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.09292799979448318
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 64, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.17797119915485382
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 32, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.17817600071430206
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [4, 16, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.17612800002098083
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [8, 8, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.18247678875923157
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [16, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.19394560158252716
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 32, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.132915198802948
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 16, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.13127680122852325
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [4, 8, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.12472319602966309
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [8, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.14561280608177185
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 8, 1, 16], 'thread': [1, 8, 1, 16], 'rstep': []}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.19840000569820404
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [4, 8, 8, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.11238399893045425
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 8, 8, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.11238399893045425
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 8, 8, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.09855999797582626
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 8, 4, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.09267199784517288
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 8, 2, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.1016319990158081
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [2, 16, 8, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.11110399663448334
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 16, 8, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.11289600282907486
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 16, 4, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:35<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.10367999970912933
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 16, 2, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.09241600334644318
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 16, 1, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.10188800096511841
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 32, 8, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.11007999628782272
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 32, 4, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.10982400178909302
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 32, 2, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: 0.10649599879980087
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 32, 1, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: 0.11084800213575363
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 64, 4, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: 0.10982400178909302
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 64, 2, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: 0.10623999685049057
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 64, 1, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: 0.11443199962377548
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 128, 2, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: 0.10649599879980087
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 128, 1, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: 0.11443199962377548
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 256, 1, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: 0.1308159977197647
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 4, 2, 16], 'thread': [1, 4, 2, 16], 'rstep': []}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: 0.19840000569820404
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [8, 4, 8, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: 0.11340799927711487
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [8, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: 0.10035199671983719
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [4, 4, 8, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: 0.11468800157308578
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, reshape_layout_transform_ladder_layout_transform_24>: {'block': [1, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:INFO]: result: 0.09139200299978256
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:INFO]: Tuning ['multiply_cast_multiply_23']
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]                                                           2024-01-15 02:19:08 [ladder:INFO]: Fusion group created: 15 ['multiply_cast_multiply_23']
Processing:  64%|██████▍   | 30/47 [08:36<04:42, 16.63s/it]Processing:  77%|███████▋  | 36/47 [08:36<02:08, 11.69s/it]                                                           2024-01-15 02:19:08 [ladder:INFO]: Tuning ['reshape_layout_transform_ladder_layout_transform_24', 'ladder_perfect_matmul_25', 'ladder_perfect_matmul_27']
Processing:  77%|███████▋  | 36/47 [08:36<02:08, 11.69s/it]                                                           2024-01-15 02:19:08 [ladder:INFO]: Tuning ['reshape_layout_transform_ladder_layout_transform_24']
Processing:  77%|███████▋  | 36/47 [08:36<02:08, 11.69s/it]                                                           2024-01-15 02:19:08 [ladder:INFO]: Fusion group created: 16 ['reshape_layout_transform_ladder_layout_transform_24']
Processing:  77%|███████▋  | 36/47 [08:36<02:08, 11.69s/it]                                                           2024-01-15 02:19:08 [ladder:INFO]: Tuning ['ladder_perfect_matmul_25', 'layout_transform_reshape_reshape_26']
Processing:  77%|███████▋  | 36/47 [08:36<02:08, 11.69s/it]                                                           2024-01-15 02:19:09 [ladder:INFO]: Tuning ['ladder_perfect_matmul_25', 'layout_transform_reshape_reshape_26']
Processing:  77%|███████▋  | 36/47 [08:38<02:08, 11.69s/it]                                                           2024-01-15 02:19:29 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: 7.930675506591797
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: 9.521561622619629
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [8, 14, 16, 16], 'warp': [4, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: 7.993958473205566
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [4, 7, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: 10.354073524475098
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: 7.28760290145874
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [8, 7, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: 7.612825870513916
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [08:58<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: 15.468544006347656
Processing:  77%|███████▋  | 36/47 [08:59<02:08, 11.69s/it]                                                           2024-01-15 02:19:30 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [08:59<02:08, 11.69s/it]                                                           2024-01-15 02:19:31 [ladder:DEBUG]: 8.397414207458496
Processing:  77%|███████▋  | 36/47 [08:59<02:08, 11.69s/it]                                                           2024-01-15 02:19:31 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [2, 14, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [08:59<02:08, 11.69s/it]                                                           2024-01-15 02:19:31 [ladder:DEBUG]: 14.633984565734863
Processing:  77%|███████▋  | 36/47 [08:59<02:08, 11.69s/it]                                                           2024-01-15 02:19:31 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [4, 14, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [08:59<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: 8.508211135864258
Processing:  77%|███████▋  | 36/47 [09:00<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [2, 7, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:00<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: 16.662527084350586
Processing:  77%|███████▋  | 36/47 [09:00<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [16, 8, 16, 16], 'warp': [8, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:00<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: 7.467008113861084
Processing:  77%|███████▋  | 36/47 [09:00<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:00<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: 9.60348129272461
Processing:  77%|███████▋  | 36/47 [09:00<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:00<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: 12.480512619018555
Processing:  77%|███████▋  | 36/47 [09:00<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [16, 7, 16, 16], 'warp': [4, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:00<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: 7.9142913818359375
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:32 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: 26.63055419921875
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: 14.320025444030762
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: 18.59563446044922
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [1, 7, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: 27.743640899658203
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [1, 14, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: 25.983591079711914
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:01<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: 29.774642944335938
Processing:  77%|███████▋  | 36/47 [09:02<02:08, 11.69s/it]                                                           2024-01-15 02:19:33 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:02<02:08, 11.69s/it]                                                           2024-01-15 02:19:34 [ladder:DEBUG]: 15.358976364135742
Processing:  77%|███████▋  | 36/47 [09:02<02:08, 11.69s/it]                                                           2024-01-15 02:19:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:02<02:08, 11.69s/it]                                                           2024-01-15 02:19:34 [ladder:DEBUG]: 8.496538162231445
Processing:  77%|███████▋  | 36/47 [09:02<02:08, 11.69s/it]                                                           2024-01-15 02:19:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:02<02:08, 11.69s/it]                                                           2024-01-15 02:19:34 [ladder:DEBUG]: 18.178049087524414
Processing:  77%|███████▋  | 36/47 [09:02<02:08, 11.69s/it]                                                           2024-01-15 02:19:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:02<02:08, 11.69s/it]                                                           2024-01-15 02:19:34 [ladder:DEBUG]: 25.706701278686523
Processing:  77%|███████▋  | 36/47 [09:02<02:08, 11.69s/it]                                                           2024-01-15 02:19:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:02<02:08, 11.69s/it]                                                           2024-01-15 02:19:34 [ladder:DEBUG]: 24.35604476928711
Processing:  77%|███████▋  | 36/47 [09:03<02:08, 11.69s/it]                                                           2024-01-15 02:19:34 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:03<02:08, 11.69s/it]                                                           2024-01-15 02:19:35 [ladder:DEBUG]: 34.84856414794922
Processing:  77%|███████▋  | 36/47 [09:03<02:08, 11.69s/it]                                                           2024-01-15 02:19:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:03<02:08, 11.69s/it]                                                           2024-01-15 02:19:35 [ladder:DEBUG]: 26.0849666595459
Processing:  77%|███████▋  | 36/47 [09:03<02:08, 11.69s/it]                                                           2024-01-15 02:19:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:03<02:08, 11.69s/it]                                                           2024-01-15 02:19:35 [ladder:DEBUG]: 15.17650032043457
Processing:  77%|███████▋  | 36/47 [09:03<02:08, 11.69s/it]                                                           2024-01-15 02:19:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:03<02:08, 11.69s/it]                                                           2024-01-15 02:19:35 [ladder:DEBUG]: 29.25076675415039
Processing:  77%|███████▋  | 36/47 [09:03<02:08, 11.69s/it]                                                           2024-01-15 02:19:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:03<02:08, 11.69s/it]                                                           2024-01-15 02:19:35 [ladder:DEBUG]: 34.090187072753906
Processing:  77%|███████▋  | 36/47 [09:04<02:08, 11.69s/it]                                                           2024-01-15 02:19:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:04<02:08, 11.69s/it]                                                           2024-01-15 02:19:36 [ladder:DEBUG]: 45.17621612548828
Processing:  77%|███████▋  | 36/47 [09:04<02:08, 11.69s/it]                                                           2024-01-15 02:19:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:04<02:08, 11.69s/it]                                                           2024-01-15 02:19:36 [ladder:DEBUG]: 27.297382354736328
Processing:  77%|███████▋  | 36/47 [09:04<02:08, 11.69s/it]                                                           2024-01-15 02:19:36 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25__layout_transform_reshape_reshape_26>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:04<02:08, 11.69s/it]                                                           2024-01-15 02:19:36 [ladder:INFO]: result: 7.28760290145874
Processing:  77%|███████▋  | 36/47 [09:04<02:08, 11.69s/it]                                                           2024-01-15 02:19:36 [ladder:INFO]: Tuning ['ladder_perfect_matmul_25']
Processing:  77%|███████▋  | 36/47 [09:04<02:08, 11.69s/it]                                                           2024-01-15 02:19:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:23<02:08, 11.69s/it]                                                           2024-01-15 02:19:55 [ladder:DEBUG]: 7.093452453613281
Processing:  77%|███████▋  | 36/47 [09:23<02:08, 11.69s/it]                                                           2024-01-15 02:19:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [8, 7, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:23<02:08, 11.69s/it]                                                           2024-01-15 02:19:55 [ladder:DEBUG]: 7.644774436950684
Processing:  77%|███████▋  | 36/47 [09:23<02:08, 11.69s/it]                                                           2024-01-15 02:19:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:23<02:08, 11.69s/it]                                                           2024-01-15 02:19:55 [ladder:DEBUG]: 9.486540794372559
Processing:  77%|███████▋  | 36/47 [09:23<02:08, 11.69s/it]                                                           2024-01-15 02:19:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:23<02:08, 11.69s/it]                                                           2024-01-15 02:19:55 [ladder:DEBUG]: 9.46298885345459
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [16, 8, 16, 16], 'warp': [8, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: 7.450418949127197
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: 6.953370094299316
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [4, 7, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: 10.410188674926758
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [8, 14, 16, 16], 'warp': [4, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: 7.794483184814453
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [16, 7, 16, 16], 'warp': [4, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: 7.721164703369141
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: 12.443647384643555
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:24<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: 15.388262748718262
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: 15.305317878723145
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: 8.390860557556152
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: 8.470937728881836
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [2, 7, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: 16.79257583618164
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [4, 14, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: 8.362188339233398
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: 18.505523681640625
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:25<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: 18.1065731048584
Processing:  77%|███████▋  | 36/47 [09:26<02:08, 11.69s/it]                                                           2024-01-15 02:19:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [2, 14, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:26<02:08, 11.69s/it]                                                           2024-01-15 02:19:58 [ladder:DEBUG]: 14.711398124694824
Processing:  77%|███████▋  | 36/47 [09:26<02:08, 11.69s/it]                                                           2024-01-15 02:19:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:26<02:08, 11.69s/it]                                                           2024-01-15 02:19:58 [ladder:DEBUG]: 24.241357803344727
Processing:  77%|███████▋  | 36/47 [09:26<02:08, 11.69s/it]                                                           2024-01-15 02:19:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:26<02:08, 11.69s/it]                                                           2024-01-15 02:19:58 [ladder:DEBUG]: 26.783946990966797
Processing:  77%|███████▋  | 36/47 [09:26<02:08, 11.69s/it]                                                           2024-01-15 02:19:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:26<02:08, 11.69s/it]                                                           2024-01-15 02:19:58 [ladder:DEBUG]: 26.17856216430664
Processing:  77%|███████▋  | 36/47 [09:26<02:08, 11.69s/it]                                                           2024-01-15 02:19:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:26<02:08, 11.69s/it]                                                           2024-01-15 02:19:58 [ladder:DEBUG]: 15.213363647460938
Processing:  77%|███████▋  | 36/47 [09:27<02:08, 11.69s/it]                                                           2024-01-15 02:19:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:27<02:08, 11.69s/it]                                                           2024-01-15 02:19:59 [ladder:DEBUG]: 14.404197692871094
Processing:  77%|███████▋  | 36/47 [09:27<02:08, 11.69s/it]                                                           2024-01-15 02:19:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [1, 7, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:27<02:08, 11.69s/it]                                                           2024-01-15 02:19:59 [ladder:DEBUG]: 27.715173721313477
Processing:  77%|███████▋  | 36/47 [09:27<02:08, 11.69s/it]                                                           2024-01-15 02:19:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:27<02:08, 11.69s/it]                                                           2024-01-15 02:19:59 [ladder:DEBUG]: 29.99623680114746
Processing:  77%|███████▋  | 36/47 [09:27<02:08, 11.69s/it]                                                           2024-01-15 02:19:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:27<02:08, 11.69s/it]                                                           2024-01-15 02:19:59 [ladder:DEBUG]: 29.145702362060547
Processing:  77%|███████▋  | 36/47 [09:27<02:08, 11.69s/it]                                                           2024-01-15 02:19:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [1, 14, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:27<02:08, 11.69s/it]                                                           2024-01-15 02:19:59 [ladder:DEBUG]: 26.133296966552734
Processing:  77%|███████▋  | 36/47 [09:28<02:08, 11.69s/it]                                                           2024-01-15 02:19:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:28<02:08, 11.69s/it]                                                           2024-01-15 02:20:00 [ladder:DEBUG]: 34.754966735839844
Processing:  77%|███████▋  | 36/47 [09:28<02:08, 11.69s/it]                                                           2024-01-15 02:20:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:28<02:08, 11.69s/it]                                                           2024-01-15 02:20:00 [ladder:DEBUG]: 34.20958709716797
Processing:  77%|███████▋  | 36/47 [09:28<02:08, 11.69s/it]                                                           2024-01-15 02:20:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:28<02:08, 11.69s/it]                                                           2024-01-15 02:20:00 [ladder:DEBUG]: 45.23253631591797
Processing:  77%|███████▋  | 36/47 [09:28<02:08, 11.69s/it]                                                           2024-01-15 02:20:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:28<02:08, 11.69s/it]                                                           2024-01-15 02:20:00 [ladder:DEBUG]: 25.70526695251465
Processing:  77%|███████▋  | 36/47 [09:29<02:08, 11.69s/it]                                                           2024-01-15 02:20:00 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:29<02:08, 11.69s/it]                                                           2024-01-15 02:20:01 [ladder:DEBUG]: 27.67339515686035
Processing:  77%|███████▋  | 36/47 [09:29<02:08, 11.69s/it]                                                           2024-01-15 02:20:01 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_25>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  77%|███████▋  | 36/47 [09:29<02:08, 11.69s/it]                                                           2024-01-15 02:20:01 [ladder:INFO]: result: 6.953370094299316
Processing:  77%|███████▋  | 36/47 [09:29<02:08, 11.69s/it]                                                           2024-01-15 02:20:01 [ladder:INFO]: Tuning ['layout_transform_reshape_reshape_26']
Processing:  77%|███████▋  | 36/47 [09:29<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.36531201004981995
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.3491840064525604
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 8, 64], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.30847999453544617
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 4, 128], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.30720001459121704
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 4, 112], 'thread': [1, 4, 28], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.3266560137271881
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.4047360122203827
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 8, 32], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.3537920117378235
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 4, 64], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.34457600116729736
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 128, 112], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.6905856132507324
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 64, 224], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:43<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.5763071775436401
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 448], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.622592031955719
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 896], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.6154240369796753
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 8, 1792], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: 0.5597184300422668
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 4, 3584], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.5242879986763
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 64, 112], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.5122047662734985
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 224], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.41246718168258667
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 448], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.4679679870605469
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 8, 896], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.40816640853881836
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 4, 1792], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.3811327815055847
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 8, 16], 'thread': [1, 8, 16], 'rstep': []}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.6835200190544128
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 112], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.3686400055885315
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 224], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.32844799757003784
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 8, 448], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.326911985874176
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 4, 32], 'thread': [1, 4, 32], 'rstep': []}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.6835200190544128
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 4, 896], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.32204800844192505
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 128, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.4682239890098572
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 64, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.37887999415397644
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.3596799969673157
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 128], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.3681280016899109
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 8, 256], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.3171840012073517
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 4, 512], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.32307198643684387
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 112], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.3671039938926697
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 8, 224], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.3199999928474426
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 4, 448], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.3169279992580414
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 64, 16], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:44<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.392192006111145
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 32, 32], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.36403200030326843
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 16, 64], 'thread': [1, 16, 8], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.35712000727653503
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 8, 128], 'thread': [1, 8, 16], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.3184640109539032
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 4, 256], 'thread': [1, 4, 32], 'rstep': [], 'step': [1, 1, 2]}}
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.31488001346588135
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 8, 112], 'thread': [1, 8, 16], 'rstep': []}}
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: 0.3036159873008728
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, layout_transform_reshape_reshape_26>: {'block': [1, 8, 112], 'thread': [1, 8, 16], 'rstep': []}}
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:INFO]: result: 0.3036159873008728
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:INFO]: Tuning ['ladder_perfect_matmul_25']
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]                                                           2024-01-15 02:20:16 [ladder:INFO]: Fusion group created: 17 ['ladder_perfect_matmul_25']
Processing:  77%|███████▋  | 36/47 [09:45<02:08, 11.69s/it]Processing:  83%|████████▎ | 39/47 [09:45<01:59, 14.96s/it]                                                           2024-01-15 02:20:16 [ladder:INFO]: Tuning ['layout_transform_reshape_reshape_26', 'ladder_perfect_matmul_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
Processing:  83%|████████▎ | 39/47 [09:45<01:59, 14.96s/it]                                                           2024-01-15 02:20:25 [ladder:INFO]: Tuning ['layout_transform_reshape_reshape_26']
Processing:  83%|████████▎ | 39/47 [09:54<01:59, 14.96s/it]                                                           2024-01-15 02:20:25 [ladder:INFO]: Fusion group created: 18 ['layout_transform_reshape_reshape_26']
Processing:  83%|████████▎ | 39/47 [09:54<01:59, 14.96s/it]Processing:  85%|████████▌ | 40/47 [09:54<01:39, 14.21s/it]                                                           2024-01-15 02:20:25 [ladder:INFO]: Tuning ['ladder_perfect_matmul_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
Processing:  85%|████████▌ | 40/47 [09:54<01:39, 14.21s/it]                                                           2024-01-15 02:20:39 [ladder:INFO]: Tuning ['ladder_perfect_matmul_27', 'sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
Processing:  85%|████████▌ | 40/47 [10:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:36<01:39, 14.21s/it]                                                           2024-01-15 02:21:08 [ladder:DEBUG]: 9.875250816345215
Processing:  85%|████████▌ | 40/47 [10:36<01:39, 14.21s/it]                                                           2024-01-15 02:21:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:36<01:39, 14.21s/it]                                                           2024-01-15 02:21:08 [ladder:DEBUG]: 7.565926551818848
Processing:  85%|████████▌ | 40/47 [10:36<01:39, 14.21s/it]                                                           2024-01-15 02:21:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 7, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:36<01:39, 14.21s/it]                                                           2024-01-15 02:21:08 [ladder:DEBUG]: 10.509721755981445
Processing:  85%|████████▌ | 40/47 [10:37<01:39, 14.21s/it]                                                           2024-01-15 02:21:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 14, 16, 16], 'warp': [4, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:37<01:39, 14.21s/it]                                                           2024-01-15 02:21:09 [ladder:DEBUG]: 7.660953521728516
Processing:  85%|████████▌ | 40/47 [10:37<01:39, 14.21s/it]                                                           2024-01-15 02:21:09 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:37<01:39, 14.21s/it]                                                           2024-01-15 02:21:09 [ladder:DEBUG]: 7.446732997894287
Processing:  85%|████████▌ | 40/47 [10:37<01:39, 14.21s/it]                                                           2024-01-15 02:21:09 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 7, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:37<01:39, 14.21s/it]                                                           2024-01-15 02:21:09 [ladder:DEBUG]: 7.865958213806152
Processing:  85%|████████▌ | 40/47 [10:37<01:39, 14.21s/it]                                                           2024-01-15 02:21:09 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:37<01:39, 14.21s/it]                                                           2024-01-15 02:21:09 [ladder:DEBUG]: 15.715327262878418
Processing:  85%|████████▌ | 40/47 [10:37<01:39, 14.21s/it]                                                           2024-01-15 02:21:09 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:37<01:39, 14.21s/it]                                                           2024-01-15 02:21:09 [ladder:DEBUG]: 8.698880195617676
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:09 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 14, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: 15.0648832321167
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 7, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: 16.550296783447266
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 14, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: 8.95201301574707
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: 9.797222137451172
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 8, 16, 16], 'warp': [8, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: 7.9652862548828125
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:38<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: 12.662374496459961
Processing:  85%|████████▌ | 40/47 [10:39<01:39, 14.21s/it]                                                           2024-01-15 02:21:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 7, 16, 16], 'warp': [4, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:39<01:39, 14.21s/it]                                                           2024-01-15 02:21:11 [ladder:DEBUG]: 7.856741905212402
Processing:  85%|████████▌ | 40/47 [10:39<01:39, 14.21s/it]                                                           2024-01-15 02:21:11 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:39<01:39, 14.21s/it]                                                           2024-01-15 02:21:11 [ladder:DEBUG]: 26.815692901611328
Processing:  85%|████████▌ | 40/47 [10:39<01:39, 14.21s/it]                                                           2024-01-15 02:21:11 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:39<01:39, 14.21s/it]                                                           2024-01-15 02:21:11 [ladder:DEBUG]: 14.884249687194824
Processing:  85%|████████▌ | 40/47 [10:39<01:39, 14.21s/it]                                                           2024-01-15 02:21:11 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:39<01:39, 14.21s/it]                                                           2024-01-15 02:21:11 [ladder:DEBUG]: 18.507980346679688
Processing:  85%|████████▌ | 40/47 [10:40<01:39, 14.21s/it]                                                           2024-01-15 02:21:11 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 7, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:40<01:39, 14.21s/it]                                                           2024-01-15 02:21:12 [ladder:DEBUG]: 28.54400062561035
Processing:  85%|████████▌ | 40/47 [10:40<01:39, 14.21s/it]                                                           2024-01-15 02:21:12 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 14, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:40<01:39, 14.21s/it]                                                           2024-01-15 02:21:12 [ladder:DEBUG]: 26.60372543334961
Processing:  85%|████████▌ | 40/47 [10:40<01:39, 14.21s/it]                                                           2024-01-15 02:21:12 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:40<01:39, 14.21s/it]                                                           2024-01-15 02:21:13 [ladder:DEBUG]: 30.317773818969727
Processing:  85%|████████▌ | 40/47 [10:41<01:39, 14.21s/it]                                                           2024-01-15 02:21:13 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:41<01:39, 14.21s/it]                                                           2024-01-15 02:21:13 [ladder:DEBUG]: 15.525888442993164
Processing:  85%|████████▌ | 40/47 [10:41<01:39, 14.21s/it]                                                           2024-01-15 02:21:13 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:41<01:39, 14.21s/it]                                                           2024-01-15 02:21:13 [ladder:DEBUG]: 8.876646041870117
Processing:  85%|████████▌ | 40/47 [10:41<01:39, 14.21s/it]                                                           2024-01-15 02:21:13 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:41<01:39, 14.21s/it]                                                           2024-01-15 02:21:13 [ladder:DEBUG]: 18.499584197998047
Processing:  85%|████████▌ | 40/47 [10:42<01:39, 14.21s/it]                                                           2024-01-15 02:21:13 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:42<01:39, 14.21s/it]                                                           2024-01-15 02:21:14 [ladder:DEBUG]: 25.843505859375
Processing:  85%|████████▌ | 40/47 [10:42<01:39, 14.21s/it]                                                           2024-01-15 02:21:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:42<01:39, 14.21s/it]                                                           2024-01-15 02:21:14 [ladder:DEBUG]: 24.441652297973633
Processing:  85%|████████▌ | 40/47 [10:42<01:39, 14.21s/it]                                                           2024-01-15 02:21:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:42<01:39, 14.21s/it]                                                           2024-01-15 02:21:14 [ladder:DEBUG]: 34.960792541503906
Processing:  85%|████████▌ | 40/47 [10:42<01:39, 14.21s/it]                                                           2024-01-15 02:21:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:42<01:39, 14.21s/it]                                                           2024-01-15 02:21:14 [ladder:DEBUG]: 26.34219741821289
Processing:  85%|████████▌ | 40/47 [10:43<01:39, 14.21s/it]                                                           2024-01-15 02:21:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:43<01:39, 14.21s/it]                                                           2024-01-15 02:21:15 [ladder:DEBUG]: 15.644058227539062
Processing:  85%|████████▌ | 40/47 [10:43<01:39, 14.21s/it]                                                           2024-01-15 02:21:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:43<01:39, 14.21s/it]                                                           2024-01-15 02:21:15 [ladder:DEBUG]: 29.367090225219727
Processing:  85%|████████▌ | 40/47 [10:43<01:39, 14.21s/it]                                                           2024-01-15 02:21:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:43<01:39, 14.21s/it]                                                           2024-01-15 02:21:15 [ladder:DEBUG]: 34.167396545410156
Processing:  85%|████████▌ | 40/47 [10:43<01:39, 14.21s/it]                                                           2024-01-15 02:21:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:43<01:39, 14.21s/it]                                                           2024-01-15 02:21:15 [ladder:DEBUG]: 45.34743118286133
Processing:  85%|████████▌ | 40/47 [10:44<01:39, 14.21s/it]                                                           2024-01-15 02:21:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:44<01:39, 14.21s/it]                                                           2024-01-15 02:21:16 [ladder:DEBUG]: 28.048999786376953
Processing:  85%|████████▌ | 40/47 [10:44<01:39, 14.21s/it]                                                           2024-01-15 02:21:16 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27__sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [10:44<01:39, 14.21s/it]                                                           2024-01-15 02:21:16 [ladder:INFO]: result: 7.446732997894287
Processing:  85%|████████▌ | 40/47 [10:44<01:39, 14.21s/it]                                                           2024-01-15 02:21:16 [ladder:INFO]: Tuning ['ladder_perfect_matmul_27']
Processing:  85%|████████▌ | 40/47 [10:44<01:39, 14.21s/it]                                                           2024-01-15 02:21:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:04<01:39, 14.21s/it]                                                           2024-01-15 02:21:36 [ladder:DEBUG]: 7.039590358734131
Processing:  85%|████████▌ | 40/47 [11:04<01:39, 14.21s/it]                                                           2024-01-15 02:21:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [8, 7, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:04<01:39, 14.21s/it]                                                           2024-01-15 02:21:36 [ladder:DEBUG]: 7.5259904861450195
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:36 [ladder:DEBUG]: 9.50886344909668
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: 9.570303916931152
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [16, 8, 16, 16], 'warp': [8, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: 7.4369025230407715
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: 6.968934535980225
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [4, 7, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: 10.246963500976562
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [8, 14, 16, 16], 'warp': [4, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: 7.83462381362915
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [16, 7, 16, 16], 'warp': [4, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:05<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: 7.718297481536865
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: 12.510208129882812
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: 15.360204696655273
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: 15.293848991394043
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: 8.295219421386719
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: 8.52889633178711
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [2, 7, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: 16.728267669677734
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [4, 14, 16, 16], 'warp': [2, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:06<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: 8.509849548339844
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: 18.528871536254883
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:38 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:39 [ladder:DEBUG]: 18.147327423095703
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:39 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [2, 14, 16, 16], 'warp': [1, 7, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:39 [ladder:DEBUG]: 14.679040908813477
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:39 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:39 [ladder:DEBUG]: 24.25241470336914
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:39 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:39 [ladder:DEBUG]: 26.7522029876709
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:39 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:07<01:39, 14.21s/it]                                                           2024-01-15 02:21:39 [ladder:DEBUG]: 26.05629539489746
Processing:  85%|████████▌ | 40/47 [11:08<01:39, 14.21s/it]                                                           2024-01-15 02:21:39 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:08<01:39, 14.21s/it]                                                           2024-01-15 02:21:40 [ladder:DEBUG]: 15.317400932312012
Processing:  85%|████████▌ | 40/47 [11:08<01:39, 14.21s/it]                                                           2024-01-15 02:21:40 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:08<01:39, 14.21s/it]                                                           2024-01-15 02:21:40 [ladder:DEBUG]: 14.384946823120117
Processing:  85%|████████▌ | 40/47 [11:08<01:39, 14.21s/it]                                                           2024-01-15 02:21:40 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [1, 7, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:08<01:39, 14.21s/it]                                                           2024-01-15 02:21:40 [ladder:DEBUG]: 27.751628875732422
Processing:  85%|████████▌ | 40/47 [11:08<01:39, 14.21s/it]                                                           2024-01-15 02:21:40 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:08<01:39, 14.21s/it]                                                           2024-01-15 02:21:40 [ladder:DEBUG]: 30.023677825927734
Processing:  85%|████████▌ | 40/47 [11:08<01:39, 14.21s/it]                                                           2024-01-15 02:21:40 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:08<01:39, 14.21s/it]                                                           2024-01-15 02:21:40 [ladder:DEBUG]: 29.14406394958496
Processing:  85%|████████▌ | 40/47 [11:09<01:39, 14.21s/it]                                                           2024-01-15 02:21:40 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [1, 14, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:09<01:39, 14.21s/it]                                                           2024-01-15 02:21:41 [ladder:DEBUG]: 26.047693252563477
Processing:  85%|████████▌ | 40/47 [11:09<01:39, 14.21s/it]                                                           2024-01-15 02:21:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:09<01:39, 14.21s/it]                                                           2024-01-15 02:21:41 [ladder:DEBUG]: 34.84016799926758
Processing:  85%|████████▌ | 40/47 [11:09<01:39, 14.21s/it]                                                           2024-01-15 02:21:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:09<01:39, 14.21s/it]                                                           2024-01-15 02:21:41 [ladder:DEBUG]: 34.34455108642578
Processing:  85%|████████▌ | 40/47 [11:09<01:39, 14.21s/it]                                                           2024-01-15 02:21:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:09<01:39, 14.21s/it]                                                           2024-01-15 02:21:41 [ladder:DEBUG]: 45.23417663574219
Processing:  85%|████████▌ | 40/47 [11:10<01:39, 14.21s/it]                                                           2024-01-15 02:21:41 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:10<01:39, 14.21s/it]                                                           2024-01-15 02:21:42 [ladder:DEBUG]: 25.712230682373047
Processing:  85%|████████▌ | 40/47 [11:10<01:39, 14.21s/it]                                                           2024-01-15 02:21:42 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:10<01:39, 14.21s/it]                                                           2024-01-15 02:21:42 [ladder:DEBUG]: 27.61359214782715
Processing:  85%|████████▌ | 40/47 [11:10<01:39, 14.21s/it]                                                           2024-01-15 02:21:42 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_27>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  85%|████████▌ | 40/47 [11:10<01:39, 14.21s/it]                                                           2024-01-15 02:21:42 [ladder:INFO]: result: 6.968934535980225
Processing:  85%|████████▌ | 40/47 [11:10<01:39, 14.21s/it]                                                           2024-01-15 02:21:42 [ladder:INFO]: Tuning ['sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
Processing:  85%|████████▌ | 40/47 [11:10<01:39, 14.21s/it]                                                           2024-01-15 02:22:02 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 7, 16, 16], 'thread': [8, 1, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:30<01:39, 14.21s/it]                                                           2024-01-15 02:22:03 [ladder:DEBUG]: 1.8911231756210327
Processing:  85%|████████▌ | 40/47 [11:31<01:39, 14.21s/it]                                                           2024-01-15 02:22:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 14, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:31<01:39, 14.21s/it]                                                           2024-01-15 02:22:03 [ladder:DEBUG]: 1.4639103412628174
Processing:  85%|████████▌ | 40/47 [11:32<01:39, 14.21s/it]                                                           2024-01-15 02:22:03 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 28, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:32<01:39, 14.21s/it]                                                           2024-01-15 02:22:04 [ladder:DEBUG]: 1.3985792398452759
Processing:  85%|████████▌ | 40/47 [11:32<01:39, 14.21s/it]                                                           2024-01-15 02:22:04 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 56, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:32<01:39, 14.21s/it]                                                           2024-01-15 02:22:05 [ladder:DEBUG]: 1.3850624561309814
Processing:  85%|████████▌ | 40/47 [11:33<01:39, 14.21s/it]                                                           2024-01-15 02:22:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 7, 16, 16], 'thread': [4, 1, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:33<01:39, 14.21s/it]                                                           2024-01-15 02:22:05 [ladder:DEBUG]: 0.8863743543624878
Processing:  85%|████████▌ | 40/47 [11:33<01:39, 14.21s/it]                                                           2024-01-15 02:22:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 14, 16, 16], 'thread': [2, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:33<01:39, 14.21s/it]                                                           2024-01-15 02:22:05 [ladder:DEBUG]: 0.8658944368362427
Processing:  85%|████████▌ | 40/47 [11:33<01:39, 14.21s/it]                                                           2024-01-15 02:22:05 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 28, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:33<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: 0.8333312273025513
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 7, 16, 16], 'thread': [2, 1, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: 0.49664002656936646
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 14, 16, 16], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: 0.4820992052555084
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 8, 16, 16], 'thread': [1, 8, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: 0.46305280923843384
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 4, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: 0.4607999920845032
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 7, 16, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: 0.47349756956100464
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 4, 16, 16], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: 0.49704962968826294
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 8, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: 0.5531648397445679
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 16, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: 0.5511168241500854
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 4, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: 0.5738495588302612
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:06 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 8, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:34<01:39, 14.21s/it]                                                           2024-01-15 02:22:07 [ladder:DEBUG]: 1.7219583988189697
Processing:  85%|████████▌ | 40/47 [11:35<01:39, 14.21s/it]                                                           2024-01-15 02:22:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 16, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:35<01:39, 14.21s/it]                                                           2024-01-15 02:22:07 [ladder:DEBUG]: 1.4370815753936768
Processing:  85%|████████▌ | 40/47 [11:36<01:39, 14.21s/it]                                                           2024-01-15 02:22:07 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 32, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:36<01:39, 14.21s/it]                                                           2024-01-15 02:22:08 [ladder:DEBUG]: 1.4311424493789673
Processing:  85%|████████▌ | 40/47 [11:36<01:39, 14.21s/it]                                                           2024-01-15 02:22:08 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 64, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:36<01:39, 14.21s/it]                                                           2024-01-15 02:22:09 [ladder:DEBUG]: 1.63778555393219
Processing:  85%|████████▌ | 40/47 [11:37<01:39, 14.21s/it]                                                           2024-01-15 02:22:09 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 4, 16, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:37<01:39, 14.21s/it]                                                           2024-01-15 02:22:09 [ladder:DEBUG]: 3.233996868133545
Processing:  85%|████████▌ | 40/47 [11:37<01:39, 14.21s/it]                                                           2024-01-15 02:22:09 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 8, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:37<01:39, 14.21s/it]                                                           2024-01-15 02:22:09 [ladder:DEBUG]: 1.14094078540802
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:09 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 16, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: 1.1411455869674683
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 32, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: 1.1315200328826904
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 4, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: 1.344102382659912
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 2, 16, 16], 'thread': [1, 2, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: 0.5021696090698242
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 2, 16, 16], 'thread': [4, 2, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: 0.4786176085472107
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 2, 16, 16], 'thread': [2, 2, 4, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: 0.49664002656936646
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 2, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:38<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: 0.6610943675041199
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:10 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [32, 2, 16, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:11 [ladder:DEBUG]: 2.7987968921661377
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:11 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 2, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:11 [ladder:DEBUG]: 1.2972031831741333
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:11 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 1, 16, 16], 'thread': [2, 1, 8, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:11 [ladder:DEBUG]: 0.5009407997131348
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:11 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 4, 16, 8], 'thread': [1, 4, 4, 8], 'rstep': [], 'step': [1, 1, 2, 1]}}
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:11 [ladder:DEBUG]: 0.4861951768398285
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:11 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 1, 16, 16], 'thread': [1, 1, 16, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:11 [ladder:DEBUG]: 0.5017600059509277
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:11 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 7, 16, 16], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:39<01:39, 14.21s/it]                                                           2024-01-15 02:22:12 [ladder:DEBUG]: 3.2567296028137207
Processing:  85%|████████▌ | 40/47 [11:41<01:39, 14.21s/it]                                                           2024-01-15 02:22:12 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [8, 14, 16, 16], 'thread': [8, 2, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:41<01:39, 14.21s/it]                                                           2024-01-15 02:22:14 [ladder:DEBUG]: 1.9992576837539673
Processing:  85%|████████▌ | 40/47 [11:42<01:39, 14.21s/it]                                                           2024-01-15 02:22:14 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [4, 28, 16, 16], 'thread': [4, 4, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:42<01:39, 14.21s/it]                                                           2024-01-15 02:22:15 [ladder:DEBUG]: 1.701888084411621
Processing:  85%|████████▌ | 40/47 [11:43<01:39, 14.21s/it]                                                           2024-01-15 02:22:15 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 56, 16, 16], 'thread': [2, 8, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:43<01:39, 14.21s/it]                                                           2024-01-15 02:22:16 [ladder:DEBUG]: 1.678540825843811
Processing:  85%|████████▌ | 40/47 [11:44<01:39, 14.21s/it]                                                           2024-01-15 02:22:16 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [1, 112, 16, 16], 'thread': [1, 16, 1, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:44<01:39, 14.21s/it]                                                           2024-01-15 02:22:17 [ladder:DEBUG]: 1.6850944757461548
Processing:  85%|████████▌ | 40/47 [11:45<01:39, 14.21s/it]                                                           2024-01-15 02:22:17 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [16, 7, 16, 8], 'thread': [16, 1, 1, 8], 'rstep': [], 'step': [1, 1, 2, 1]}}
Processing:  85%|████████▌ | 40/47 [11:45<01:39, 14.21s/it]                                                           2024-01-15 02:22:18 [ladder:DEBUG]: 1.5118335485458374
Processing:  85%|████████▌ | 40/47 [11:46<01:39, 14.21s/it]                                                           2024-01-15 02:22:18 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28>: {'block': [2, 4, 16, 16], 'thread': [2, 4, 2, 8], 'rstep': [], 'step': [1, 1, 1, 2]}}
Processing:  85%|████████▌ | 40/47 [11:46<01:39, 14.21s/it]                                                           2024-01-15 02:22:18 [ladder:INFO]: result: 0.4607999920845032
Processing:  85%|████████▌ | 40/47 [11:46<01:39, 14.21s/it]                                                           2024-01-15 02:22:18 [ladder:INFO]: Tuning ['ladder_perfect_matmul_27']
Processing:  85%|████████▌ | 40/47 [11:46<01:39, 14.21s/it]                                                           2024-01-15 02:22:18 [ladder:INFO]: Fusion group created: 19 ['ladder_perfect_matmul_27']
Processing:  85%|████████▌ | 40/47 [11:46<01:39, 14.21s/it]Processing:  89%|████████▉ | 42/47 [11:46<02:05, 25.07s/it]                                                           2024-01-15 02:22:18 [ladder:INFO]: Tuning ['sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28', 'ladder_perfect_matmul_29']
Processing:  89%|████████▉ | 42/47 [11:46<02:05, 25.07s/it]                                                           2024-01-15 02:22:18 [ladder:INFO]: Tuning ['sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
Processing:  89%|████████▉ | 42/47 [11:46<02:05, 25.07s/it]                                                           2024-01-15 02:22:18 [ladder:INFO]: Fusion group created: 20 ['sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28']
Processing:  89%|████████▉ | 42/47 [11:46<02:05, 25.07s/it]                                                           2024-01-15 02:22:18 [ladder:INFO]: Tuning ['ladder_perfect_matmul_29', 'layout_transform_reshape_reshape_add_30']
Processing:  89%|████████▉ | 42/47 [11:46<02:05, 25.07s/it]                                                           2024-01-15 02:22:19 [ladder:INFO]: Tuning ['ladder_perfect_matmul_29', 'layout_transform_reshape_reshape_add_30']
Processing:  89%|████████▉ | 42/47 [11:47<02:05, 25.07s/it]                                                           2024-01-15 02:22:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:03<02:05, 25.07s/it]                                                           2024-01-15 02:22:35 [ladder:DEBUG]: 7.2681474685668945
Processing:  89%|████████▉ | 42/47 [12:03<02:05, 25.07s/it]                                                           2024-01-15 02:22:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:03<02:05, 25.07s/it]                                                           2024-01-15 02:22:35 [ladder:DEBUG]: 10.351411819458008
Processing:  89%|████████▉ | 42/47 [12:03<02:05, 25.07s/it]                                                           2024-01-15 02:22:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:03<02:05, 25.07s/it]                                                           2024-01-15 02:22:35 [ladder:DEBUG]: 7.045734405517578
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:35 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:35 [ladder:DEBUG]: 11.606630325317383
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [16, 8, 16, 16], 'warp': [8, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:36 [ladder:DEBUG]: 7.876198768615723
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:36 [ladder:DEBUG]: 14.59916877746582
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:36 [ladder:DEBUG]: 17.411685943603516
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:36 [ladder:DEBUG]: 8.609996795654297
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:04<02:05, 25.07s/it]                                                           2024-01-15 02:22:36 [ladder:DEBUG]: 23.829504013061523
Processing:  89%|████████▉ | 42/47 [12:05<02:05, 25.07s/it]                                                           2024-01-15 02:22:36 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:05<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: 9.92829418182373
Processing:  89%|████████▉ | 42/47 [12:05<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:05<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: 20.33090591430664
Processing:  89%|████████▉ | 42/47 [12:05<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:05<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: 22.054502487182617
Processing:  89%|████████▉ | 42/47 [12:05<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:05<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: 26.087627410888672
Processing:  89%|████████▉ | 42/47 [12:05<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:05<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: 30.459697723388672
Processing:  89%|████████▉ | 42/47 [12:06<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:06<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: 15.508890151977539
Processing:  89%|████████▉ | 42/47 [12:06<02:05, 25.07s/it]                                                           2024-01-15 02:22:37 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:06<02:05, 25.07s/it]                                                           2024-01-15 02:22:38 [ladder:DEBUG]: 34.230064392089844
Processing:  89%|████████▉ | 42/47 [12:06<02:05, 25.07s/it]                                                           2024-01-15 02:22:38 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:06<02:05, 25.07s/it]                                                           2024-01-15 02:22:38 [ladder:DEBUG]: 24.912487030029297
Processing:  89%|████████▉ | 42/47 [12:06<02:05, 25.07s/it]                                                           2024-01-15 02:22:38 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:06<02:05, 25.07s/it]                                                           2024-01-15 02:22:38 [ladder:DEBUG]: 46.529945373535156
Processing:  89%|████████▉ | 42/47 [12:06<02:05, 25.07s/it]                                                           2024-01-15 02:22:38 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:06<02:05, 25.07s/it]                                                           2024-01-15 02:22:39 [ladder:DEBUG]: 34.071346282958984
Processing:  89%|████████▉ | 42/47 [12:07<02:05, 25.07s/it]                                                           2024-01-15 02:22:39 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:07<02:05, 25.07s/it]                                                           2024-01-15 02:22:39 [ladder:DEBUG]: 37.49519348144531
Processing:  89%|████████▉ | 42/47 [12:07<02:05, 25.07s/it]                                                           2024-01-15 02:22:39 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:07<02:05, 25.07s/it]                                                           2024-01-15 02:22:39 [ladder:DEBUG]: 37.507484436035156
Processing:  89%|████████▉ | 42/47 [12:07<02:05, 25.07s/it]                                                           2024-01-15 02:22:39 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:07<02:05, 25.07s/it]                                                           2024-01-15 02:22:39 [ladder:DEBUG]: 27.415552139282227
Processing:  89%|████████▉ | 42/47 [12:07<02:05, 25.07s/it]                                                           2024-01-15 02:22:39 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:07<02:05, 25.07s/it]                                                           2024-01-15 02:22:40 [ladder:DEBUG]: 46.60920333862305
Processing:  89%|████████▉ | 42/47 [12:08<02:05, 25.07s/it]                                                           2024-01-15 02:22:40 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:08<02:05, 25.07s/it]                                                           2024-01-15 02:22:40 [ladder:DEBUG]: 49.49053192138672
Processing:  89%|████████▉ | 42/47 [12:08<02:05, 25.07s/it]                                                           2024-01-15 02:22:40 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29__layout_transform_reshape_reshape_add_30>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {3: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:08<02:05, 25.07s/it]                                                           2024-01-15 02:22:40 [ladder:INFO]: result: 7.045734405517578
Processing:  89%|████████▉ | 42/47 [12:08<02:05, 25.07s/it]                                                           2024-01-15 02:22:40 [ladder:INFO]: Tuning ['ladder_perfect_matmul_29']
Processing:  89%|████████▉ | 42/47 [12:08<02:05, 25.07s/it]                                                           2024-01-15 02:22:54 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [8, 8, 16, 16], 'warp': [4, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:22<02:05, 25.07s/it]                                                           2024-01-15 02:22:54 [ladder:DEBUG]: 7.107378959655762
Processing:  89%|████████▉ | 42/47 [12:22<02:05, 25.07s/it]                                                           2024-01-15 02:22:54 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [4, 8, 16, 16], 'warp': [2, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:22<02:05, 25.07s/it]                                                           2024-01-15 02:22:54 [ladder:DEBUG]: 10.812211036682129
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:54 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [8, 4, 16, 16], 'warp': [4, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:54 [ladder:DEBUG]: 12.238438606262207
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:54 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: 7.049625396728516
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [16, 8, 16, 16], 'warp': [8, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: 7.667916774749756
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [4, 4, 16, 16], 'warp': [2, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: 14.476491928100586
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [2, 8, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: 17.30826187133789
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [8, 2, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: 23.089765548706055
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [4, 16, 16, 16], 'warp': [2, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:23<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: 8.493670463562012
Processing:  89%|████████▉ | 42/47 [12:24<02:05, 25.07s/it]                                                           2024-01-15 02:22:55 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [16, 4, 16, 16], 'warp': [8, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:24<02:05, 25.07s/it]                                                           2024-01-15 02:22:56 [ladder:DEBUG]: 10.315366744995117
Processing:  89%|████████▉ | 42/47 [12:24<02:05, 25.07s/it]                                                           2024-01-15 02:22:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [2, 4, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:24<02:05, 25.07s/it]                                                           2024-01-15 02:22:56 [ladder:DEBUG]: 20.35855484008789
Processing:  89%|████████▉ | 42/47 [12:24<02:05, 25.07s/it]                                                           2024-01-15 02:22:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [4, 2, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:24<02:05, 25.07s/it]                                                           2024-01-15 02:22:56 [ladder:DEBUG]: 22.468608856201172
Processing:  89%|████████▉ | 42/47 [12:24<02:05, 25.07s/it]                                                           2024-01-15 02:22:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [2, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:24<02:05, 25.07s/it]                                                           2024-01-15 02:22:56 [ladder:DEBUG]: 26.158695220947266
Processing:  89%|████████▉ | 42/47 [12:24<02:05, 25.07s/it]                                                           2024-01-15 02:22:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [1, 8, 16, 16], 'warp': [1, 2, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:24<02:05, 25.07s/it]                                                           2024-01-15 02:22:56 [ladder:DEBUG]: 30.32248306274414
Processing:  89%|████████▉ | 42/47 [12:25<02:05, 25.07s/it]                                                           2024-01-15 02:22:56 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [8, 1, 16, 16], 'warp': [2, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:25<02:05, 25.07s/it]                                                           2024-01-15 02:22:57 [ladder:DEBUG]: 44.542362213134766
Processing:  89%|████████▉ | 42/47 [12:25<02:05, 25.07s/it]                                                           2024-01-15 02:22:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [2, 16, 16, 16], 'warp': [1, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:25<02:05, 25.07s/it]                                                           2024-01-15 02:22:57 [ladder:DEBUG]: 15.357542037963867
Processing:  89%|████████▉ | 42/47 [12:25<02:05, 25.07s/it]                                                           2024-01-15 02:22:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [16, 2, 16, 16], 'warp': [8, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:25<02:05, 25.07s/it]                                                           2024-01-15 02:22:57 [ladder:DEBUG]: 23.971431732177734
Processing:  89%|████████▉ | 42/47 [12:25<02:05, 25.07s/it]                                                           2024-01-15 02:22:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [1, 4, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:25<02:05, 25.07s/it]                                                           2024-01-15 02:22:57 [ladder:DEBUG]: 34.15019607543945
Processing:  89%|████████▉ | 42/47 [12:25<02:05, 25.07s/it]                                                           2024-01-15 02:22:57 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [4, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:25<02:05, 25.07s/it]                                                           2024-01-15 02:22:58 [ladder:DEBUG]: 35.575809478759766
Processing:  89%|████████▉ | 42/47 [12:26<02:05, 25.07s/it]                                                           2024-01-15 02:22:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [1, 2, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:26<02:05, 25.07s/it]                                                           2024-01-15 02:22:58 [ladder:DEBUG]: 38.7602424621582
Processing:  89%|████████▉ | 42/47 [12:26<02:05, 25.07s/it]                                                           2024-01-15 02:22:58 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [2, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:26<02:05, 25.07s/it]                                                           2024-01-15 02:22:59 [ladder:DEBUG]: 36.72494125366211
Processing:  89%|████████▉ | 42/47 [12:27<02:05, 25.07s/it]                                                           2024-01-15 02:22:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [1, 1, 16, 16], 'warp': [1, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:27<02:05, 25.07s/it]                                                           2024-01-15 02:22:59 [ladder:DEBUG]: 46.77857208251953
Processing:  89%|████████▉ | 42/47 [12:27<02:05, 25.07s/it]                                                           2024-01-15 02:22:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [1, 16, 16, 16], 'warp': [1, 4, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:27<02:05, 25.07s/it]                                                           2024-01-15 02:22:59 [ladder:DEBUG]: 26.917888641357422
Processing:  89%|████████▉ | 42/47 [12:27<02:05, 25.07s/it]                                                           2024-01-15 02:22:59 [ladder:DEBUG]: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [16, 1, 16, 16], 'warp': [4, 1, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:27<02:05, 25.07s/it]                                                           2024-01-15 02:23:00 [ladder:DEBUG]: 48.2885627746582
Processing:  89%|████████▉ | 42/47 [12:28<02:05, 25.07s/it]                                                           2024-01-15 02:23:00 [ladder:DEBUG]: Best Config: {'globals': {'Rasterization': <NoRasterization>}, <Node, ladder_perfect_matmul_29>: {'block': [8, 16, 16, 16], 'warp': [4, 8, 16, 16], 'wmma': [16, 16, 16], 'use_cutlass': False, 'rstep': [32, 1], 'use_tc': '80', 'strides': {2: <Stride, 2, 16>}}}
Processing:  89%|████████▉ | 42/47 [12:28<02:05, 25.07s/it]                                                           2024-01-15 02:23:00 [ladder:INFO]: result: 7.049625396728516
Processing:  89%|████████▉ | 42/47 [12:28<02:05, 25.07s/it]                                                           2024-01-15 02:23:00 [ladder:INFO]: Tuning ['layout_transform_reshape_reshape_add_30']
Processing:  89%|████████▉ | 42/47 [12:28<02:05, 25.07s/it]                                                           2024-01-15 02:23:00 [ladder:INFO]: Fusion group created: 21 ['ladder_perfect_matmul_29', 'layout_transform_reshape_reshape_add_30']
Processing:  89%|████████▉ | 42/47 [12:28<02:05, 25.07s/it]Processing:  96%|█████████▌| 45/47 [12:28<00:42, 21.10s/it]Processing: 100%|██████████| 47/47 [12:28<00:00, 15.92s/it]
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'int'>
<class 'str'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'> <class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'float'>
<class 'str'>
<class 'float'>

2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented

2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented


2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented

2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented

2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented

2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented

2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented

2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented
2024-01-15 02:17:03 [ladder:ERROR]: Fail to create schedule for <Node, nn_batch_matmul_17__reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18>, the error is Schedule not implemented

Execution time summary:
 mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  
  33.8611      33.8659      33.8751      33.8466       0.0112   
               
Execution time summary:
 mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  
  33.8863      33.8976      33.9130      33.8457       0.0239   
               
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #0 tvmgen_welder_cast_multiply_0_mean_add_sqrt_divide_1:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 142.95 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #1 tvmgen_welder_multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 141.926 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #2 tvmgen_welder_ladder_perfect_matmul_3_layout_transform_reshape_reshape_reshape_transpose_4:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 1993.63 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #3 tvmgen_welder_multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 103.68 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #4 tvmgen_welder_ladder_perfect_matmul_6_layout_transform_reshape_reshape_reshape_transpose_7:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 347.955 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #5 tvmgen_welder_multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 61.8616 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #6 tvmgen_welder_nn_batch_matmul_9_reshape_divide_10:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 3241.27 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #7 tvmgen_welder_max_11_subtract_exp_12:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 2066.94 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #8 tvmgen_welder_sum_13_divide_cast_cast_reshape_14:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 84.224 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #9 tvmgen_welder_ladder_perfect_matmul_15:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 342.221 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #10 tvmgen_welder_layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 100.045 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #11 tvmgen_welder_nn_batch_matmul_17:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 1516.65 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #12 tvmgen_welder_reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 96.5759 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #13 tvmgen_welder_ladder_perfect_matmul_19_layout_transform_reshape_reshape_add_20:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 2041.34 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #14 tvmgen_welder_cast_multiply_21_mean_add_sqrt_divide_22:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 141.721 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #15 tvmgen_welder_multiply_cast_multiply_23:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 132.198 us/iter
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #16 tvmgen_welder_reshape_layout_transform_ladder_layout_transform_24:
[02:23:49] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 103.322 us/iter
[02:23:50] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #17 tvmgen_welder_ladder_perfect_matmul_25:
[02:23:50] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 6907.29 us/iter
[02:23:50] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #18 tvmgen_welder_layout_transform_reshape_reshape_26:
[02:23:50] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 311.398 us/iter
[02:23:50] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #19 tvmgen_welder_ladder_perfect_matmul_27:
[02:23:50] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 6914.15 us/iter
[02:23:50] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #20 tvmgen_welder_sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28:
[02:23:50] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 475.75 us/iter
[02:23:50] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:95: Op #21 tvmgen_welder_ladder_perfect_matmul_29_layout_transform_reshape_reshape_add_30:
[02:23:50] /home/t-leiwang/ladder_workspace/LadderTVM/src/runtime/graph_executor/debug/graph_executor_debug.cc:98: Iteration: 0: 7015.32 us/iter
Node Name                                                                                                                                            Ops                                                                                                                                                  Time(us)   Time(%)  Shape                Inputs  Outputs  Measurements(us)  
---------                                                                                                                                            ---                                                                                                                                                  --------   -------  -----                ------  -------  ----------------  
tvmgen_welder_ladder_perfect_matmul_29_layout_transform_reshape_reshape_add_30                                                                       tvmgen_welder_ladder_perfect_matmul_29_layout_transform_reshape_reshape_add_30                                                                       7015.321   20.463   (1, 4096, 8192)      3       1        [7015.321]        
tvmgen_welder_ladder_perfect_matmul_27                                                                                                               tvmgen_welder_ladder_perfect_matmul_27                                                                                                               6914.15    20.168   (256, 1792, 16, 16)  2       1        [6914.15]         
tvmgen_welder_ladder_perfect_matmul_25                                                                                                               tvmgen_welder_ladder_perfect_matmul_25                                                                                                               6907.29    20.148   (256, 1792, 16, 16)  2       1        [6907.29]         
tvmgen_welder_nn_batch_matmul_9_reshape_divide_10                                                                                                    tvmgen_welder_nn_batch_matmul_9_reshape_divide_10                                                                                                    3241.267   9.455    (1, 64, 4096, 4096)  2       1        [3241.267]        
tvmgen_welder_max_11_subtract_exp_12                                                                                                                 tvmgen_welder_max_11_subtract_exp_12                                                                                                                 2066.944   6.029    (1, 64, 4096, 4096)  2       1        [2066.944]        
tvmgen_welder_ladder_perfect_matmul_19_layout_transform_reshape_reshape_add_20                                                                       tvmgen_welder_ladder_perfect_matmul_19_layout_transform_reshape_reshape_add_20                                                                       2041.344   5.954    (1, 4096, 8192)      3       1        [2041.344]        
tvmgen_welder_ladder_perfect_matmul_3_layout_transform_reshape_reshape_reshape_transpose_4                                                           tvmgen_welder_ladder_perfect_matmul_3_layout_transform_reshape_reshape_reshape_transpose_4                                                           1993.626   5.815    (1, 64, 4096, 128)   2       1        [1993.626]        
tvmgen_welder_nn_batch_matmul_17                                                                                                                     tvmgen_welder_nn_batch_matmul_17                                                                                                                     1516.646   4.424    (64, 4096, 128)      2       1        [1516.646]        
tvmgen_welder_sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28                         tvmgen_welder_sigmoid_multiply_layout_transform_reshape_reshape_multiply_reshape_layout_transform_ladder_layout_transform_28                         475.75     1.388    (256, 1792, 16, 16)  2       1        [475.75]          
tvmgen_welder_ladder_perfect_matmul_6_layout_transform_reshape_reshape_reshape_transpose_7                                                           tvmgen_welder_ladder_perfect_matmul_6_layout_transform_reshape_reshape_reshape_transpose_7                                                           347.955    1.015    (1, 8, 4096, 128)    2       1        [347.955]         
tvmgen_welder_ladder_perfect_matmul_15                                                                                                               tvmgen_welder_ladder_perfect_matmul_15                                                                                                               342.221    0.998    (256, 64, 16, 16)    2       1        [342.221]         
tvmgen_welder_layout_transform_reshape_reshape_26                                                                                                    tvmgen_welder_layout_transform_reshape_reshape_26                                                                                                    311.398    0.908    (1, 4096, 28672)     1       1        [311.398]         
tvmgen_welder_cast_multiply_0_mean_add_sqrt_divide_1                                                                                                 tvmgen_welder_cast_multiply_0_mean_add_sqrt_divide_1                                                                                                 142.95     0.417    (1, 4096, 8192)      1       2        [142.95]          
tvmgen_welder_cast_multiply_0_mean_add_sqrt_divide_1                                                                                                 tvmgen_welder_cast_multiply_0_mean_add_sqrt_divide_1                                                                                                 142.95     0.417    (1, 4096, 1)         1       2        [142.95]          
tvmgen_welder_multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2                                                              tvmgen_welder_multiply_cast_multiply_reshape_layout_transform_ladder_layout_transform_2                                                              141.926    0.414    (256, 512, 16, 16)   3       1        [141.926]         
tvmgen_welder_cast_multiply_21_mean_add_sqrt_divide_22                                                                                               tvmgen_welder_cast_multiply_21_mean_add_sqrt_divide_22                                                                                               141.722    0.413    (1, 4096, 8192)      1       2        [141.721]         
tvmgen_welder_cast_multiply_21_mean_add_sqrt_divide_22                                                                                               tvmgen_welder_cast_multiply_21_mean_add_sqrt_divide_22                                                                                               141.722    0.413    (1, 4096, 1)         1       2        [141.721]         
tvmgen_welder_multiply_cast_multiply_23                                                                                                              tvmgen_welder_multiply_cast_multiply_23                                                                                                              132.198    0.386    (1, 4096, 8192)      3       1        [132.198]         
tvmgen_welder_multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5                                                       tvmgen_welder_multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_reshape_5                                                       103.68     0.302    (64, 4096, 128)      3       1        [103.68]          
tvmgen_welder_reshape_layout_transform_ladder_layout_transform_24                                                                                    tvmgen_welder_reshape_layout_transform_ladder_layout_transform_24                                                                                    103.322    0.301    (256, 512, 16, 16)   1       1        [103.322]         
tvmgen_welder_layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16                               tvmgen_welder_layout_transform_reshape_reshape_reshape_transpose_expand_dims_broadcast_to_reshape_reshape_transpose_16                               100.045    0.292    (64, 128, 4096)      1       1        [100.045]         
tvmgen_welder_reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18                                                          tvmgen_welder_reshape_transpose_reshape_reshape_layout_transform_ladder_layout_transform_18                                                          96.576     0.282    (256, 512, 16, 16)   1       1        [96.576]          
tvmgen_welder_sum_13_divide_cast_cast_reshape_14                                                                                                     tvmgen_welder_sum_13_divide_cast_cast_reshape_14                                                                                                     84.224     0.246    (64, 4096, 4096)     2       1        [84.224]          
tvmgen_welder_multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8  tvmgen_welder_multiply_strided_slice_negative_strided_slice_concatenate_multiply_add_expand_dims_broadcast_to_reshape_transpose_reshape_transpose_8  61.862     0.18     (64, 4096, 128)      3       1        [61.862]          
Total_time                                                                                                                                           -                                                                                                                                                    34282.417  -        -                    -       -        -                 
