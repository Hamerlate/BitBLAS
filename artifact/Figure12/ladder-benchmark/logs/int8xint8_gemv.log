int8xint8_gemv.py
2024-05-06 13:25:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 7], 'thread': [1, 7], 'rstep': [2048], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}
2024-05-06 13:25:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 8], 'thread': [1, 8], 'rstep': [2048], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}
2024-05-06 13:25:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 14], 'thread': [1, 14], 'rstep': [1024], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}
2024-05-06 13:25:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 4], 'thread': [1, 4], 'rstep': [4096], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}
2024-05-06 13:25:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 16], 'thread': [1, 16], 'rstep': [1024], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'A': 8, 'B': 16}}
2024-05-06 13:25:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 2], 'thread': [1, 2], 'rstep': [8192], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}
2024-05-06 13:25:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 56], 'thread': [1, 56], 'rstep': [256], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}
2024-05-06 13:25:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 28], 'thread': [1, 28], 'rstep': [512], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}
2024-05-06 13:25:00 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 64], 'thread': [1, 64], 'rstep': [256], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B': 16}}
2024-05-06 13:25:01 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 32], 'thread': [1, 32], 'rstep': [512], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B': 16}}
2024-05-06 13:25:01 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 1], 'thread': [1, 1], 'rstep': [8192], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}
2024-05-06 13:25:01 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_simt.TIRSIMTScheduler'> config: {'block': [1, 112], 'thread': [1, 112], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}
2024-05-06 13:25:01 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_simt.TIRSIMTScheduler'> config: {'block': [1, 128], 'thread': [1, 128], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}
2024-05-06 13:25:01 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_simt.TIRSIMTScheduler'> config: {'block': [1, 224], 'thread': [1, 112], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}
2024-05-06 13:25:01 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_simt.TIRSIMTScheduler'> config: {'block': [1, 256], 'thread': [1, 128], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}
2024-05-06 13:25:01 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_simt.TIRSIMTScheduler'> config: {'block': [1, 448], 'thread': [1, 112], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}
{<Node, ladder_matmul>: {'block': [1, 7], 'thread': [1, 7], 'rstep': [2048], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}}
0.5404160022735596
{<Node, ladder_matmul>: {'block': [1, 8], 'thread': [1, 8], 'rstep': [2048], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}}
0.5358933210372925
{<Node, ladder_matmul>: {'block': [1, 14], 'thread': [1, 14], 'rstep': [1024], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}}
0.5966506600379944
{<Node, ladder_matmul>: {'block': [1, 4], 'thread': [1, 4], 'rstep': [4096], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}}
0.5070263147354126
{<Node, ladder_matmul>: {'block': [1, 16], 'thread': [1, 16], 'rstep': [1024], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'A': 8, 'B': 16}}}
0.5703679919242859
{<Node, ladder_matmul>: {'block': [1, 2], 'thread': [1, 2], 'rstep': [8192], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}}
0.49488455057144165
{<Node, ladder_matmul>: {'block': [1, 56], 'thread': [1, 56], 'rstep': [256], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}}
1.0521600246429443
{<Node, ladder_matmul>: {'block': [1, 28], 'thread': [1, 28], 'rstep': [512], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}}
0.6709247827529907
{<Node, ladder_matmul>: {'block': [1, 64], 'thread': [1, 64], 'rstep': [256], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B': 16}}}
1.0593279600143433
{<Node, ladder_matmul>: {'block': [1, 32], 'thread': [1, 32], 'rstep': [512], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B': 16}}}
0.6699007749557495
{<Node, ladder_matmul>: {'block': [1, 1], 'thread': [1, 1], 'rstep': [8192], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}}
0.49459198117256165
{<Node, ladder_matmul>: {'block': [1, 112], 'thread': [1, 112], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}}
10.376397132873535
{<Node, ladder_matmul>: {'block': [1, 128], 'thread': [1, 128], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}}
11.573247909545898
{<Node, ladder_matmul>: {'block': [1, 224], 'thread': [1, 112], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}}
11.510784149169922
{<Node, ladder_matmul>: {'block': [1, 256], 'thread': [1, 128], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}}
12.877619743347168
{<Node, ladder_matmul>: {'block': [1, 448], 'thread': [1, 112], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}}
100000000.0
top1: 0.5404160022735596 	top10: 0.49459198117256165
--------------------------------------------------------------------------------
best config: {<Node, ladder_matmul>: {'block': [1, 1], 'thread': [1, 1], 'rstep': [8192], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}}
best latency: 0.49459198117256165
best code: __global__ void __launch_bounds__(128) Fused(int8_t* __restrict__ A, int8_t* __restrict__ B, int* __restrict__ C) {
  
  int in_thread_C_local[1];
  signed char A_local[16];
  signed char B_local[16];
  __shared__ int red_buf0[128];
  in_thread_C_local[0] = 0;
  for (int k_0 = 0; k_0 < 28; ++k_0) {
    *(int4*)(A_local + 0) = *(int4*)(A + ((k_0 * 2048) + (((int)threadIdx.x) * 16)));
    *(int4*)(B_local + 0) = *(int4*)(B + (((((int)blockIdx.x) * 57344) + (k_0 * 2048)) + (((int)threadIdx.x) * 16)));
    for (int k_2 = 0; k_2 < 16; ++k_2) {
      in_thread_C_local[0] = (in_thread_C_local[0] + (((int)A_local[k_2]) * ((int)B_local[k_2])));
    }
  }
  __syncthreads();
  ((volatile int*)red_buf0)[((int)threadIdx.x)] = in_thread_C_local[0];
  __syncthreads();
  if (((int)threadIdx.x) < 64) {
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 64)]);
  }
  __syncthreads();
  if (((int)threadIdx.x) < 32) {
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 32)]);
  }
  __syncthreads();
  if (((int)threadIdx.x) < 16) {
    int w_16_0 = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 16)]);
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = w_16_0;
    int w_8_0 = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 8)]);
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = w_8_0;
    int w_4_0 = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 4)]);
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = w_4_0;
    int w_2_0 = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 2)]);
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = w_2_0;
    int w_1_0 = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 1)]);
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = w_1_0;
  }
  __syncthreads();
  C[((int)blockIdx.x)] = ((volatile int*)red_buf0)[0];
}


2024-05-06 13:25:05 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 4], 'thread': [1, 4], 'rstep': [4096], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}
2024-05-06 13:25:05 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 8], 'thread': [1, 8], 'rstep': [2048], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}
2024-05-06 13:25:05 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 2], 'thread': [1, 2], 'rstep': [7168], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 8, 'B': 16}}
2024-05-06 13:25:05 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 32], 'thread': [1, 32], 'rstep': [512], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B': 16}}
2024-05-06 13:25:05 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 16], 'thread': [1, 16], 'rstep': [1024], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'A': 8, 'B': 16}}
2024-05-06 13:25:06 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 1], 'thread': [1, 1], 'rstep': [7168], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 8, 'B': 8}}
2024-05-06 13:25:06 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_reduce_interthread.TIRReduceInterThreadScheduler'> config: {'block': [1, 64], 'thread': [1, 64], 'rstep': [256], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B': 16}}
2024-05-06 13:25:06 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_simt.TIRSIMTScheduler'> config: {'block': [1, 128], 'thread': [1, 128], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}
2024-05-06 13:25:06 [ladder:DEBUG]: Using template: <class 'ladder.schedule.tir_simt.TIRSIMTScheduler'> config: {'block': [1, 256], 'thread': [1, 128], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}
{<Node, ladder_matmul>: {'block': [1, 4], 'thread': [1, 4], 'rstep': [4096], 'reduce_thread': [32], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}}
0.1592320054769516
{<Node, ladder_matmul>: {'block': [1, 8], 'thread': [1, 8], 'rstep': [2048], 'reduce_thread': [16], 'block_order': <NoRasterization>, 'vectorize': {'A': 16, 'B': 16}}}
0.1563306599855423
{<Node, ladder_matmul>: {'block': [1, 2], 'thread': [1, 2], 'rstep': [7168], 'reduce_thread': [64], 'block_order': <NoRasterization>, 'vectorize': {'A': 8, 'B': 16}}}
0.15069866180419922
{<Node, ladder_matmul>: {'block': [1, 32], 'thread': [1, 32], 'rstep': [512], 'reduce_thread': [4], 'block_order': <NoRasterization>, 'vectorize': {'A': 4, 'B': 16}}}
0.24081067740917206
{<Node, ladder_matmul>: {'block': [1, 16], 'thread': [1, 16], 'rstep': [1024], 'reduce_thread': [8], 'block_order': <NoRasterization>, 'vectorize': {'A': 8, 'B': 16}}}
0.17083734273910522
{<Node, ladder_matmul>: {'block': [1, 1], 'thread': [1, 1], 'rstep': [7168], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 8, 'B': 8}}}
0.15052799880504608
{<Node, ladder_matmul>: {'block': [1, 64], 'thread': [1, 64], 'rstep': [256], 'reduce_thread': [2], 'block_order': <NoRasterization>, 'vectorize': {'A': 2, 'B': 16}}}
0.4155733287334442
{<Node, ladder_matmul>: {'block': [1, 128], 'thread': [1, 128], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}}
2.996633529663086
{<Node, ladder_matmul>: {'block': [1, 256], 'thread': [1, 128], 'rstep': [128], 'block_order': <NoRasterization>, 'vectorize': {'B': 16}}}
6.439526557922363
top1: 0.1592320054769516 	top10: 0.15052799880504608
--------------------------------------------------------------------------------
best config: {<Node, ladder_matmul>: {'block': [1, 1], 'thread': [1, 1], 'rstep': [7168], 'reduce_thread': [128], 'block_order': <NoRasterization>, 'vectorize': {'A': 8, 'B': 8}}}
best latency: 0.15052799880504608
best code: __global__ void __launch_bounds__(128) Fused(int8_t* __restrict__ A, int8_t* __restrict__ B, int* __restrict__ C) {
  
  int in_thread_C_local[1];
  signed char A_local[8];
  signed char B_local[8];
  __shared__ int red_buf0[128];
  in_thread_C_local[0] = 0;
  for (int k_0 = 0; k_0 < 28; ++k_0) {
    *(int2*)(A_local + 0) = *(int2*)(A + ((k_0 * 1024) + (((int)threadIdx.x) * 8)));
    *(int2*)(B_local + 0) = *(int2*)(B + (((((int)blockIdx.x) * 28672) + (k_0 * 1024)) + (((int)threadIdx.x) * 8)));
    for (int k_2 = 0; k_2 < 8; ++k_2) {
      in_thread_C_local[0] = (in_thread_C_local[0] + (((int)A_local[k_2]) * ((int)B_local[k_2])));
    }
  }
  __syncthreads();
  ((volatile int*)red_buf0)[((int)threadIdx.x)] = in_thread_C_local[0];
  __syncthreads();
  if (((int)threadIdx.x) < 64) {
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 64)]);
  }
  __syncthreads();
  if (((int)threadIdx.x) < 32) {
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 32)]);
  }
  __syncthreads();
  if (((int)threadIdx.x) < 16) {
    int w_16_0 = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 16)]);
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = w_16_0;
    int w_8_0 = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 8)]);
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = w_8_0;
    int w_4_0 = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 4)]);
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = w_4_0;
    int w_2_0 = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 2)]);
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = w_2_0;
    int w_1_0 = (((volatile int*)red_buf0)[((int)threadIdx.x)] + ((volatile int*)red_buf0)[(((int)threadIdx.x) + 1)]);
    ((volatile int*)red_buf0)[((int)threadIdx.x)] = w_1_0;
  }
  __syncthreads();
  C[((int)blockIdx.x)] = ((volatile int*)red_buf0)[0];
}


1_14336_57344	0.49459198117256165
1_8192_28672	0.15052799880504608
