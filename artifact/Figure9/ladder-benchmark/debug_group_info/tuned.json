[
  {
    "nodes": [
      0
    ],
    "node_names": [
      "layout_transform_0"
    ],
    "group_id": 0,
    "input_desc": [
      [
        "layout_transform_0",
        0,
        0
      ]
    ],
    "output_desc": [
      [
        "layout_transform_0",
        0,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group0(half* __restrict__ p0, half* __restrict__ T_layout_trans) {\n  \n  T_layout_trans[(((((((int)blockIdx.x) / 7) * 2688) + ((((int)threadIdx.x) >> 5) * 672)) + ((((int)blockIdx.x) % 7) * 96)) + ((((int)threadIdx.x) & 31) * 3))] = p0[(((((((int)blockIdx.x) / 7) * 896) + ((((int)threadIdx.x) >> 5) * 224)) + ((((int)blockIdx.x) % 7) * 32)) + (((int)threadIdx.x) & 31))];\n  T_layout_trans[((((((((int)blockIdx.x) / 7) * 2688) + ((((int)threadIdx.x) >> 5) * 672)) + ((((int)blockIdx.x) % 7) * 96)) + ((((int)threadIdx.x) & 31) * 3)) + 1)] = p0[((((((((int)blockIdx.x) / 7) * 896) + ((((int)threadIdx.x) >> 5) * 224)) + ((((int)blockIdx.x) % 7) * 32)) + (((int)threadIdx.x) & 31)) + 50176)];\n  T_layout_trans[((((((((int)blockIdx.x) / 7) * 2688) + ((((int)threadIdx.x) >> 5) * 672)) + ((((int)blockIdx.x) % 7) * 96)) + ((((int)threadIdx.x) & 31) * 3)) + 2)] = p0[((((((((int)blockIdx.x) / 7) * 896) + ((((int)threadIdx.x) >> 5) * 224)) + ((((int)blockIdx.x) % 7) * 32)) + (((int)threadIdx.x) & 31)) + 100352)];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      392,
      1,
      1
    ],
    "latency": 0.0034917648881673813,
    "name": "Group0",
    "gain": 0
  },
  {
    "nodes": [
      1,
      2
    ],
    "node_names": [
      "nn_conv2d_add_1",
      "layout_transform_reshape_transpose_concatenate_add_2"
    ],
    "group_id": 1,
    "input_desc": [
      [
        "nn_conv2d_add_1",
        1,
        0
      ],
      [
        "nn_conv2d_add_1",
        1,
        1
      ],
      [
        "nn_conv2d_add_1",
        1,
        2
      ],
      [
        "layout_transform_reshape_transpose_concatenate_add_2",
        2,
        1
      ],
      [
        "layout_transform_reshape_transpose_concatenate_add_2",
        2,
        2
      ]
    ],
    "output_desc": [
      [
        "layout_transform_reshape_transpose_concatenate_add_2",
        2,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group1(half* __restrict__ input0, half* __restrict__ input1, half* __restrict__ input2, half* __restrict__ input3, half* __restrict__ input4, half* __restrict__ output0) {\n  \n  half mediate1_local[1];\n  __shared__ half mediate0_shared[96];\n  __shared__ half input1_shared[96];\n  mediate1_local[0] = __float2half_rn(0.000000e+00f);\n  for (int ry_outer = 0; ry_outer < 32; ++ry_outer) {\n    __syncthreads();\n    mediate0_shared[((int)threadIdx.x)] = input0[(((((((((int)blockIdx.x) + 18432) % 18816) / 2688) * 21504) + (ry_outer * 672)) + ((((((int)blockIdx.x) / 384) + 6) % 7) * 96)) + ((int)threadIdx.x))];\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_s < 8; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) {\n      if (((int)threadIdx.x) < 12) {\n        input1_shared[((((int)threadIdx.x) * 8) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s)] = input1[((((ry_outer * 36864) + (((int)threadIdx.x) * 3072)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_s * 384)) + ((((((((int)blockIdx.x) - 384) / 18816) + (((((int)blockIdx.x) - 384) % 18816) >> 31)) + (((int)blockIdx.x) % 384)) % 384) + (384 & ((((((((int)blockIdx.x) - 384) / 18816) + (((((int)blockIdx.x) - 384) % 18816) >> 31)) + (((int)blockIdx.x) % 384)) % 384) >> 31))))];\n      }\n    }\n    __syncthreads();\n    for (int rx_inner = 0; rx_inner < 32; ++rx_inner) {\n      for (int rc_inner = 0; rc_inner < 3; ++rc_inner) {\n        mediate1_local[0] = (mediate1_local[0] + (mediate0_shared[((rx_inner * 3) + rc_inner)] * input1_shared[((rx_inner * 3) + rc_inner)]));\n      }\n    }\n  }\n  output0[((int)blockIdx.x)] = (((384 <= ((int)blockIdx.x)) ? (mediate1_local[0] + input2[(((int)blockIdx.x) % 384)]) : input3[((int)blockIdx.x)]) + input4[((int)blockIdx.x)]);\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      196,
      1,
      1
    ],
    "latency": 0.03338240087032318,
    "name": "Group1",
    "gain": 0.027487088227644563
  },
  {
    "nodes": [
      3
    ],
    "node_names": [
      "mean_3"
    ],
    "group_id": 2,
    "input_desc": [
      [
        "mean_3",
        3,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_3",
        3,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      4,
      5,
      6
    ],
    "node_names": [
      "subtract_multiply_4",
      "mean_add_sqrt_5",
      "divide_multiply_add_reshape_6"
    ],
    "group_id": 3,
    "input_desc": [
      [
        "subtract_multiply_4",
        4,
        0
      ],
      [
        "subtract_multiply_4",
        4,
        1
      ],
      [
        "divide_multiply_add_reshape_6",
        6,
        2
      ],
      [
        "divide_multiply_add_reshape_6",
        6,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_6",
        6,
        0
      ]
    ],
    "code": "__device__ void Group3_0_subtract_multiply_4(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group3_1_mean_add_sqrt_5(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group3_2_divide_multiply_add_reshape_6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group3(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group3_0_subtract_multiply_4(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group3_1_mean_add_sqrt_5((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group3_2_divide_multiply_add_reshape_6((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005529600195586681,
    "name": "Group3",
    "gain": 0.0030750336591154337
  },
  {
    "nodes": [
      7
    ],
    "node_names": [
      "ladder_quant_linear_7"
    ],
    "group_id": 4,
    "input_desc": [
      [
        "ladder_quant_linear_7",
        7,
        0
      ],
      [
        "ladder_quant_linear_7",
        7,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_7",
        7,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      8
    ],
    "node_names": [
      "reshape_add_reshape_transpose_8"
    ],
    "group_id": 5,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_8",
        8,
        0
      ],
      [
        "reshape_add_reshape_transpose_8",
        8,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_8",
        8,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      9
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_9"
    ],
    "group_id": 6,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_9",
        9,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_9",
        9,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_9",
        9,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      10
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_10"
    ],
    "group_id": 7,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_10",
        10,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_10",
        10,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_10",
        10,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      11,
      12
    ],
    "node_names": [
      "nn_batch_matmul_11",
      "reshape_12"
    ],
    "group_id": 8,
    "input_desc": [
      [
        "nn_batch_matmul_11",
        11,
        0
      ],
      [
        "nn_batch_matmul_11",
        11,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_12",
        12,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      13,
      14,
      15,
      16
    ],
    "node_names": [
      "max_13",
      "subtract_exp_14",
      "sum_15",
      "divide_reshape_16"
    ],
    "group_id": 9,
    "input_desc": [
      [
        "subtract_exp_14",
        14,
        0
      ],
      [
        "max_13",
        13,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_16",
        16,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      17,
      18
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_17",
      "nn_batch_matmul_18"
    ],
    "group_id": 10,
    "input_desc": [
      [
        "nn_batch_matmul_18",
        18,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_17",
        17,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_18",
        18,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      19
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_19"
    ],
    "group_id": 11,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_19",
        19,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_19",
        19,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      20
    ],
    "node_names": [
      "ladder_quant_linear_20"
    ],
    "group_id": 12,
    "input_desc": [
      [
        "ladder_quant_linear_20",
        20,
        0
      ],
      [
        "ladder_quant_linear_20",
        20,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_20",
        20,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      21
    ],
    "node_names": [
      "reshape_add_add_21"
    ],
    "group_id": 13,
    "input_desc": [
      [
        "reshape_add_add_21",
        21,
        0
      ],
      [
        "reshape_add_add_21",
        21,
        1
      ],
      [
        "reshape_add_add_21",
        21,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_21",
        21,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      22
    ],
    "node_names": [
      "mean_22"
    ],
    "group_id": 14,
    "input_desc": [
      [
        "mean_22",
        22,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_22",
        22,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      23,
      24,
      25
    ],
    "node_names": [
      "subtract_multiply_23",
      "mean_add_sqrt_24",
      "divide_multiply_add_reshape_25"
    ],
    "group_id": 15,
    "input_desc": [
      [
        "subtract_multiply_23",
        23,
        0
      ],
      [
        "subtract_multiply_23",
        23,
        1
      ],
      [
        "divide_multiply_add_reshape_25",
        25,
        2
      ],
      [
        "divide_multiply_add_reshape_25",
        25,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_25",
        25,
        0
      ]
    ],
    "code": "__device__ void Group15_0_subtract_multiply_23(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group15_1_mean_add_sqrt_24(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group15_2_divide_multiply_add_reshape_25(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group15(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group15_0_subtract_multiply_23(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group15_1_mean_add_sqrt_24((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group15_2_divide_multiply_add_reshape_25((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005529600195586681,
    "name": "Group15",
    "gain": 0.0031527478713542223
  },
  {
    "nodes": [
      26
    ],
    "node_names": [
      "ladder_quant_linear_26"
    ],
    "group_id": 16,
    "input_desc": [
      [
        "ladder_quant_linear_26",
        26,
        0
      ],
      [
        "ladder_quant_linear_26",
        26,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_26",
        26,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      27,
      28
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_27",
      "multiply_reshape_28"
    ],
    "group_id": 17,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_27",
        27,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_27",
        27,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_28",
        28,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      29
    ],
    "node_names": [
      "ladder_quant_linear_29"
    ],
    "group_id": 18,
    "input_desc": [
      [
        "ladder_quant_linear_29",
        29,
        0
      ],
      [
        "ladder_quant_linear_29",
        29,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_29",
        29,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      30
    ],
    "node_names": [
      "reshape_add_add_30"
    ],
    "group_id": 19,
    "input_desc": [
      [
        "reshape_add_add_30",
        30,
        0
      ],
      [
        "reshape_add_add_30",
        30,
        1
      ],
      [
        "reshape_add_add_30",
        30,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_30",
        30,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      31
    ],
    "node_names": [
      "mean_31"
    ],
    "group_id": 20,
    "input_desc": [
      [
        "mean_31",
        31,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_31",
        31,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      32,
      33,
      34
    ],
    "node_names": [
      "subtract_multiply_32",
      "mean_add_sqrt_33",
      "divide_multiply_add_reshape_34"
    ],
    "group_id": 21,
    "input_desc": [
      [
        "subtract_multiply_32",
        32,
        0
      ],
      [
        "subtract_multiply_32",
        32,
        1
      ],
      [
        "divide_multiply_add_reshape_34",
        34,
        2
      ],
      [
        "divide_multiply_add_reshape_34",
        34,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_34",
        34,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      35
    ],
    "node_names": [
      "ladder_quant_linear_35"
    ],
    "group_id": 22,
    "input_desc": [
      [
        "ladder_quant_linear_35",
        35,
        0
      ],
      [
        "ladder_quant_linear_35",
        35,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_35",
        35,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      36
    ],
    "node_names": [
      "reshape_add_reshape_transpose_36"
    ],
    "group_id": 23,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_36",
        36,
        0
      ],
      [
        "reshape_add_reshape_transpose_36",
        36,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_36",
        36,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      37
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_37"
    ],
    "group_id": 24,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_37",
        37,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_37",
        37,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_37",
        37,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      38
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_38"
    ],
    "group_id": 25,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_38",
        38,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_38",
        38,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_38",
        38,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      39,
      40
    ],
    "node_names": [
      "nn_batch_matmul_39",
      "reshape_40"
    ],
    "group_id": 26,
    "input_desc": [
      [
        "nn_batch_matmul_39",
        39,
        0
      ],
      [
        "nn_batch_matmul_39",
        39,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_40",
        40,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      41,
      42,
      43,
      44
    ],
    "node_names": [
      "max_41",
      "subtract_exp_42",
      "sum_43",
      "divide_reshape_44"
    ],
    "group_id": 27,
    "input_desc": [
      [
        "subtract_exp_42",
        42,
        0
      ],
      [
        "max_41",
        41,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_44",
        44,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      45,
      46
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_45",
      "nn_batch_matmul_46"
    ],
    "group_id": 28,
    "input_desc": [
      [
        "nn_batch_matmul_46",
        46,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_45",
        45,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_46",
        46,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      47
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_47"
    ],
    "group_id": 29,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_47",
        47,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_47",
        47,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      48
    ],
    "node_names": [
      "ladder_quant_linear_48"
    ],
    "group_id": 30,
    "input_desc": [
      [
        "ladder_quant_linear_48",
        48,
        0
      ],
      [
        "ladder_quant_linear_48",
        48,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_48",
        48,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      49
    ],
    "node_names": [
      "reshape_add_add_49"
    ],
    "group_id": 31,
    "input_desc": [
      [
        "reshape_add_add_49",
        49,
        0
      ],
      [
        "reshape_add_add_49",
        49,
        1
      ],
      [
        "reshape_add_add_49",
        49,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_49",
        49,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      50
    ],
    "node_names": [
      "mean_50"
    ],
    "group_id": 32,
    "input_desc": [
      [
        "mean_50",
        50,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_50",
        50,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      51,
      52,
      53
    ],
    "node_names": [
      "subtract_multiply_51",
      "mean_add_sqrt_52",
      "divide_multiply_add_reshape_53"
    ],
    "group_id": 33,
    "input_desc": [
      [
        "subtract_multiply_51",
        51,
        0
      ],
      [
        "subtract_multiply_51",
        51,
        1
      ],
      [
        "divide_multiply_add_reshape_53",
        53,
        2
      ],
      [
        "divide_multiply_add_reshape_53",
        53,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_53",
        53,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      54
    ],
    "node_names": [
      "ladder_quant_linear_54"
    ],
    "group_id": 34,
    "input_desc": [
      [
        "ladder_quant_linear_54",
        54,
        0
      ],
      [
        "ladder_quant_linear_54",
        54,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_54",
        54,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      55,
      56
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_55",
      "multiply_reshape_56"
    ],
    "group_id": 35,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_55",
        55,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_55",
        55,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_56",
        56,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      57
    ],
    "node_names": [
      "ladder_quant_linear_57"
    ],
    "group_id": 36,
    "input_desc": [
      [
        "ladder_quant_linear_57",
        57,
        0
      ],
      [
        "ladder_quant_linear_57",
        57,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_57",
        57,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      58
    ],
    "node_names": [
      "reshape_add_add_58"
    ],
    "group_id": 37,
    "input_desc": [
      [
        "reshape_add_add_58",
        58,
        0
      ],
      [
        "reshape_add_add_58",
        58,
        1
      ],
      [
        "reshape_add_add_58",
        58,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_58",
        58,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      59
    ],
    "node_names": [
      "mean_59"
    ],
    "group_id": 38,
    "input_desc": [
      [
        "mean_59",
        59,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_59",
        59,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      60,
      61,
      62
    ],
    "node_names": [
      "subtract_multiply_60",
      "mean_add_sqrt_61",
      "divide_multiply_add_reshape_62"
    ],
    "group_id": 39,
    "input_desc": [
      [
        "subtract_multiply_60",
        60,
        0
      ],
      [
        "subtract_multiply_60",
        60,
        1
      ],
      [
        "divide_multiply_add_reshape_62",
        62,
        2
      ],
      [
        "divide_multiply_add_reshape_62",
        62,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_62",
        62,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      63
    ],
    "node_names": [
      "ladder_quant_linear_63"
    ],
    "group_id": 40,
    "input_desc": [
      [
        "ladder_quant_linear_63",
        63,
        0
      ],
      [
        "ladder_quant_linear_63",
        63,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_63",
        63,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      64
    ],
    "node_names": [
      "reshape_add_reshape_transpose_64"
    ],
    "group_id": 41,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_64",
        64,
        0
      ],
      [
        "reshape_add_reshape_transpose_64",
        64,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_64",
        64,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      65
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_65"
    ],
    "group_id": 42,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_65",
        65,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_65",
        65,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_65",
        65,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      66
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_66"
    ],
    "group_id": 43,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_66",
        66,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_66",
        66,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_66",
        66,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      67,
      68
    ],
    "node_names": [
      "nn_batch_matmul_67",
      "reshape_68"
    ],
    "group_id": 44,
    "input_desc": [
      [
        "nn_batch_matmul_67",
        67,
        0
      ],
      [
        "nn_batch_matmul_67",
        67,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_68",
        68,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      69,
      70,
      71,
      72
    ],
    "node_names": [
      "max_69",
      "subtract_exp_70",
      "sum_71",
      "divide_reshape_72"
    ],
    "group_id": 45,
    "input_desc": [
      [
        "subtract_exp_70",
        70,
        0
      ],
      [
        "max_69",
        69,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_72",
        72,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      73,
      74
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_73",
      "nn_batch_matmul_74"
    ],
    "group_id": 46,
    "input_desc": [
      [
        "nn_batch_matmul_74",
        74,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_73",
        73,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_74",
        74,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      75
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_75"
    ],
    "group_id": 47,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_75",
        75,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_75",
        75,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      76
    ],
    "node_names": [
      "ladder_quant_linear_76"
    ],
    "group_id": 48,
    "input_desc": [
      [
        "ladder_quant_linear_76",
        76,
        0
      ],
      [
        "ladder_quant_linear_76",
        76,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_76",
        76,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      77
    ],
    "node_names": [
      "reshape_add_add_77"
    ],
    "group_id": 49,
    "input_desc": [
      [
        "reshape_add_add_77",
        77,
        0
      ],
      [
        "reshape_add_add_77",
        77,
        1
      ],
      [
        "reshape_add_add_77",
        77,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_77",
        77,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      78
    ],
    "node_names": [
      "mean_78"
    ],
    "group_id": 50,
    "input_desc": [
      [
        "mean_78",
        78,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_78",
        78,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      79,
      80,
      81
    ],
    "node_names": [
      "subtract_multiply_79",
      "mean_add_sqrt_80",
      "divide_multiply_add_reshape_81"
    ],
    "group_id": 51,
    "input_desc": [
      [
        "subtract_multiply_79",
        79,
        0
      ],
      [
        "subtract_multiply_79",
        79,
        1
      ],
      [
        "divide_multiply_add_reshape_81",
        81,
        2
      ],
      [
        "divide_multiply_add_reshape_81",
        81,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_81",
        81,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      82
    ],
    "node_names": [
      "ladder_quant_linear_82"
    ],
    "group_id": 52,
    "input_desc": [
      [
        "ladder_quant_linear_82",
        82,
        0
      ],
      [
        "ladder_quant_linear_82",
        82,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_82",
        82,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      83,
      84
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_83",
      "multiply_reshape_84"
    ],
    "group_id": 53,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_83",
        83,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_83",
        83,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_84",
        84,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      85
    ],
    "node_names": [
      "ladder_quant_linear_85"
    ],
    "group_id": 54,
    "input_desc": [
      [
        "ladder_quant_linear_85",
        85,
        0
      ],
      [
        "ladder_quant_linear_85",
        85,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_85",
        85,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      86
    ],
    "node_names": [
      "reshape_add_add_86"
    ],
    "group_id": 55,
    "input_desc": [
      [
        "reshape_add_add_86",
        86,
        0
      ],
      [
        "reshape_add_add_86",
        86,
        1
      ],
      [
        "reshape_add_add_86",
        86,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_86",
        86,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      87
    ],
    "node_names": [
      "mean_87"
    ],
    "group_id": 56,
    "input_desc": [
      [
        "mean_87",
        87,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_87",
        87,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      88,
      89,
      90
    ],
    "node_names": [
      "subtract_multiply_88",
      "mean_add_sqrt_89",
      "divide_multiply_add_reshape_90"
    ],
    "group_id": 57,
    "input_desc": [
      [
        "subtract_multiply_88",
        88,
        0
      ],
      [
        "subtract_multiply_88",
        88,
        1
      ],
      [
        "divide_multiply_add_reshape_90",
        90,
        2
      ],
      [
        "divide_multiply_add_reshape_90",
        90,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_90",
        90,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      91
    ],
    "node_names": [
      "ladder_quant_linear_91"
    ],
    "group_id": 58,
    "input_desc": [
      [
        "ladder_quant_linear_91",
        91,
        0
      ],
      [
        "ladder_quant_linear_91",
        91,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_91",
        91,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      92
    ],
    "node_names": [
      "reshape_add_reshape_transpose_92"
    ],
    "group_id": 59,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_92",
        92,
        0
      ],
      [
        "reshape_add_reshape_transpose_92",
        92,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_92",
        92,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      93
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_93"
    ],
    "group_id": 60,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_93",
        93,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_93",
        93,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_93",
        93,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      94
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_94"
    ],
    "group_id": 61,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_94",
        94,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_94",
        94,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_94",
        94,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      95,
      96
    ],
    "node_names": [
      "nn_batch_matmul_95",
      "reshape_96"
    ],
    "group_id": 62,
    "input_desc": [
      [
        "nn_batch_matmul_95",
        95,
        0
      ],
      [
        "nn_batch_matmul_95",
        95,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_96",
        96,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      97,
      98,
      99,
      100
    ],
    "node_names": [
      "max_97",
      "subtract_exp_98",
      "sum_99",
      "divide_reshape_100"
    ],
    "group_id": 63,
    "input_desc": [
      [
        "subtract_exp_98",
        98,
        0
      ],
      [
        "max_97",
        97,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_100",
        100,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      101,
      102
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_101",
      "nn_batch_matmul_102"
    ],
    "group_id": 64,
    "input_desc": [
      [
        "nn_batch_matmul_102",
        102,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_101",
        101,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_102",
        102,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      103
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_103"
    ],
    "group_id": 65,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_103",
        103,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_103",
        103,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      104
    ],
    "node_names": [
      "ladder_quant_linear_104"
    ],
    "group_id": 66,
    "input_desc": [
      [
        "ladder_quant_linear_104",
        104,
        0
      ],
      [
        "ladder_quant_linear_104",
        104,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_104",
        104,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      105
    ],
    "node_names": [
      "reshape_add_add_105"
    ],
    "group_id": 67,
    "input_desc": [
      [
        "reshape_add_add_105",
        105,
        0
      ],
      [
        "reshape_add_add_105",
        105,
        1
      ],
      [
        "reshape_add_add_105",
        105,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_105",
        105,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      106
    ],
    "node_names": [
      "mean_106"
    ],
    "group_id": 68,
    "input_desc": [
      [
        "mean_106",
        106,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_106",
        106,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      107,
      108,
      109
    ],
    "node_names": [
      "subtract_multiply_107",
      "mean_add_sqrt_108",
      "divide_multiply_add_reshape_109"
    ],
    "group_id": 69,
    "input_desc": [
      [
        "subtract_multiply_107",
        107,
        0
      ],
      [
        "subtract_multiply_107",
        107,
        1
      ],
      [
        "divide_multiply_add_reshape_109",
        109,
        2
      ],
      [
        "divide_multiply_add_reshape_109",
        109,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_109",
        109,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      110
    ],
    "node_names": [
      "ladder_quant_linear_110"
    ],
    "group_id": 70,
    "input_desc": [
      [
        "ladder_quant_linear_110",
        110,
        0
      ],
      [
        "ladder_quant_linear_110",
        110,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_110",
        110,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      111,
      112
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_111",
      "multiply_reshape_112"
    ],
    "group_id": 71,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_111",
        111,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_111",
        111,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_112",
        112,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      113
    ],
    "node_names": [
      "ladder_quant_linear_113"
    ],
    "group_id": 72,
    "input_desc": [
      [
        "ladder_quant_linear_113",
        113,
        0
      ],
      [
        "ladder_quant_linear_113",
        113,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_113",
        113,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      114
    ],
    "node_names": [
      "reshape_add_add_114"
    ],
    "group_id": 73,
    "input_desc": [
      [
        "reshape_add_add_114",
        114,
        0
      ],
      [
        "reshape_add_add_114",
        114,
        1
      ],
      [
        "reshape_add_add_114",
        114,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_114",
        114,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      115
    ],
    "node_names": [
      "mean_115"
    ],
    "group_id": 74,
    "input_desc": [
      [
        "mean_115",
        115,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_115",
        115,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      116,
      117,
      118
    ],
    "node_names": [
      "subtract_multiply_116",
      "mean_add_sqrt_117",
      "divide_multiply_add_reshape_118"
    ],
    "group_id": 75,
    "input_desc": [
      [
        "subtract_multiply_116",
        116,
        0
      ],
      [
        "subtract_multiply_116",
        116,
        1
      ],
      [
        "divide_multiply_add_reshape_118",
        118,
        2
      ],
      [
        "divide_multiply_add_reshape_118",
        118,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_118",
        118,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      119
    ],
    "node_names": [
      "ladder_quant_linear_119"
    ],
    "group_id": 76,
    "input_desc": [
      [
        "ladder_quant_linear_119",
        119,
        0
      ],
      [
        "ladder_quant_linear_119",
        119,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_119",
        119,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      120
    ],
    "node_names": [
      "reshape_add_reshape_transpose_120"
    ],
    "group_id": 77,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_120",
        120,
        0
      ],
      [
        "reshape_add_reshape_transpose_120",
        120,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_120",
        120,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      121
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_121"
    ],
    "group_id": 78,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_121",
        121,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_121",
        121,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_121",
        121,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      122
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_122"
    ],
    "group_id": 79,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_122",
        122,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_122",
        122,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_122",
        122,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      123,
      124
    ],
    "node_names": [
      "nn_batch_matmul_123",
      "reshape_124"
    ],
    "group_id": 80,
    "input_desc": [
      [
        "nn_batch_matmul_123",
        123,
        0
      ],
      [
        "nn_batch_matmul_123",
        123,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_124",
        124,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      125,
      126,
      127,
      128
    ],
    "node_names": [
      "max_125",
      "subtract_exp_126",
      "sum_127",
      "divide_reshape_128"
    ],
    "group_id": 81,
    "input_desc": [
      [
        "subtract_exp_126",
        126,
        0
      ],
      [
        "max_125",
        125,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_128",
        128,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      129,
      130
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_129",
      "nn_batch_matmul_130"
    ],
    "group_id": 82,
    "input_desc": [
      [
        "nn_batch_matmul_130",
        130,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_129",
        129,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_130",
        130,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      131
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_131"
    ],
    "group_id": 83,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_131",
        131,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_131",
        131,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      132
    ],
    "node_names": [
      "ladder_quant_linear_132"
    ],
    "group_id": 84,
    "input_desc": [
      [
        "ladder_quant_linear_132",
        132,
        0
      ],
      [
        "ladder_quant_linear_132",
        132,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_132",
        132,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      133
    ],
    "node_names": [
      "reshape_add_add_133"
    ],
    "group_id": 85,
    "input_desc": [
      [
        "reshape_add_add_133",
        133,
        0
      ],
      [
        "reshape_add_add_133",
        133,
        1
      ],
      [
        "reshape_add_add_133",
        133,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_133",
        133,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      134
    ],
    "node_names": [
      "mean_134"
    ],
    "group_id": 86,
    "input_desc": [
      [
        "mean_134",
        134,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_134",
        134,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      135,
      136,
      137
    ],
    "node_names": [
      "subtract_multiply_135",
      "mean_add_sqrt_136",
      "divide_multiply_add_reshape_137"
    ],
    "group_id": 87,
    "input_desc": [
      [
        "subtract_multiply_135",
        135,
        0
      ],
      [
        "subtract_multiply_135",
        135,
        1
      ],
      [
        "divide_multiply_add_reshape_137",
        137,
        2
      ],
      [
        "divide_multiply_add_reshape_137",
        137,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_137",
        137,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      138
    ],
    "node_names": [
      "ladder_quant_linear_138"
    ],
    "group_id": 88,
    "input_desc": [
      [
        "ladder_quant_linear_138",
        138,
        0
      ],
      [
        "ladder_quant_linear_138",
        138,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_138",
        138,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      139,
      140
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_139",
      "multiply_reshape_140"
    ],
    "group_id": 89,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_139",
        139,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_139",
        139,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_140",
        140,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      141
    ],
    "node_names": [
      "ladder_quant_linear_141"
    ],
    "group_id": 90,
    "input_desc": [
      [
        "ladder_quant_linear_141",
        141,
        0
      ],
      [
        "ladder_quant_linear_141",
        141,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_141",
        141,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      142
    ],
    "node_names": [
      "reshape_add_add_142"
    ],
    "group_id": 91,
    "input_desc": [
      [
        "reshape_add_add_142",
        142,
        0
      ],
      [
        "reshape_add_add_142",
        142,
        1
      ],
      [
        "reshape_add_add_142",
        142,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_142",
        142,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      143
    ],
    "node_names": [
      "mean_143"
    ],
    "group_id": 92,
    "input_desc": [
      [
        "mean_143",
        143,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_143",
        143,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      144,
      145,
      146
    ],
    "node_names": [
      "subtract_multiply_144",
      "mean_add_sqrt_145",
      "divide_multiply_add_reshape_146"
    ],
    "group_id": 93,
    "input_desc": [
      [
        "subtract_multiply_144",
        144,
        0
      ],
      [
        "subtract_multiply_144",
        144,
        1
      ],
      [
        "divide_multiply_add_reshape_146",
        146,
        2
      ],
      [
        "divide_multiply_add_reshape_146",
        146,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_146",
        146,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      147
    ],
    "node_names": [
      "ladder_quant_linear_147"
    ],
    "group_id": 94,
    "input_desc": [
      [
        "ladder_quant_linear_147",
        147,
        0
      ],
      [
        "ladder_quant_linear_147",
        147,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_147",
        147,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      148
    ],
    "node_names": [
      "reshape_add_reshape_transpose_148"
    ],
    "group_id": 95,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_148",
        148,
        0
      ],
      [
        "reshape_add_reshape_transpose_148",
        148,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_148",
        148,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      149
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_149"
    ],
    "group_id": 96,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_149",
        149,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_149",
        149,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_149",
        149,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      150
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_150"
    ],
    "group_id": 97,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_150",
        150,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_150",
        150,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_150",
        150,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      151,
      152
    ],
    "node_names": [
      "nn_batch_matmul_151",
      "reshape_152"
    ],
    "group_id": 98,
    "input_desc": [
      [
        "nn_batch_matmul_151",
        151,
        0
      ],
      [
        "nn_batch_matmul_151",
        151,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_152",
        152,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      153,
      154,
      155,
      156
    ],
    "node_names": [
      "max_153",
      "subtract_exp_154",
      "sum_155",
      "divide_reshape_156"
    ],
    "group_id": 99,
    "input_desc": [
      [
        "subtract_exp_154",
        154,
        0
      ],
      [
        "max_153",
        153,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_156",
        156,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      157,
      158
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_157",
      "nn_batch_matmul_158"
    ],
    "group_id": 100,
    "input_desc": [
      [
        "nn_batch_matmul_158",
        158,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_157",
        157,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_158",
        158,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      159
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_159"
    ],
    "group_id": 101,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_159",
        159,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_159",
        159,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      160
    ],
    "node_names": [
      "ladder_quant_linear_160"
    ],
    "group_id": 102,
    "input_desc": [
      [
        "ladder_quant_linear_160",
        160,
        0
      ],
      [
        "ladder_quant_linear_160",
        160,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_160",
        160,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      161
    ],
    "node_names": [
      "reshape_add_add_161"
    ],
    "group_id": 103,
    "input_desc": [
      [
        "reshape_add_add_161",
        161,
        0
      ],
      [
        "reshape_add_add_161",
        161,
        1
      ],
      [
        "reshape_add_add_161",
        161,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_161",
        161,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      162
    ],
    "node_names": [
      "mean_162"
    ],
    "group_id": 104,
    "input_desc": [
      [
        "mean_162",
        162,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_162",
        162,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      163,
      164,
      165
    ],
    "node_names": [
      "subtract_multiply_163",
      "mean_add_sqrt_164",
      "divide_multiply_add_reshape_165"
    ],
    "group_id": 105,
    "input_desc": [
      [
        "subtract_multiply_163",
        163,
        0
      ],
      [
        "subtract_multiply_163",
        163,
        1
      ],
      [
        "divide_multiply_add_reshape_165",
        165,
        2
      ],
      [
        "divide_multiply_add_reshape_165",
        165,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_165",
        165,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      166
    ],
    "node_names": [
      "ladder_quant_linear_166"
    ],
    "group_id": 106,
    "input_desc": [
      [
        "ladder_quant_linear_166",
        166,
        0
      ],
      [
        "ladder_quant_linear_166",
        166,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_166",
        166,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      167,
      168
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_167",
      "multiply_reshape_168"
    ],
    "group_id": 107,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_167",
        167,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_167",
        167,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_168",
        168,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      169
    ],
    "node_names": [
      "ladder_quant_linear_169"
    ],
    "group_id": 108,
    "input_desc": [
      [
        "ladder_quant_linear_169",
        169,
        0
      ],
      [
        "ladder_quant_linear_169",
        169,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_169",
        169,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      170
    ],
    "node_names": [
      "reshape_add_add_170"
    ],
    "group_id": 109,
    "input_desc": [
      [
        "reshape_add_add_170",
        170,
        0
      ],
      [
        "reshape_add_add_170",
        170,
        1
      ],
      [
        "reshape_add_add_170",
        170,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_170",
        170,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      171
    ],
    "node_names": [
      "mean_171"
    ],
    "group_id": 110,
    "input_desc": [
      [
        "mean_171",
        171,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_171",
        171,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      172,
      173,
      174
    ],
    "node_names": [
      "subtract_multiply_172",
      "mean_add_sqrt_173",
      "divide_multiply_add_reshape_174"
    ],
    "group_id": 111,
    "input_desc": [
      [
        "subtract_multiply_172",
        172,
        0
      ],
      [
        "subtract_multiply_172",
        172,
        1
      ],
      [
        "divide_multiply_add_reshape_174",
        174,
        2
      ],
      [
        "divide_multiply_add_reshape_174",
        174,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_174",
        174,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      175
    ],
    "node_names": [
      "ladder_quant_linear_175"
    ],
    "group_id": 112,
    "input_desc": [
      [
        "ladder_quant_linear_175",
        175,
        0
      ],
      [
        "ladder_quant_linear_175",
        175,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_175",
        175,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      176
    ],
    "node_names": [
      "reshape_add_reshape_transpose_176"
    ],
    "group_id": 113,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_176",
        176,
        0
      ],
      [
        "reshape_add_reshape_transpose_176",
        176,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_176",
        176,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      177
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_177"
    ],
    "group_id": 114,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_177",
        177,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_177",
        177,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_177",
        177,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      178
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_178"
    ],
    "group_id": 115,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_178",
        178,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_178",
        178,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_178",
        178,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      179,
      180
    ],
    "node_names": [
      "nn_batch_matmul_179",
      "reshape_180"
    ],
    "group_id": 116,
    "input_desc": [
      [
        "nn_batch_matmul_179",
        179,
        0
      ],
      [
        "nn_batch_matmul_179",
        179,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_180",
        180,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      181,
      182,
      183,
      184
    ],
    "node_names": [
      "max_181",
      "subtract_exp_182",
      "sum_183",
      "divide_reshape_184"
    ],
    "group_id": 117,
    "input_desc": [
      [
        "subtract_exp_182",
        182,
        0
      ],
      [
        "max_181",
        181,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_184",
        184,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      185,
      186
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_185",
      "nn_batch_matmul_186"
    ],
    "group_id": 118,
    "input_desc": [
      [
        "nn_batch_matmul_186",
        186,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_185",
        185,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_186",
        186,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      187
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_187"
    ],
    "group_id": 119,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_187",
        187,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_187",
        187,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      188
    ],
    "node_names": [
      "ladder_quant_linear_188"
    ],
    "group_id": 120,
    "input_desc": [
      [
        "ladder_quant_linear_188",
        188,
        0
      ],
      [
        "ladder_quant_linear_188",
        188,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_188",
        188,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      189
    ],
    "node_names": [
      "reshape_add_add_189"
    ],
    "group_id": 121,
    "input_desc": [
      [
        "reshape_add_add_189",
        189,
        0
      ],
      [
        "reshape_add_add_189",
        189,
        1
      ],
      [
        "reshape_add_add_189",
        189,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_189",
        189,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      190
    ],
    "node_names": [
      "mean_190"
    ],
    "group_id": 122,
    "input_desc": [
      [
        "mean_190",
        190,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_190",
        190,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      191,
      192,
      193
    ],
    "node_names": [
      "subtract_multiply_191",
      "mean_add_sqrt_192",
      "divide_multiply_add_reshape_193"
    ],
    "group_id": 123,
    "input_desc": [
      [
        "subtract_multiply_191",
        191,
        0
      ],
      [
        "subtract_multiply_191",
        191,
        1
      ],
      [
        "divide_multiply_add_reshape_193",
        193,
        2
      ],
      [
        "divide_multiply_add_reshape_193",
        193,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_193",
        193,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      194
    ],
    "node_names": [
      "ladder_quant_linear_194"
    ],
    "group_id": 124,
    "input_desc": [
      [
        "ladder_quant_linear_194",
        194,
        0
      ],
      [
        "ladder_quant_linear_194",
        194,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_194",
        194,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      195,
      196
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_195",
      "multiply_reshape_196"
    ],
    "group_id": 125,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_195",
        195,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_195",
        195,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_196",
        196,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      197
    ],
    "node_names": [
      "ladder_quant_linear_197"
    ],
    "group_id": 126,
    "input_desc": [
      [
        "ladder_quant_linear_197",
        197,
        0
      ],
      [
        "ladder_quant_linear_197",
        197,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_197",
        197,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      198
    ],
    "node_names": [
      "reshape_add_add_198"
    ],
    "group_id": 127,
    "input_desc": [
      [
        "reshape_add_add_198",
        198,
        0
      ],
      [
        "reshape_add_add_198",
        198,
        1
      ],
      [
        "reshape_add_add_198",
        198,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_198",
        198,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      199
    ],
    "node_names": [
      "mean_199"
    ],
    "group_id": 128,
    "input_desc": [
      [
        "mean_199",
        199,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_199",
        199,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      200,
      201,
      202
    ],
    "node_names": [
      "subtract_multiply_200",
      "mean_add_sqrt_201",
      "divide_multiply_add_reshape_202"
    ],
    "group_id": 129,
    "input_desc": [
      [
        "subtract_multiply_200",
        200,
        0
      ],
      [
        "subtract_multiply_200",
        200,
        1
      ],
      [
        "divide_multiply_add_reshape_202",
        202,
        2
      ],
      [
        "divide_multiply_add_reshape_202",
        202,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_202",
        202,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      203
    ],
    "node_names": [
      "ladder_quant_linear_203"
    ],
    "group_id": 130,
    "input_desc": [
      [
        "ladder_quant_linear_203",
        203,
        0
      ],
      [
        "ladder_quant_linear_203",
        203,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_203",
        203,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      204
    ],
    "node_names": [
      "reshape_add_reshape_transpose_204"
    ],
    "group_id": 131,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_204",
        204,
        0
      ],
      [
        "reshape_add_reshape_transpose_204",
        204,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_204",
        204,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      205
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_205"
    ],
    "group_id": 132,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_205",
        205,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_205",
        205,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_205",
        205,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      206
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_206"
    ],
    "group_id": 133,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_206",
        206,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_206",
        206,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_206",
        206,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      207,
      208
    ],
    "node_names": [
      "nn_batch_matmul_207",
      "reshape_208"
    ],
    "group_id": 134,
    "input_desc": [
      [
        "nn_batch_matmul_207",
        207,
        0
      ],
      [
        "nn_batch_matmul_207",
        207,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_208",
        208,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      209,
      210,
      211,
      212
    ],
    "node_names": [
      "max_209",
      "subtract_exp_210",
      "sum_211",
      "divide_reshape_212"
    ],
    "group_id": 135,
    "input_desc": [
      [
        "subtract_exp_210",
        210,
        0
      ],
      [
        "max_209",
        209,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_212",
        212,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      213,
      214
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_213",
      "nn_batch_matmul_214"
    ],
    "group_id": 136,
    "input_desc": [
      [
        "nn_batch_matmul_214",
        214,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_213",
        213,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_214",
        214,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      215
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_215"
    ],
    "group_id": 137,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_215",
        215,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_215",
        215,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      216
    ],
    "node_names": [
      "ladder_quant_linear_216"
    ],
    "group_id": 138,
    "input_desc": [
      [
        "ladder_quant_linear_216",
        216,
        0
      ],
      [
        "ladder_quant_linear_216",
        216,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_216",
        216,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      217
    ],
    "node_names": [
      "reshape_add_add_217"
    ],
    "group_id": 139,
    "input_desc": [
      [
        "reshape_add_add_217",
        217,
        0
      ],
      [
        "reshape_add_add_217",
        217,
        1
      ],
      [
        "reshape_add_add_217",
        217,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_217",
        217,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      218
    ],
    "node_names": [
      "mean_218"
    ],
    "group_id": 140,
    "input_desc": [
      [
        "mean_218",
        218,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_218",
        218,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      219,
      220,
      221
    ],
    "node_names": [
      "subtract_multiply_219",
      "mean_add_sqrt_220",
      "divide_multiply_add_reshape_221"
    ],
    "group_id": 141,
    "input_desc": [
      [
        "subtract_multiply_219",
        219,
        0
      ],
      [
        "subtract_multiply_219",
        219,
        1
      ],
      [
        "divide_multiply_add_reshape_221",
        221,
        2
      ],
      [
        "divide_multiply_add_reshape_221",
        221,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_221",
        221,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      222
    ],
    "node_names": [
      "ladder_quant_linear_222"
    ],
    "group_id": 142,
    "input_desc": [
      [
        "ladder_quant_linear_222",
        222,
        0
      ],
      [
        "ladder_quant_linear_222",
        222,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_222",
        222,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      223,
      224
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_223",
      "multiply_reshape_224"
    ],
    "group_id": 143,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_223",
        223,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_223",
        223,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_224",
        224,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      225
    ],
    "node_names": [
      "ladder_quant_linear_225"
    ],
    "group_id": 144,
    "input_desc": [
      [
        "ladder_quant_linear_225",
        225,
        0
      ],
      [
        "ladder_quant_linear_225",
        225,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_225",
        225,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      226
    ],
    "node_names": [
      "reshape_add_add_226"
    ],
    "group_id": 145,
    "input_desc": [
      [
        "reshape_add_add_226",
        226,
        0
      ],
      [
        "reshape_add_add_226",
        226,
        1
      ],
      [
        "reshape_add_add_226",
        226,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_226",
        226,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      227
    ],
    "node_names": [
      "mean_227"
    ],
    "group_id": 146,
    "input_desc": [
      [
        "mean_227",
        227,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_227",
        227,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      228,
      229,
      230
    ],
    "node_names": [
      "subtract_multiply_228",
      "mean_add_sqrt_229",
      "divide_multiply_add_reshape_230"
    ],
    "group_id": 147,
    "input_desc": [
      [
        "subtract_multiply_228",
        228,
        0
      ],
      [
        "subtract_multiply_228",
        228,
        1
      ],
      [
        "divide_multiply_add_reshape_230",
        230,
        2
      ],
      [
        "divide_multiply_add_reshape_230",
        230,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_230",
        230,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      231
    ],
    "node_names": [
      "ladder_quant_linear_231"
    ],
    "group_id": 148,
    "input_desc": [
      [
        "ladder_quant_linear_231",
        231,
        0
      ],
      [
        "ladder_quant_linear_231",
        231,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_231",
        231,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      232
    ],
    "node_names": [
      "reshape_add_reshape_transpose_232"
    ],
    "group_id": 149,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_232",
        232,
        0
      ],
      [
        "reshape_add_reshape_transpose_232",
        232,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_232",
        232,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      233
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_233"
    ],
    "group_id": 150,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_233",
        233,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_233",
        233,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_233",
        233,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      234
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_234"
    ],
    "group_id": 151,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_234",
        234,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_234",
        234,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_234",
        234,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      235,
      236
    ],
    "node_names": [
      "nn_batch_matmul_235",
      "reshape_236"
    ],
    "group_id": 152,
    "input_desc": [
      [
        "nn_batch_matmul_235",
        235,
        0
      ],
      [
        "nn_batch_matmul_235",
        235,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_236",
        236,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      237,
      238,
      239,
      240
    ],
    "node_names": [
      "max_237",
      "subtract_exp_238",
      "sum_239",
      "divide_reshape_240"
    ],
    "group_id": 153,
    "input_desc": [
      [
        "subtract_exp_238",
        238,
        0
      ],
      [
        "max_237",
        237,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_240",
        240,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      241,
      242
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_241",
      "nn_batch_matmul_242"
    ],
    "group_id": 154,
    "input_desc": [
      [
        "nn_batch_matmul_242",
        242,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_241",
        241,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_242",
        242,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      243
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_243"
    ],
    "group_id": 155,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_243",
        243,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_243",
        243,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      244
    ],
    "node_names": [
      "ladder_quant_linear_244"
    ],
    "group_id": 156,
    "input_desc": [
      [
        "ladder_quant_linear_244",
        244,
        0
      ],
      [
        "ladder_quant_linear_244",
        244,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_244",
        244,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      245
    ],
    "node_names": [
      "reshape_add_add_245"
    ],
    "group_id": 157,
    "input_desc": [
      [
        "reshape_add_add_245",
        245,
        0
      ],
      [
        "reshape_add_add_245",
        245,
        1
      ],
      [
        "reshape_add_add_245",
        245,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_245",
        245,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      246
    ],
    "node_names": [
      "mean_246"
    ],
    "group_id": 158,
    "input_desc": [
      [
        "mean_246",
        246,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_246",
        246,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      247,
      248,
      249
    ],
    "node_names": [
      "subtract_multiply_247",
      "mean_add_sqrt_248",
      "divide_multiply_add_reshape_249"
    ],
    "group_id": 159,
    "input_desc": [
      [
        "subtract_multiply_247",
        247,
        0
      ],
      [
        "subtract_multiply_247",
        247,
        1
      ],
      [
        "divide_multiply_add_reshape_249",
        249,
        2
      ],
      [
        "divide_multiply_add_reshape_249",
        249,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_249",
        249,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      250
    ],
    "node_names": [
      "ladder_quant_linear_250"
    ],
    "group_id": 160,
    "input_desc": [
      [
        "ladder_quant_linear_250",
        250,
        0
      ],
      [
        "ladder_quant_linear_250",
        250,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_250",
        250,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      251,
      252
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_251",
      "multiply_reshape_252"
    ],
    "group_id": 161,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_251",
        251,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_251",
        251,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_252",
        252,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      253
    ],
    "node_names": [
      "ladder_quant_linear_253"
    ],
    "group_id": 162,
    "input_desc": [
      [
        "ladder_quant_linear_253",
        253,
        0
      ],
      [
        "ladder_quant_linear_253",
        253,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_253",
        253,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      254
    ],
    "node_names": [
      "reshape_add_add_254"
    ],
    "group_id": 163,
    "input_desc": [
      [
        "reshape_add_add_254",
        254,
        0
      ],
      [
        "reshape_add_add_254",
        254,
        1
      ],
      [
        "reshape_add_add_254",
        254,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_254",
        254,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      255
    ],
    "node_names": [
      "mean_255"
    ],
    "group_id": 164,
    "input_desc": [
      [
        "mean_255",
        255,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_255",
        255,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      256,
      257,
      258
    ],
    "node_names": [
      "subtract_multiply_256",
      "mean_add_sqrt_257",
      "divide_multiply_add_reshape_258"
    ],
    "group_id": 165,
    "input_desc": [
      [
        "subtract_multiply_256",
        256,
        0
      ],
      [
        "subtract_multiply_256",
        256,
        1
      ],
      [
        "divide_multiply_add_reshape_258",
        258,
        2
      ],
      [
        "divide_multiply_add_reshape_258",
        258,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_258",
        258,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      259
    ],
    "node_names": [
      "ladder_quant_linear_259"
    ],
    "group_id": 166,
    "input_desc": [
      [
        "ladder_quant_linear_259",
        259,
        0
      ],
      [
        "ladder_quant_linear_259",
        259,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_259",
        259,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      260
    ],
    "node_names": [
      "reshape_add_reshape_transpose_260"
    ],
    "group_id": 167,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_260",
        260,
        0
      ],
      [
        "reshape_add_reshape_transpose_260",
        260,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_260",
        260,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      261
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_261"
    ],
    "group_id": 168,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_261",
        261,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_261",
        261,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_261",
        261,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      262
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_262"
    ],
    "group_id": 169,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_262",
        262,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_262",
        262,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_262",
        262,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      263,
      264
    ],
    "node_names": [
      "nn_batch_matmul_263",
      "reshape_264"
    ],
    "group_id": 170,
    "input_desc": [
      [
        "nn_batch_matmul_263",
        263,
        0
      ],
      [
        "nn_batch_matmul_263",
        263,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_264",
        264,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      265,
      266,
      267,
      268
    ],
    "node_names": [
      "max_265",
      "subtract_exp_266",
      "sum_267",
      "divide_reshape_268"
    ],
    "group_id": 171,
    "input_desc": [
      [
        "subtract_exp_266",
        266,
        0
      ],
      [
        "max_265",
        265,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_268",
        268,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      269,
      270
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_269",
      "nn_batch_matmul_270"
    ],
    "group_id": 172,
    "input_desc": [
      [
        "nn_batch_matmul_270",
        270,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_269",
        269,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_270",
        270,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      271
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_271"
    ],
    "group_id": 173,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_271",
        271,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_271",
        271,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      272
    ],
    "node_names": [
      "ladder_quant_linear_272"
    ],
    "group_id": 174,
    "input_desc": [
      [
        "ladder_quant_linear_272",
        272,
        0
      ],
      [
        "ladder_quant_linear_272",
        272,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_272",
        272,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      273
    ],
    "node_names": [
      "reshape_add_add_273"
    ],
    "group_id": 175,
    "input_desc": [
      [
        "reshape_add_add_273",
        273,
        0
      ],
      [
        "reshape_add_add_273",
        273,
        1
      ],
      [
        "reshape_add_add_273",
        273,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_273",
        273,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      274
    ],
    "node_names": [
      "mean_274"
    ],
    "group_id": 176,
    "input_desc": [
      [
        "mean_274",
        274,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_274",
        274,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      275,
      276,
      277
    ],
    "node_names": [
      "subtract_multiply_275",
      "mean_add_sqrt_276",
      "divide_multiply_add_reshape_277"
    ],
    "group_id": 177,
    "input_desc": [
      [
        "subtract_multiply_275",
        275,
        0
      ],
      [
        "subtract_multiply_275",
        275,
        1
      ],
      [
        "divide_multiply_add_reshape_277",
        277,
        2
      ],
      [
        "divide_multiply_add_reshape_277",
        277,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_277",
        277,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      278
    ],
    "node_names": [
      "ladder_quant_linear_278"
    ],
    "group_id": 178,
    "input_desc": [
      [
        "ladder_quant_linear_278",
        278,
        0
      ],
      [
        "ladder_quant_linear_278",
        278,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_278",
        278,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      279,
      280
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_279",
      "multiply_reshape_280"
    ],
    "group_id": 179,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_279",
        279,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_279",
        279,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_280",
        280,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      281
    ],
    "node_names": [
      "ladder_quant_linear_281"
    ],
    "group_id": 180,
    "input_desc": [
      [
        "ladder_quant_linear_281",
        281,
        0
      ],
      [
        "ladder_quant_linear_281",
        281,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_281",
        281,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      282
    ],
    "node_names": [
      "reshape_add_add_282"
    ],
    "group_id": 181,
    "input_desc": [
      [
        "reshape_add_add_282",
        282,
        0
      ],
      [
        "reshape_add_add_282",
        282,
        1
      ],
      [
        "reshape_add_add_282",
        282,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_282",
        282,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      283
    ],
    "node_names": [
      "mean_283"
    ],
    "group_id": 182,
    "input_desc": [
      [
        "mean_283",
        283,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_283",
        283,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      284,
      285,
      286
    ],
    "node_names": [
      "subtract_multiply_284",
      "mean_add_sqrt_285",
      "divide_multiply_add_reshape_286"
    ],
    "group_id": 183,
    "input_desc": [
      [
        "subtract_multiply_284",
        284,
        0
      ],
      [
        "subtract_multiply_284",
        284,
        1
      ],
      [
        "divide_multiply_add_reshape_286",
        286,
        2
      ],
      [
        "divide_multiply_add_reshape_286",
        286,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_286",
        286,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      287
    ],
    "node_names": [
      "ladder_quant_linear_287"
    ],
    "group_id": 184,
    "input_desc": [
      [
        "ladder_quant_linear_287",
        287,
        0
      ],
      [
        "ladder_quant_linear_287",
        287,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_287",
        287,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      288
    ],
    "node_names": [
      "reshape_add_reshape_transpose_288"
    ],
    "group_id": 185,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_288",
        288,
        0
      ],
      [
        "reshape_add_reshape_transpose_288",
        288,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_288",
        288,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      289
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_289"
    ],
    "group_id": 186,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_289",
        289,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_289",
        289,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_289",
        289,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      290
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_290"
    ],
    "group_id": 187,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_290",
        290,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_290",
        290,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_290",
        290,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      291,
      292
    ],
    "node_names": [
      "nn_batch_matmul_291",
      "reshape_292"
    ],
    "group_id": 188,
    "input_desc": [
      [
        "nn_batch_matmul_291",
        291,
        0
      ],
      [
        "nn_batch_matmul_291",
        291,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_292",
        292,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      293,
      294,
      295,
      296
    ],
    "node_names": [
      "max_293",
      "subtract_exp_294",
      "sum_295",
      "divide_reshape_296"
    ],
    "group_id": 189,
    "input_desc": [
      [
        "subtract_exp_294",
        294,
        0
      ],
      [
        "max_293",
        293,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_296",
        296,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      297,
      298
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_297",
      "nn_batch_matmul_298"
    ],
    "group_id": 190,
    "input_desc": [
      [
        "nn_batch_matmul_298",
        298,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_297",
        297,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_298",
        298,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      299
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_299"
    ],
    "group_id": 191,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_299",
        299,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_299",
        299,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      300
    ],
    "node_names": [
      "ladder_quant_linear_300"
    ],
    "group_id": 192,
    "input_desc": [
      [
        "ladder_quant_linear_300",
        300,
        0
      ],
      [
        "ladder_quant_linear_300",
        300,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_300",
        300,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      301
    ],
    "node_names": [
      "reshape_add_add_301"
    ],
    "group_id": 193,
    "input_desc": [
      [
        "reshape_add_add_301",
        301,
        0
      ],
      [
        "reshape_add_add_301",
        301,
        1
      ],
      [
        "reshape_add_add_301",
        301,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_301",
        301,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      302
    ],
    "node_names": [
      "mean_302"
    ],
    "group_id": 194,
    "input_desc": [
      [
        "mean_302",
        302,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_302",
        302,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      303,
      304,
      305
    ],
    "node_names": [
      "subtract_multiply_303",
      "mean_add_sqrt_304",
      "divide_multiply_add_reshape_305"
    ],
    "group_id": 195,
    "input_desc": [
      [
        "subtract_multiply_303",
        303,
        0
      ],
      [
        "subtract_multiply_303",
        303,
        1
      ],
      [
        "divide_multiply_add_reshape_305",
        305,
        2
      ],
      [
        "divide_multiply_add_reshape_305",
        305,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_305",
        305,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      306
    ],
    "node_names": [
      "ladder_quant_linear_306"
    ],
    "group_id": 196,
    "input_desc": [
      [
        "ladder_quant_linear_306",
        306,
        0
      ],
      [
        "ladder_quant_linear_306",
        306,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_306",
        306,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      307,
      308
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_307",
      "multiply_reshape_308"
    ],
    "group_id": 197,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_307",
        307,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_307",
        307,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_308",
        308,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      309
    ],
    "node_names": [
      "ladder_quant_linear_309"
    ],
    "group_id": 198,
    "input_desc": [
      [
        "ladder_quant_linear_309",
        309,
        0
      ],
      [
        "ladder_quant_linear_309",
        309,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_309",
        309,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      310
    ],
    "node_names": [
      "reshape_add_add_310"
    ],
    "group_id": 199,
    "input_desc": [
      [
        "reshape_add_add_310",
        310,
        0
      ],
      [
        "reshape_add_add_310",
        310,
        1
      ],
      [
        "reshape_add_add_310",
        310,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_310",
        310,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      311
    ],
    "node_names": [
      "mean_311"
    ],
    "group_id": 200,
    "input_desc": [
      [
        "mean_311",
        311,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_311",
        311,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      312,
      313,
      314
    ],
    "node_names": [
      "subtract_multiply_312",
      "mean_add_sqrt_313",
      "divide_multiply_add_reshape_314"
    ],
    "group_id": 201,
    "input_desc": [
      [
        "subtract_multiply_312",
        312,
        0
      ],
      [
        "subtract_multiply_312",
        312,
        1
      ],
      [
        "divide_multiply_add_reshape_314",
        314,
        2
      ],
      [
        "divide_multiply_add_reshape_314",
        314,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_314",
        314,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      315
    ],
    "node_names": [
      "ladder_quant_linear_315"
    ],
    "group_id": 202,
    "input_desc": [
      [
        "ladder_quant_linear_315",
        315,
        0
      ],
      [
        "ladder_quant_linear_315",
        315,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_315",
        315,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group4(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 18) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 18) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 18) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 2304)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 4608)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 6912)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 18) * 11520) + ((((int)threadIdx.x) >> 6) * 1152)) + ((((int)blockIdx.x) % 18) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      90,
      1,
      1
    ],
    "latency": 0.021913599222898483,
    "name": "Group4",
    "gain": 0
  },
  {
    "nodes": [
      316
    ],
    "node_names": [
      "reshape_add_reshape_transpose_316"
    ],
    "group_id": 203,
    "input_desc": [
      [
        "reshape_add_reshape_transpose_316",
        316,
        0
      ],
      [
        "reshape_add_reshape_transpose_316",
        316,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_add_reshape_transpose_316",
        316,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group5(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose) {\n  \n  T_transpose[(((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[(((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))]);\n  T_transpose[((((((((int)blockIdx.x) >> 1) * 640) + ((((int)threadIdx.x) >> 5) * 128)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 64)] = (p1[((((((int)blockIdx.x) / 10) * 64) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31))] + p0[((((((((((int)blockIdx.x) % 10) >> 1) * 11520) + ((((int)threadIdx.x) >> 5) * 2304)) + ((((int)blockIdx.x) / 10) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 1152)]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      180,
      1,
      1
    ],
    "latency": 0.0028310588095337152,
    "name": "Group5",
    "gain": 0
  },
  {
    "nodes": [
      317
    ],
    "node_names": [
      "strided_slice_squeeze_multiply_reshape_317"
    ],
    "group_id": 204,
    "input_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_317",
        317,
        0
      ],
      [
        "strided_slice_squeeze_multiply_reshape_317",
        317,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_multiply_reshape_317",
        317,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group6(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape) {\n  \n  T_reshape[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] * p1[0]);\n  T_reshape[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025298823602497578,
    "name": "Group6",
    "gain": 0
  },
  {
    "nodes": [
      318
    ],
    "node_names": [
      "strided_slice_squeeze_transpose_multiply_reshape_transpose_318"
    ],
    "group_id": 205,
    "input_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_318",
        318,
        0
      ],
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_318",
        318,
        1
      ]
    ],
    "output_desc": [
      [
        "strided_slice_squeeze_transpose_multiply_reshape_transpose_318",
        318,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group7(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_transpose_7) {\n  \n  T_transpose_7[((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2))] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19200)] * p1[0]);\n  T_transpose_7[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 1)] = (p0[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 2)) + 19201)] * p1[0]);\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.0025317647960036993,
    "name": "Group7",
    "gain": 0
  },
  {
    "nodes": [
      319,
      320
    ],
    "node_names": [
      "nn_batch_matmul_319",
      "reshape_320"
    ],
    "group_id": 206,
    "input_desc": [
      [
        "nn_batch_matmul_319",
        319,
        0
      ],
      [
        "nn_batch_matmul_319",
        319,
        1
      ]
    ],
    "output_desc": [
      [
        "reshape_320",
        320,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(125) Group8(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT) {\n  \n  half T_batch_matmul_NT_local[2];\n  __shared__ half p0_shared[1600];\n  __shared__ half p1_shared[640];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 125)];\n  p0_shared[(((int)threadIdx.x) + 250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 250)];\n  p0_shared[(((int)threadIdx.x) + 375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 375)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 625)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 625)];\n  p0_shared[(((int)threadIdx.x) + 750)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 750)];\n  p0_shared[(((int)threadIdx.x) + 875)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 875)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1125)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1125)];\n  p0_shared[(((int)threadIdx.x) + 1250)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1250)];\n  p0_shared[(((int)threadIdx.x) + 1375)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1375)];\n  if (((int)threadIdx.x) < 100) {\n    p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) / 5) * 1600) + ((int)threadIdx.x)) + 1500)];\n  }\n  p1_shared[((int)threadIdx.x)] = p1[((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x))];\n  p1_shared[(((int)threadIdx.x) + 125)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 125)];\n  p1_shared[(((int)threadIdx.x) + 250)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 250)];\n  p1_shared[(((int)threadIdx.x) + 375)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 375)];\n  p1_shared[(((int)threadIdx.x) + 500)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 500)];\n  if (((int)threadIdx.x) < 15) {\n    p1_shared[(((int)threadIdx.x) + 625)] = p1[(((((((int)blockIdx.x) / 10) * 3200) + ((((int)blockIdx.x) % 5) * 640)) + ((int)threadIdx.x)) + 625)];\n  }\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 64; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[(((((int)threadIdx.x) % 5) * 128) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) / 5) * 64) + k_1)] * p1_shared[((((((int)threadIdx.x) % 5) * 128) + k_1) + 64)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) / 5) * 1250) + ((((int)threadIdx.x) / 5) * 50)) + ((((int)blockIdx.x) % 5) * 10)) + ((((int)threadIdx.x) % 5) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n}\n\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.004476000089198351,
    "name": "Group8",
    "gain": 0.0
  },
  {
    "nodes": [
      321,
      322,
      323,
      324
    ],
    "node_names": [
      "max_321",
      "subtract_exp_322",
      "sum_323",
      "divide_reshape_324"
    ],
    "group_id": 207,
    "input_desc": [
      [
        "subtract_exp_322",
        322,
        0
      ],
      [
        "max_321",
        321,
        0
      ]
    ],
    "output_desc": [
      [
        "divide_reshape_324",
        324,
        0
      ]
    ],
    "code": "__device__ void Group9_0_max_13(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)(shared+0);\n  p0_red_local[0] = __float2half_rn(-6.550400e+04f);\n  p0_red_local[1] = __float2half_rn(-6.550400e+04f);\n  *(uint1*)(p0_shared + (((int)threadIdx.x) * 2)) = *(uint1*)(p0 + ((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)));\n  __syncthreads();\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = max(p0_red_local[0], p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_1_subtract_exp_14(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_exp, char* shared) {\n  half* p1_shared = (half*)p1;\n  half p1_shared_local[1];\n  p1_shared_local[0] = p1_shared[(((int)threadIdx.x) / 25)];\n  __syncthreads();\n  T_exp[(((int)threadIdx.x) * 2)] = hexp((p0[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] - p1_shared_local[0]));\n  T_exp[((((int)threadIdx.x) * 2) + 1)] = hexp((p0[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] - p1_shared_local[0]));\n  __syncthreads();\n}\n\n__device__ void Group9_2_sum_15(half* __restrict__ p0, half* __restrict__ p0_red, char* shared) {\n  half p0_red_local[2];\n  half* p0_shared = (half*)p0;\n  p0_red_local[0] = __float2half_rn(0.000000e+00f);\n  p0_red_local[1] = __float2half_rn(0.000000e+00f);\n  for (int k3_inner = 0; k3_inner < 50; ++k3_inner) {\n    if ((((int)threadIdx.x) % 25) < 1) {\n      p0_red_local[0] = (p0_red_local[0] + p0_shared[(((((int)threadIdx.x) / 25) * 50) + k3_inner)]);\n    }\n  }\n  __syncthreads();\n  if ((((int)threadIdx.x) % 25) < 1) {\n    p0_red[(((int)threadIdx.x) * 2)] = p0_red_local[0];\n  }\n  __syncthreads();\n}\n\n__device__ void Group9_3_divide_reshape_16(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] / p1_shared[(((int)threadIdx.x) / 25)]);\n  T_reshape[(((((int)blockIdx.x) * 250) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] / p1_shared[(((int)threadIdx.x) / 25)]);\n}\n\n__global__ void __launch_bounds__(125) Group9(half* input0, half* input1, half* output0) {\n  __shared__ char shared[1024];\n  Group9_0_max_13(input1, (half*)(shared+0), shared+0);\n  Group9_1_subtract_exp_14(input0, (half*)(shared+0), (half*)(shared+0), shared+512);\n  Group9_2_sum_15((half*)(shared+0), (half*)(shared+512), shared+512);\n  Group9_3_divide_reshape_16((half*)(shared+0), (half*)(shared+512), output0, shared+1024);\n}\n",
    "block_size": [
      125,
      1,
      1
    ],
    "grid_size": [
      60,
      1,
      1
    ],
    "latency": 0.00470399996265769,
    "name": "Group9",
    "gain": 0.006354630226269364
  },
  {
    "nodes": [
      325,
      326
    ],
    "node_names": [
      "strided_slice_squeeze_reshape_transpose_325",
      "nn_batch_matmul_326"
    ],
    "group_id": 208,
    "input_desc": [
      [
        "nn_batch_matmul_326",
        326,
        0
      ],
      [
        "strided_slice_squeeze_reshape_transpose_325",
        325,
        0
      ]
    ],
    "output_desc": [
      [
        "nn_batch_matmul_326",
        326,
        0
      ]
    ],
    "code": "__device__ void Group10_0_strided_slice_squeeze_reshape_transpose_17(half* __restrict__ p0, half* __restrict__ T_transpose, char* shared) {\n  __syncthreads();\n  T_transpose[(((int)threadIdx.x) * 2)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38400)];\n  T_transpose[((((int)threadIdx.x) * 2) + 200)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38404)];\n  T_transpose[((((int)threadIdx.x) * 2) + 1)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38464)];\n  T_transpose[((((int)threadIdx.x) * 2) + 201)] = p0[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) % 25) * 128)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) / 25)) + 38468)];\n  __syncthreads();\n}\n\n__device__ void Group10_1_nn_batch_matmul_18(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_batch_matmul_NT, char* shared) {\n  half* p1_shared = (half*)p1;\n  half T_batch_matmul_NT_local[4];\n  half* p0_shared = (half*)(shared+0);\n  p1_shared[((int)threadIdx.x)] = p1[((int)threadIdx.x)];\n  p1_shared[(((int)threadIdx.x) + 100)] = p1[(((int)threadIdx.x) + 100)];\n  p1_shared[(((int)threadIdx.x) + 200)] = p1[(((int)threadIdx.x) + 200)];\n  p1_shared[(((int)threadIdx.x) + 300)] = p1[(((int)threadIdx.x) + 300)];\n  T_batch_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[2] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[1] = __float2half_rn(0.000000e+00f);\n  T_batch_matmul_NT_local[3] = __float2half_rn(0.000000e+00f);\n  p0_shared[((int)threadIdx.x)] = p0[(((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x))];\n  p0_shared[(((int)threadIdx.x) + 100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 100)];\n  p0_shared[(((int)threadIdx.x) + 200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 200)];\n  p0_shared[(((int)threadIdx.x) + 300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 300)];\n  p0_shared[(((int)threadIdx.x) + 400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 400)];\n  p0_shared[(((int)threadIdx.x) + 500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 500)];\n  p0_shared[(((int)threadIdx.x) + 600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 600)];\n  p0_shared[(((int)threadIdx.x) + 700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 700)];\n  p0_shared[(((int)threadIdx.x) + 800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 800)];\n  p0_shared[(((int)threadIdx.x) + 900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 900)];\n  p0_shared[(((int)threadIdx.x) + 1000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1000)];\n  p0_shared[(((int)threadIdx.x) + 1100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1100)];\n  p0_shared[(((int)threadIdx.x) + 1200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1200)];\n  p0_shared[(((int)threadIdx.x) + 1300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1300)];\n  p0_shared[(((int)threadIdx.x) + 1400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1400)];\n  p0_shared[(((int)threadIdx.x) + 1500)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1500)];\n  p0_shared[(((int)threadIdx.x) + 1600)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1600)];\n  p0_shared[(((int)threadIdx.x) + 1700)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1700)];\n  p0_shared[(((int)threadIdx.x) + 1800)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1800)];\n  p0_shared[(((int)threadIdx.x) + 1900)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 1900)];\n  p0_shared[(((int)threadIdx.x) + 2000)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2000)];\n  p0_shared[(((int)threadIdx.x) + 2100)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2100)];\n  p0_shared[(((int)threadIdx.x) + 2200)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2200)];\n  p0_shared[(((int)threadIdx.x) + 2300)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2300)];\n  p0_shared[(((int)threadIdx.x) + 2400)] = p0[((((((int)blockIdx.x) >> 3) * 2500) + ((int)threadIdx.x)) + 2400)];\n  __syncthreads();\n  for (int k_1 = 0; k_1 < 50; ++k_1) {\n    T_batch_matmul_NT_local[0] = (T_batch_matmul_NT_local[0] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[2] = (T_batch_matmul_NT_local[2] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[(((((int)threadIdx.x) & 3) * 100) + k_1)]));\n    T_batch_matmul_NT_local[1] = (T_batch_matmul_NT_local[1] + (p0_shared[(((((int)threadIdx.x) >> 2) * 50) + k_1)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n    T_batch_matmul_NT_local[3] = (T_batch_matmul_NT_local[3] + (p0_shared[((((((int)threadIdx.x) >> 2) * 50) + k_1) + 1250)] * p1_shared[((((((int)threadIdx.x) & 3) * 100) + k_1) + 50)]));\n  }\n  T_batch_matmul_NT[(((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2))] = T_batch_matmul_NT_local[0];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1600)] = T_batch_matmul_NT_local[2];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1)] = T_batch_matmul_NT_local[1];\n  T_batch_matmul_NT[((((((((int)blockIdx.x) >> 3) * 3200) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) & 3) * 2)) + 1601)] = T_batch_matmul_NT_local[3];\n}\n\n__global__ void __launch_bounds__(100) Group10(half* input0, half* input1, half* output0) {\n  __shared__ char shared[5824];\n  Group10_0_strided_slice_squeeze_reshape_transpose_17(input1, (half*)(shared+0), shared+0);\n  Group10_1_nn_batch_matmul_18(input0, (half*)(shared+0), output0, shared+800);\n}\n",
    "block_size": [
      100,
      1,
      1
    ],
    "grid_size": [
      48,
      1,
      1
    ],
    "latency": 0.00467657158151269,
    "name": "Group10",
    "gain": 0.0015047616325318813
  },
  {
    "nodes": [
      327
    ],
    "node_names": [
      "reshape_transpose_reshape_reshape_327"
    ],
    "group_id": 209,
    "input_desc": [
      [
        "reshape_transpose_reshape_reshape_327",
        327,
        0
      ]
    ],
    "output_desc": [
      [
        "reshape_transpose_reshape_reshape_327",
        327,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(96) Group11(half* __restrict__ p0, half* __restrict__ T_reshape_4) {\n  \n  T_reshape_4[(((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2))] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + ((((((int)blockIdx.x) & 3) * 32) + ((((int)threadIdx.x) % 48) * 2)) & 63))];\n  T_reshape_4[((((((((int)blockIdx.x) >> 2) * 768) + ((((int)threadIdx.x) / 48) * 384)) + ((((int)blockIdx.x) & 3) * 96)) + ((((int)threadIdx.x) % 48) * 2)) + 1)] = p0[((((((((((int)blockIdx.x) & 3) * 3) + ((((int)threadIdx.x) % 48) >> 4)) >> 1) * 3200) + ((((int)blockIdx.x) >> 2) * 128)) + ((((int)threadIdx.x) / 48) * 64)) + (((((((int)blockIdx.x) & 3) * 96) + ((((int)threadIdx.x) % 48) * 2)) + 1) & 63))];\n}\n\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      100,
      1,
      1
    ],
    "latency": 0.0026168888434767723,
    "name": "Group11",
    "gain": 0
  },
  {
    "nodes": [
      328
    ],
    "node_names": [
      "ladder_quant_linear_328"
    ],
    "group_id": 210,
    "input_desc": [
      [
        "ladder_quant_linear_328",
        328,
        0
      ],
      [
        "ladder_quant_linear_328",
        328,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_328",
        328,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group12(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[768];\n  __shared__ half B_decode_shared[1536];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint1*)(p0_shared + ((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2))) = *(uint1*)(p0 + ((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 256)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 256));\n  *(uint1*)(p0_shared + (((((int)threadIdx.y) * 32) + (((int)threadIdx.x) * 2)) + 512)) = *(uint1*)(p0 + (((((((int)blockIdx.x) / 96) * 768) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.x) * 2)) + 512));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 1536) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 24; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 384) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.01535542868077755,
    "name": "Group12",
    "gain": 0
  },
  {
    "nodes": [
      329
    ],
    "node_names": [
      "reshape_add_add_329"
    ],
    "group_id": 211,
    "input_desc": [
      [
        "reshape_add_add_329",
        329,
        0
      ],
      [
        "reshape_add_add_329",
        329,
        1
      ],
      [
        "reshape_add_add_329",
        329,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_329",
        329,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      330
    ],
    "node_names": [
      "mean_330"
    ],
    "group_id": 212,
    "input_desc": [
      [
        "mean_330",
        330,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_330",
        330,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      331,
      332,
      333
    ],
    "node_names": [
      "subtract_multiply_331",
      "mean_add_sqrt_332",
      "divide_multiply_add_reshape_333"
    ],
    "group_id": 213,
    "input_desc": [
      [
        "subtract_multiply_331",
        331,
        0
      ],
      [
        "subtract_multiply_331",
        331,
        1
      ],
      [
        "divide_multiply_add_reshape_333",
        333,
        2
      ],
      [
        "divide_multiply_add_reshape_333",
        333,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_reshape_333",
        333,
        0
      ]
    ],
    "code": "__device__ void Group21_0_subtract_multiply_32(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  __syncthreads();\n  output_proxy[((int)threadIdx.x)] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((int)threadIdx.x) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group21_1_mean_add_sqrt_33(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float T_cast_red_local[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  T_cast_red_local[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner = 0; k2_inner < 384; ++k2_inner) {\n    if (((int)threadIdx.x) < 1) {\n      T_cast_red_local[0] = (T_cast_red_local[0] + T_cast_shared[k2_inner]);\n    }\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 1) {\n    T_sqrt[((int)threadIdx.x)] = hsqrt((((half)(T_cast_red_local[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n  }\n  __syncthreads();\n}\n\n__device__ void Group21_2_divide_multiply_add_reshape_34(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  half* p1_shared = (half*)p1;\n  T_reshape[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (((p0_shared[((int)threadIdx.x)] / p1_shared[0]) * p2[((int)threadIdx.x)]) + p3[((int)threadIdx.x)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (((p0_shared[(((int)threadIdx.x) + 128)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 128)]) + p3[(((int)threadIdx.x) + 128)]);\n  T_reshape[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (((p0_shared[(((int)threadIdx.x) + 256)] / p1_shared[0]) * p2[(((int)threadIdx.x) + 256)]) + p3[(((int)threadIdx.x) + 256)]);\n}\n\n__global__ void __launch_bounds__(128) Group21(half* input0, half* input1, half* input2, half* input3, half* output0) {\n  __shared__ char shared[3072];\n  Group21_0_subtract_multiply_32(input0, input1, (half*)(shared+0), (half*)(shared+768), shared+0);\n  Group21_1_mean_add_sqrt_33((half*)(shared+768), (half*)(shared+768), shared+1536);\n  Group21_2_divide_multiply_add_reshape_34((half*)(shared+0), (half*)(shared+768), input2, input3, output0, shared+1536);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.005740799941122532,
    "name": "Group21",
    "gain": 0.0029026910196989775
  },
  {
    "nodes": [
      334
    ],
    "node_names": [
      "ladder_quant_linear_334"
    ],
    "group_id": 214,
    "input_desc": [
      [
        "ladder_quant_linear_334",
        334,
        0
      ],
      [
        "ladder_quant_linear_334",
        334,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_334",
        334,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group16(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half T_quant_linear_local[5];\n  __shared__ half p0_shared[960];\n  __shared__ half B_decode_shared[6144];\n  T_quant_linear_local[0] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[1] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[2] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[3] = __float2half_rn(0.000000e+00f);\n  T_quant_linear_local[4] = __float2half_rn(0.000000e+00f);\n  for (int k_outer = 0; k_outer < 4; ++k_outer) {\n    __syncthreads();\n    p0_shared[((int)threadIdx.x)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96))];\n    p0_shared[(((int)threadIdx.x) + 128)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 128) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 256)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 256) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 384)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 1536)];\n    p0_shared[(((int)threadIdx.x) + 512)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 512) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 32) % 96))];\n    p0_shared[(((int)threadIdx.x) + 640)] = p0[(((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 640) / 96) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) + 64) % 96))];\n    p0_shared[(((int)threadIdx.x) + 768)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + ((((int)threadIdx.x) / 96) * 384)) + (k_outer * 96)) + (((int)threadIdx.x) % 96)) + 3072)];\n    if (((int)threadIdx.x) < 64) {\n      p0_shared[(((int)threadIdx.x) + 896)] = p0[((((((((int)blockIdx.x) / 24) * 3840) + (((((int)threadIdx.x) + 896) / 96) * 384)) + (k_outer * 96)) + ((int)threadIdx.x)) + 32)];\n    }\n    for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n        short __1 = ((short)p1[((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n      B_decode_shared[((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n    }\n    for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n        short __2 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 128) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n    }\n    for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n        short __3 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 256) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n    }\n    for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n        short __4 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_3) + 6144)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n    }\n    for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n        short __5 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 512) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n    }\n    for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n        short __6 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 640) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n    }\n    for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n        short __7 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_6) + 12288)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n    }\n    for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n        short __8 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 896) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n    }\n    for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n        short __9 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1024) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n    }\n    for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n        short __10 = ((short)p1[(((((((((int)blockIdx.x) % 24) * 24576) + ((((int)threadIdx.x) / 24) * 384)) + (k_outer * 96)) + ((((int)threadIdx.x) % 24) * 4)) + ax0_ax1_fused_inner_s_9) + 18432)]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n    }\n    for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n        short __11 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1280) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 32) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n    }\n    for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n        short __12 = ((short)p1[(((((((int)blockIdx.x) % 24) * 24576) + (((((int)threadIdx.x) + 1408) / 24) * 384)) + (k_outer * 96)) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 64) % 96))]) << (short)8;\n      B_decode_shared[(((((int)threadIdx.x) * 4) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 96; ++k_inner) {\n      T_quant_linear_local[0] = (T_quant_linear_local[0] + (p0_shared[(((((int)threadIdx.x) >> 6) * 96) + k_inner)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[1] = (T_quant_linear_local[1] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 192)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[2] = (T_quant_linear_local[2] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 384)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[3] = (T_quant_linear_local[3] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 576)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n      T_quant_linear_local[4] = (T_quant_linear_local[4] + (p0_shared[((((((int)threadIdx.x) >> 6) * 96) + k_inner) + 768)] * B_decode_shared[(((((int)threadIdx.x) & 63) * 96) + k_inner)]));\n    }\n  }\n  T_quant_linear[(((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63))] = T_quant_linear_local[0];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 3072)] = T_quant_linear_local[1];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 6144)] = T_quant_linear_local[2];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 9216)] = T_quant_linear_local[3];\n  T_quant_linear[((((((((int)blockIdx.x) / 24) * 15360) + ((((int)threadIdx.x) >> 6) * 1536)) + ((((int)blockIdx.x) % 24) * 64)) + (((int)threadIdx.x) & 63)) + 12288)] = T_quant_linear_local[4];\n}\n\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.022118400782346725,
    "name": "Group16",
    "gain": 0
  },
  {
    "nodes": [
      335,
      336
    ],
    "node_names": [
      "reshape_add_divide_erf_add_multiply_335",
      "multiply_reshape_336"
    ],
    "group_id": 215,
    "input_desc": [
      [
        "reshape_add_divide_erf_add_multiply_335",
        335,
        0
      ],
      [
        "reshape_add_divide_erf_add_multiply_335",
        335,
        1
      ]
    ],
    "output_desc": [
      [
        "multiply_reshape_336",
        336,
        0
      ]
    ],
    "code": "__device__ void Group17_0_reshape_add_divide_erf_add_multiply_27(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_multiply, char* shared) {\n  __syncthreads();\n  T_multiply[(((int)threadIdx.x) * 2)] = ((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * (herf(((p1[(((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2))] + p0[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  T_multiply[((((int)threadIdx.x) * 2) + 1)] = ((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * (herf(((p1[((((((int)blockIdx.x) & 7) * 192) + (((int)threadIdx.x) * 2)) + 1)] + p0[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)]) * __float2half_rn(7.071823e-01f))) + __float2half_rn(1.000000e+00f)));\n  __syncthreads();\n}\n\n__device__ void Group17_1_multiply_reshape_28(half* __restrict__ p0, half* __restrict__ T_reshape, char* shared) {\n  half* p0_shared = (half*)p0;\n  T_reshape[((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2))] = (p0_shared[(((int)threadIdx.x) * 2)] * __float2half_rn(5.000000e-01f));\n  T_reshape[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + 1)] = (p0_shared[((((int)threadIdx.x) * 2) + 1)] * __float2half_rn(5.000000e-01f));\n}\n\n__global__ void __launch_bounds__(96) Group17(half* input0, half* input1, half* output0) {\n  __shared__ char shared[384];\n  Group17_0_reshape_add_divide_erf_add_multiply_27(input0, input1, (half*)(shared+0), shared+0);\n  Group17_1_multiply_reshape_28((half*)(shared+0), output0, shared+384);\n}\n",
    "block_size": [
      96,
      1,
      1
    ],
    "grid_size": [
      400,
      1,
      1
    ],
    "latency": 0.00354036339558661,
    "name": "Group17",
    "gain": 0.002302303444594145
  },
  {
    "nodes": [
      337
    ],
    "node_names": [
      "ladder_quant_linear_337"
    ],
    "group_id": 216,
    "input_desc": [
      [
        "ladder_quant_linear_337",
        337,
        0
      ],
      [
        "ladder_quant_linear_337",
        337,
        1
      ]
    ],
    "output_desc": [
      [
        "ladder_quant_linear_337",
        337,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group18(half* __restrict__ p0, int8_t* __restrict__ p1, half* __restrict__ T_quant_linear) {\n  \n  half normal_reduce_temp0[1];\n  __shared__ half p0_shared[3072];\n  __shared__ half B_decode_shared[6144];\n  half red_buf0[1];\n  normal_reduce_temp0[0] = __float2half_rn(0.000000e+00f);\n  *(uint4*)(p0_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8))) = *(uint4*)(p0 + ((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 1024)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 1024));\n  *(uint4*)(p0_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 8)) + 2048)) = *(uint4*)(p0 + (((((((int)blockIdx.x) / 96) * 3072) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 8)) + 2048));\n  for (int ax0_ax1_fused_inner_s = 0; ax0_ax1_fused_inner_s < 4; ++ax0_ax1_fused_inner_s) {\n      short __1 = ((short)p1[(((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)]) << (short)8;\n    B_decode_shared[(((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s)] = (*(half *)(&(__1)));\n  }\n  for (int ax0_ax1_fused_inner_s_1 = 0; ax0_ax1_fused_inner_s_1 < 4; ++ax0_ax1_fused_inner_s_1) {\n      short __2 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_1) + 512)] = (*(half *)(&(__2)));\n  }\n  for (int ax0_ax1_fused_inner_s_2 = 0; ax0_ax1_fused_inner_s_2 < 4; ++ax0_ax1_fused_inner_s_2) {\n      short __3 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_2) + 1024)] = (*(half *)(&(__3)));\n  }\n  for (int ax0_ax1_fused_inner_s_3 = 0; ax0_ax1_fused_inner_s_3 < 4; ++ax0_ax1_fused_inner_s_3) {\n      short __4 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_3) + 1536)] = (*(half *)(&(__4)));\n  }\n  for (int ax0_ax1_fused_inner_s_4 = 0; ax0_ax1_fused_inner_s_4 < 4; ++ax0_ax1_fused_inner_s_4) {\n      short __5 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_4) + 2048)] = (*(half *)(&(__5)));\n  }\n  for (int ax0_ax1_fused_inner_s_5 = 0; ax0_ax1_fused_inner_s_5 < 4; ++ax0_ax1_fused_inner_s_5) {\n      short __6 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_5) + 2560)] = (*(half *)(&(__6)));\n  }\n  for (int ax0_ax1_fused_inner_s_6 = 0; ax0_ax1_fused_inner_s_6 < 4; ++ax0_ax1_fused_inner_s_6) {\n      short __7 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_6) + 3072)] = (*(half *)(&(__7)));\n  }\n  for (int ax0_ax1_fused_inner_s_7 = 0; ax0_ax1_fused_inner_s_7 < 4; ++ax0_ax1_fused_inner_s_7) {\n      short __8 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_7) + 3584)] = (*(half *)(&(__8)));\n  }\n  for (int ax0_ax1_fused_inner_s_8 = 0; ax0_ax1_fused_inner_s_8 < 4; ++ax0_ax1_fused_inner_s_8) {\n      short __9 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_8) + 4096)] = (*(half *)(&(__9)));\n  }\n  for (int ax0_ax1_fused_inner_s_9 = 0; ax0_ax1_fused_inner_s_9 < 4; ++ax0_ax1_fused_inner_s_9) {\n      short __10 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_9) + 4608)] = (*(half *)(&(__10)));\n  }\n  for (int ax0_ax1_fused_inner_s_10 = 0; ax0_ax1_fused_inner_s_10 < 4; ++ax0_ax1_fused_inner_s_10) {\n      short __11 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_10) + 5120)] = (*(half *)(&(__11)));\n  }\n  for (int ax0_ax1_fused_inner_s_11 = 0; ax0_ax1_fused_inner_s_11 < 4; ++ax0_ax1_fused_inner_s_11) {\n      short __12 = ((short)p1[((((((((int)blockIdx.x) % 96) * 6144) + (((int)threadIdx.y) * 64)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)]) << (short)8;\n    B_decode_shared[((((((int)threadIdx.y) * 64) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_inner_s_11) + 5632)] = (*(half *)(&(__12)));\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 96; ++k_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (p0_shared[((((((int)threadIdx.y) >> 2) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))] * B_decode_shared[((((((int)threadIdx.y) & 3) * 1536) + (k_inner_outer * 16)) + ((int)threadIdx.x))]));\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n  T_quant_linear[(((((((int)blockIdx.x) / 96) * 768) + ((((int)threadIdx.y) >> 2) * 384)) + ((((int)blockIdx.x) % 96) * 4)) + (((int)threadIdx.y) & 3))] = red_buf0[0];\n}\n\n",
    "block_size": [
      16,
      8,
      1
    ],
    "grid_size": [
      2400,
      1,
      1
    ],
    "latency": 0.04505600035190582,
    "name": "Group18",
    "gain": 0
  },
  {
    "nodes": [
      338
    ],
    "node_names": [
      "reshape_add_add_338"
    ],
    "group_id": 217,
    "input_desc": [
      [
        "reshape_add_add_338",
        338,
        0
      ],
      [
        "reshape_add_add_338",
        338,
        1
      ],
      [
        "reshape_add_add_338",
        338,
        2
      ]
    ],
    "output_desc": [
      [
        "reshape_add_add_338",
        338,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(160) Group13(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add_5) {\n  \n  T_add_5[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] = (p2[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))] + (p1[(((((int)blockIdx.x) % 12) * 32) + (((int)threadIdx.x) & 31))] + p0[(((((((int)blockIdx.x) / 12) * 1920) + ((((int)threadIdx.x) >> 5) * 384)) + ((((int)blockIdx.x) % 12) * 32)) + (((int)threadIdx.x) & 31))]));\n}\n\n",
    "block_size": [
      160,
      1,
      1
    ],
    "grid_size": [
      120,
      1,
      1
    ],
    "latency": 0.002623999956995249,
    "name": "Group13",
    "gain": 0
  },
  {
    "nodes": [
      339
    ],
    "node_names": [
      "mean_339"
    ],
    "group_id": 218,
    "input_desc": [
      [
        "mean_339",
        339,
        0
      ]
    ],
    "output_desc": [
      [
        "mean_339",
        339,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group2(half* __restrict__ p0, half* __restrict__ T_cast_4) {\n  \n  float normal_reduce_temp0[1];\n  __shared__ float T_cast_shared[768];\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  float2 __1;\n  uint1 __2 = *(uint1*)(p0 + (((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)));\n  __1.x = (float)(((half2*)(&(__2.x)))->x);\n  __1.y = (float)(((half2*)(&(__2.x)))->y);\n  *(float2*)(T_cast_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2))) = __1;\n  float2 __3;\n  uint1 __4 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 256));\n  __3.x = (float)(((half2*)(&(__4.x)))->x);\n  __3.y = (float)(((half2*)(&(__4.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 256)) = __3;\n  float2 __5;\n  uint1 __6 = *(uint1*)(p0 + ((((((int)blockIdx.x) * 768) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 2)) + 512));\n  __5.x = (float)(((half2*)(&(__6.x)))->x);\n  __5.y = (float)(((half2*)(&(__6.x)))->y);\n  *(float2*)(T_cast_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 2)) + 512)) = __5;\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 6; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[(((((int)threadIdx.y) * 384) + (k2_inner_outer * 64)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 16)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 8)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 4)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 2)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] + ((volatile float*)red_buf0)[(((((int)threadIdx.y) * 64) + ((int)threadIdx.x)) + 1)]);\n    ((volatile float*)red_buf0)[((((int)threadIdx.y) * 64) + ((int)threadIdx.x))] = w_1_0;\n  }\n  __syncthreads();\n  T_cast_4[((((int)blockIdx.x) * 2) + ((int)threadIdx.y))] = ((half)(((volatile float*)red_buf0)[(((int)threadIdx.y) * 64)] * 2.604167e-03f));\n}\n\n",
    "block_size": [
      64,
      2,
      1
    ],
    "grid_size": [
      25,
      1,
      1
    ],
    "latency": 0.0030693334992974997,
    "name": "Group2",
    "gain": 0
  },
  {
    "nodes": [
      340,
      341
    ],
    "node_names": [
      "subtract_multiply_340",
      "mean_add_sqrt_341"
    ],
    "group_id": 219,
    "input_desc": [
      [
        "subtract_multiply_340",
        340,
        0
      ],
      [
        "subtract_multiply_340",
        340,
        1
      ]
    ],
    "output_desc": [
      [
        "subtract_multiply_340",
        340,
        0
      ],
      [
        "mean_add_sqrt_341",
        341,
        0
      ]
    ],
    "code": "__device__ void Group219_0_subtract_multiply_340(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ output_proxy, half* __restrict__ output_proxy_1, char* shared) {\n  output_proxy[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]);\n  output_proxy[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]);\n  output_proxy[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] = (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]);\n  __syncthreads();\n  output_proxy_1[((int)threadIdx.x)] = ((p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 128)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 128)] - p1[((int)blockIdx.x)]));\n  output_proxy_1[(((int)threadIdx.x) + 256)] = ((p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]) * (p0[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 256)] - p1[((int)blockIdx.x)]));\n  __syncthreads();\n}\n\n__device__ void Group219_1_mean_add_sqrt_341(half* __restrict__ p0, half* __restrict__ T_sqrt, char* shared) {\n  float normal_reduce_temp0[1];\n  half* p0_shared = (half*)p0;\n  float* T_cast_shared = (float*)(shared+0);\n  __shared__ float red_buf0[128];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  T_cast_shared[((int)threadIdx.x)] = ((float)p0_shared[((int)threadIdx.x)]);\n  T_cast_shared[(((int)threadIdx.x) + 128)] = ((float)p0_shared[(((int)threadIdx.x) + 128)]);\n  T_cast_shared[(((int)threadIdx.x) + 256)] = ((float)p0_shared[(((int)threadIdx.x) + 256)]);\n  __syncthreads();\n  for (int k2_inner_outer = 0; k2_inner_outer < 3; ++k2_inner_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + T_cast_shared[((k2_inner_outer * 128) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  ((volatile float*)red_buf0)[((int)threadIdx.x)] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 64) {\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 64)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 32) {\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 32)]);\n  }\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 16)]);\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_16_0;\n    float w_8_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 8)]);\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_8_0;\n    float w_4_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 4)]);\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_4_0;\n    float w_2_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 2)]);\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_2_0;\n    float w_1_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 1)]);\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_1_0;\n  }\n  __syncthreads();\n  T_sqrt[((int)blockIdx.x)] = hsqrt((((half)(((volatile float*)red_buf0)[0] * 2.604167e-03f)) + __float2half_rn(1.013279e-06f)));\n}\n\n__global__ void __launch_bounds__(128) Group219(half* input0, half* input1, half* output0, half* output1) {\n  __shared__ char shared[2304];\n  Group219_0_subtract_multiply_340(input0, input1, output0, (half*)(shared+0), shared+0);\n  Group219_1_mean_add_sqrt_341((half*)(shared+0), output1, shared+768);\n}\n",
    "block_size": [
      128,
      1,
      1
    ],
    "grid_size": [
      50,
      1,
      1
    ],
    "latency": 0.0033031110651791096,
    "name": "Group219",
    "gain": 0.002524379873648286
  },
  {
    "nodes": [
      342
    ],
    "node_names": [
      "divide_multiply_add_take_342"
    ],
    "group_id": 220,
    "input_desc": [
      [
        "divide_multiply_add_take_342",
        342,
        0
      ],
      [
        "divide_multiply_add_take_342",
        342,
        1
      ],
      [
        "divide_multiply_add_take_342",
        342,
        2
      ],
      [
        "divide_multiply_add_take_342",
        342,
        3
      ]
    ],
    "output_desc": [
      [
        "divide_multiply_add_take_342",
        342,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(32) Group220(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ p3, half* __restrict__ T_take) {\n  \n  __shared__ half p2_shared[32];\n  __shared__ half p3_shared[32];\n  p2_shared[((int)threadIdx.x)] = p2[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))];\n  p3_shared[((int)threadIdx.x)] = p3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))];\n  __syncthreads();\n  T_take[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (((p0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / p1[0]) * p2_shared[((int)threadIdx.x)]) + p3_shared[((int)threadIdx.x)]);\n}\n\n",
    "block_size": [
      32,
      1,
      1
    ],
    "grid_size": [
      12,
      1,
      1
    ],
    "latency": 0.0026308572851121426,
    "name": "Group220",
    "gain": 0
  },
  {
    "nodes": [
      343
    ],
    "node_names": [
      "nn_dense_add_343"
    ],
    "group_id": 221,
    "input_desc": [
      [
        "nn_dense_add_343",
        343,
        0
      ],
      [
        "nn_dense_add_343",
        343,
        1
      ],
      [
        "nn_dense_add_343",
        343,
        2
      ]
    ],
    "output_desc": [
      [
        "nn_dense_add_343",
        343,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group221(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ p2, half* __restrict__ T_add) {\n  \n  half in_thread_T_matmul_NT_local[1];\n  half p0_local[4];\n  half p1_local[4];\n  half red_buf0[1];\n  in_thread_T_matmul_NT_local[0] = __float2half_rn(0.000000e+00f);\n  for (int k_0 = 0; k_0 < 3; ++k_0) {\n    *(uint2*)(p0_local + 0) = *(uint2*)(p0 + ((k_0 * 128) + (((int)threadIdx.x) * 4)));\n    *(uint2*)(p1_local + 0) = *(uint2*)(p1 + ((((((int)blockIdx.x) * 1536) + (((int)threadIdx.y) * 384)) + (k_0 * 128)) + (((int)threadIdx.x) * 4)));\n    for (int k_2 = 0; k_2 < 4; ++k_2) {\n      in_thread_T_matmul_NT_local[0] = (in_thread_T_matmul_NT_local[0] + (p0_local[k_2] * p1_local[k_2]));\n    }\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = in_thread_T_matmul_NT_local[0];\n  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))] = (red_buf0[0] + p2[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))]);\n}\n\n",
    "block_size": [
      32,
      4,
      1
    ],
    "grid_size": [
      250,
      1,
      1
    ],
    "latency": 0.004095999989658594,
    "name": "Group221",
    "gain": 0
  }
]